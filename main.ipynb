{
  "cells": [
    {
      "cell_type": "code",
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "end_time": "2025-03-07T15:25:24.606618Z",
          "start_time": "2025-03-07T15:25:24.587089Z"
        },
        "id": "initial_id"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/predicting_secondary_structure_of_protein/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rmfloFdiA9r",
        "outputId": "433b2330-5ee2-4a17-d420-faca529fe38b"
      },
      "id": "9rmfloFdiA9r",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Préparation des données"
      ],
      "metadata": {
        "id": "WBeHVWZW6L2S"
      },
      "id": "WBeHVWZW6L2S"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-07T15:25:24.637767Z",
          "start_time": "2025-03-07T15:25:24.615037Z"
        },
        "id": "33c6f0661d59017c"
      },
      "cell_type": "code",
      "source": [
        "def read_protein_sequences(filename):\n",
        "    sequences = []\n",
        "    current_sequence = []\n",
        "\n",
        "    with open(filename, \"r\") as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "\n",
        "            if line == \"<>\":\n",
        "                if current_sequence:\n",
        "                    sequences.append(current_sequence)\n",
        "                current_sequence = []\n",
        "                continue\n",
        "\n",
        "            if line == \"<end>\":\n",
        "                if current_sequence:\n",
        "                    sequences.append(current_sequence)\n",
        "                current_sequence = []\n",
        "                continue\n",
        "\n",
        "            if not line or line.startswith(\"#\"):\n",
        "                continue\n",
        "\n",
        "            parts = line.split()\n",
        "            if len(parts) == 2:\n",
        "                aa, ss = parts\n",
        "                current_sequence.append((aa, ss))\n",
        "\n",
        "    if current_sequence:\n",
        "        sequences.append(current_sequence)\n",
        "\n",
        "    return sequences"
      ],
      "id": "33c6f0661d59017c",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-07T15:25:24.715134Z",
          "start_time": "2025-03-07T15:25:24.672660Z"
        },
        "id": "15a28866aeb81089"
      },
      "cell_type": "code",
      "source": [
        "train = read_protein_sequences(path+\"protein-secondary-structure.train\")\n",
        "test = read_protein_sequences(path+\"protein-secondary-structure.test\")"
      ],
      "id": "15a28866aeb81089",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-07T15:28:31.099904Z",
          "start_time": "2025-03-07T15:28:31.090512Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d6d81c879e77c1f",
        "outputId": "643e5730-3ad4-4527-bc92-8b7646de37f9"
      },
      "cell_type": "code",
      "source": [
        "print(f\"Train : \", len(train), \" paires \" , \"|\" , \"Test :\", len(test), \" paires \")"
      ],
      "id": "4d6d81c879e77c1f",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train :  111  paires  | Test : 17  paires \n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-07T15:30:22.930804Z",
          "start_time": "2025-03-07T15:30:22.910858Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37e8c9b7e01ff74f",
        "outputId": "69a1d3b5-373c-4dc8-b2ab-737cf3ccb7b1"
      },
      "cell_type": "code",
      "source": [
        "# 20 acides aminés + caractère vide \"<s>\"\n",
        "acides_amines = [\n",
        "    \"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\",\n",
        "    \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"Y\", \"<s>\"\n",
        "]\n",
        "# Dictionnaire {lettre: index}\n",
        "aa_to_index = {aa: i for i, aa in enumerate(acides_amines)}\n",
        "\n",
        "# 3 structures secondaires en one-hot\n",
        "structure_secondaire = {'h' : (1,0,0), 'e' : (0,1,0), '_' : (0,0,1) }\n",
        "\n",
        "print(aa_to_index)"
      ],
      "id": "37e8c9b7e01ff74f",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'A': 0, 'C': 1, 'D': 2, 'E': 3, 'F': 4, 'G': 5, 'H': 6, 'I': 7, 'K': 8, 'L': 9, 'M': 10, 'N': 11, 'P': 12, 'Q': 13, 'R': 14, 'S': 15, 'T': 16, 'V': 17, 'W': 18, 'Y': 19, '<s>': 20}\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction de sliding window + encodage\n",
        "def sliding_window_encoding(sequence, window_size=5):\n",
        "    assert window_size % 2 == 1, \"La taille de la fenêtre doit être impaire\"\n",
        "    padding_size = window_size // 2\n",
        "\n",
        "    # Padding de la séquence avec des <s> et _ : on met un début et un fin\n",
        "    padded_sequence = [('<s>', '_')] * padding_size + sequence + [('<s>', '_')] * padding_size\n",
        "\n",
        "    inputs = []\n",
        "    targets = []\n",
        "\n",
        "    for i in range(len(sequence)):\n",
        "        window = padded_sequence[i:i+window_size]  # Longueur 5\n",
        "        letters = [aa_to_index[aa] for aa, _ in window] # De lettres à chiffres grâce à aa_to_index\n",
        "        center_structure = padded_sequence[i + padding_size][1]\n",
        "\n",
        "        # One-hot pour les lettres (21 dim) → concaténées (5 × 21 = 105)\n",
        "        letter_tensor = torch.tensor(letters, dtype=torch.long)\n",
        "        one_hot_letters = F.one_hot(letter_tensor, num_classes=len(acides_amines)).flatten()\n",
        "\n",
        "        # One-hot pour la structure centrale (3 dim)\n",
        "        structure_tensor = torch.tensor(structure_secondaire[center_structure], dtype=torch.float32)\n",
        "\n",
        "        inputs.append(one_hot_letters)\n",
        "        targets.append(structure_tensor)\n",
        "\n",
        "    # Convertir en batch de tenseurs\n",
        "    X = torch.stack(inputs)  # (nb_fenêtres, 105)\n",
        "    y = torch.stack(targets)  # (nb_fenêtres, 3)\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "gZomuhcuNpM8"
      },
      "id": "gZomuhcuNpM8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "7gJ62X7o59nK"
      },
      "id": "7gJ62X7o59nK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Encodage par fenêtre glissante"
      ],
      "metadata": {
        "id": "l0zGe7K6TSC-"
      },
      "id": "l0zGe7K6TSC-"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-07T15:25:24.901436Z",
          "start_time": "2025-03-07T15:25:24.889421Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e539ebaa7cfe0e97",
        "outputId": "869cc051-ec62-4328-e422-c9bd643e35d4"
      },
      "cell_type": "code",
      "source": [
        "# Encodage sliding window sur toutes les séquences\n",
        "def encode_all_sequences(sequences):\n",
        "    all_X = []\n",
        "    all_y = []\n",
        "    # Pour chaque séquence\n",
        "    for seq in sequences:\n",
        "        # Encodage sliding window\n",
        "        X, y = sliding_window_encoding(seq)\n",
        "        # Ajouter aux données X et y\n",
        "        all_X.append(X)\n",
        "        all_y.append(y)\n",
        "    # On retourne\n",
        "    return torch.cat(all_X, dim=0), torch.cat(all_y, dim=0)\n",
        "\n",
        "# Génération des ensembles\n",
        "\n",
        "# Entraînement\n",
        "X_train, y_train = encode_all_sequences(train)\n",
        "\n",
        "# test\n",
        "X_test_raw, y_test_raw = encode_all_sequences(test)\n",
        "\n",
        "# extraction de 20% de l'ensemble de test pour validation\n",
        "X_test, X_val, y_test, y_val = train_test_split(\n",
        "    X_test_raw, y_test_raw, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# coup d'oeil\n",
        "print(\"Taille X_train :\", X_train.shape)\n",
        "print(\"Taille y_train :\", y_train.shape)\n",
        "print(\"Taille X_test  :\", X_test.shape)\n",
        "print(\"Taille y_test  :\", y_test.shape)\n",
        "print(\"Taille X_val   :\", X_val.shape)\n",
        "print(\"Taille y_val   :\", y_val.shape)"
      ],
      "id": "e539ebaa7cfe0e97",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taille X_train : torch.Size([18104, 105])\n",
            "Taille y_train : torch.Size([18104, 3])\n",
            "Taille X_test  : torch.Size([2816, 105])\n",
            "Taille y_test  : torch.Size([2816, 3])\n",
            "Taille X_val   : torch.Size([704, 105])\n",
            "Taille y_val   : torch.Size([704, 3])\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Fonction pour préparer les données encodées avec sliding window\n",
        "def prepare_data(X_train, y_train, X_val, y_val, batch_size=128):\n",
        "    # Conversion des données d'entraînement en TensorDataset et création du DataLoader\n",
        "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Conversion des données de validation en TensorDataset et création du DataLoader\n",
        "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "y_q4KKONiNh9"
      },
      "id": "y_q4KKONiNh9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encodage de séquences complètes pour le LSTM et préparation des données"
      ],
      "metadata": {
        "id": "x_ysVRe1xlA2"
      },
      "id": "x_ysVRe1xlA2"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Fonction pour encoder des séquences complètes sans fenêtre glissante\n",
        "def encode_full_sequences(sequences):\n",
        "    X = []  # séquences d'acides aminés\n",
        "    y = []  # séquences de structures secondaires (labels)\n",
        "\n",
        "    # Pour chaque séquence\n",
        "    for seq in sequences:\n",
        "\n",
        "        # On l'encode directement\n",
        "\n",
        "        # 1 - Encoder chaque acide aminé en indice numérique\n",
        "        encoded_seq = [aa_to_index[aa] for aa, _ in seq]\n",
        "\n",
        "        # 2 - Encoder chaque structure en index numérique (0,1,2)\n",
        "        encoded_labels = [structure_secondaire[ss].index(1) for _, ss in seq]\n",
        "\n",
        "        X.append(encoded_seq)\n",
        "        y.append(encoded_labels)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Fonction pour préparer les données pour le lstm\n",
        "def prepare_data_for_lstm(train, test, batch_size=32, padding_value=20, label_padding=-100):\n",
        "\n",
        "    # Encodage complet sans sliding window\n",
        "    X_train_full, y_train_full = encode_full_sequences(train)\n",
        "    X_test_full, y_test_full = encode_full_sequences(test)\n",
        "\n",
        "    # Fonction pour gérer les tailles des séquences variables chez le lstm\n",
        "    def pad_sequences(sequences, labels):\n",
        "        max_len = max(len(seq) for seq in sequences)\n",
        "        print(max_len)\n",
        "        padded_seqs = []\n",
        "        padded_labels = []\n",
        "\n",
        "        # Pour chaque couple\n",
        "        for seq, lab in zip(sequences, labels):\n",
        "            # On crée autant de séquences à padder\n",
        "            seq_padding = [padding_value] * (max_len - len(seq))\n",
        "            # On les ajoutes\n",
        "            padded_seqs.append(seq + seq_padding)\n",
        "\n",
        "            # Pareil pour les labels\n",
        "            lab_padding = [label_padding] * (max_len - len(lab))\n",
        "            padded_labels.append(lab + lab_padding)\n",
        "\n",
        "        return padded_seqs, padded_labels\n",
        "\n",
        "    X_train_padded, y_train_padded = pad_sequences(X_train_full, y_train_full)\n",
        "    X_test_padded, y_test_padded = pad_sequences(X_test_full, y_test_full)\n",
        "\n",
        "    # Création des datasets\n",
        "    train_dataset = TensorDataset(torch.tensor(X_train_padded), torch.tensor(y_train_padded))\n",
        "    test_dataset = TensorDataset(torch.tensor(X_test_padded), torch.tensor(y_test_padded))\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "qzWJtS8hkL2T"
      },
      "id": "qzWJtS8hkL2T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, test_loader = prepare_data_for_lstm(train, test, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpBfN3uYAyxp",
        "outputId": "33075f28-603c-46fa-f0da-6f46c77f976a"
      },
      "id": "JpBfN3uYAyxp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "497\n",
            "461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sgT03gHE6aXT"
      },
      "id": "sgT03gHE6aXT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "pKxdQMmdSQXh"
      },
      "id": "pKxdQMmdSQXh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### On définit trois modèles : MLP, récurrents et LSTM"
      ],
      "metadata": {
        "id": "cIXii-tpTDsq"
      },
      "id": "cIXii-tpTDsq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Modèle MLP\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_dim=105, hidden_dim=64, output_dim=3):\n",
        "        super(MLPModel, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# Modèle RNN\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.rnn = nn.RNN(105, 100, num_layers=2, nonlinearity=\"tanh\",batch_first=True, dropout=0.3)\n",
        "        self.fc = nn.Linear(100, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 1, 105)\n",
        "        output, _ = self.rnn(x)\n",
        "        return self.fc(output[:, -1])\n",
        "\n",
        "\n",
        "# Modèle LSTM : On va l'entraîner avec des séquences complètes\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size=21, embedding_dim=256, hidden_dim=200, output_dim=3):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=20)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, (hn, cn) = self.lstm(x)\n",
        "        return self.fc(output)\n"
      ],
      "metadata": {
        "id": "aXGEnueuSb0A"
      },
      "id": "aXGEnueuSb0A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Entraînement"
      ],
      "metadata": {
        "id": "M4vFDKDTSqLi"
      },
      "id": "M4vFDKDTSqLi"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# On définit trois optimiseurs\n",
        "optimizers = {'adam': optim.Adam, 'sgd': optim.SGD, 'rmsprop': optim.RMSprop}"
      ],
      "metadata": {
        "id": "aMyDxhVMNmsQ"
      },
      "id": "aMyDxhVMNmsQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction pour entraîner les modèles non LSTM\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer_name='adam', lr=0.01, epochs=5):\n",
        "    # Vérification des optimiseurs disponibles\n",
        "    optimizers = {\n",
        "        'adam': torch.optim.Adam,\n",
        "        'sgd': torch.optim.SGD,\n",
        "        'rmsprop': torch.optim.RMSprop\n",
        "    }\n",
        "\n",
        "    # Sélection de l'optimiseur\n",
        "    if optimizer_name.lower() in optimizers:\n",
        "        optimizer = optimizers[optimizer_name.lower()](model.parameters(), lr=lr)\n",
        "    else:\n",
        "        raise ValueError(f\"Optimizer '{optimizer_name}' not recognized. Choose from {list(optimizers.keys())}.\")\n",
        "\n",
        "    # Fonction de perte\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Déplacer le modèle et la fonction de perte au GPU si disponible\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    # Entraînement\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Remise à zéro des gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Conversion des labels one-hot en indices si nécessaire\n",
        "            if labels.dim() > 1 and labels.size(1) > 1:\n",
        "                labels = torch.argmax(labels, dim=1)\n",
        "\n",
        "            # Calcul de la perte et rétropropagation\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Afficher les informations d'entraînement\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Évaluation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Conversion des labels one-hot en indices si nécessaire\n",
        "            if labels.dim() > 1 and labels.size(1) > 1:\n",
        "                labels = torch.argmax(labels, dim=1)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Validation Accuracy: {accuracy:.4f}')\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "IBC8foPoSnJ7"
      },
      "id": "IBC8foPoSnJ7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fonction pour entraîner un LSTM"
      ],
      "metadata": {
        "id": "VQkLjPr8_sfb"
      },
      "id": "VQkLjPr8_sfb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction pour entraîner le LSTM\n",
        "def train_lstm_full_sequences(model, train_dataset, val_dataset, optimizer_name='adam', lr=0.001, epochs=10, batch_size=32):\n",
        "    # Dataloaders pour les données\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "    # Optimiseurs disponibles\n",
        "    if optimizer_name.lower() not in optimizers:\n",
        "        raise ValueError(f\"Optimizer '{optimizer_name}' not recognized.\")\n",
        "\n",
        "    optimizer = optimizers[optimizer_name.lower()](model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for sequences, labels in train_loader:\n",
        "            sequences = sequences.to(device).long()\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(sequences)\n",
        "            outputs = outputs.view(-1, outputs.shape[-1])\n",
        "            labels = labels.view(-1)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader) if len(train_loader) > 0 else 0.0\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Évaluation finale pour obtenir l'accuracy\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sequences, labels in val_loader:\n",
        "            sequences = sequences.to(device).long()\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(sequences)\n",
        "            predictions = torch.argmax(outputs, dim=-1)\n",
        "            mask = labels != -100\n",
        "            correct += ((predictions == labels) & mask).sum().item()\n",
        "            total += mask.sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "t8rTJLMekOIZ"
      },
      "id": "t8rTJLMekOIZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train"
      ],
      "metadata": {
        "id": "QNBJqsJsXzO2"
      },
      "id": "QNBJqsJsXzO2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_train"
      ],
      "metadata": {
        "id": "EwouktrlX57v"
      },
      "id": "EwouktrlX57v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCM0MP-vYAg2",
        "outputId": "68c7fab5-4e8c-4789-96a9-e3f153769136"
      },
      "id": "RCM0MP-vYAg2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 1,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDR0aQtQYCvD",
        "outputId": "07da36fd-8e77-469c-ac83-2694c82217af"
      },
      "id": "iDR0aQtQYCvD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.],\n",
              "        [0., 0., 1.],\n",
              "        [0., 0., 1.],\n",
              "        ...,\n",
              "        [0., 1., 0.],\n",
              "        [0., 0., 1.],\n",
              "        [0., 1., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = {'MLP': MLPModel(), 'RNN': RNNModel(), 'LSTM': LSTMModel()}"
      ],
      "metadata": {
        "id": "i4Etd16mMNZw"
      },
      "id": "i4Etd16mMNZw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction pour exécuter un modèle\n",
        "def execute_model(model_name: str, optimizer_name: str, lr: float, window_size: int, epochs: int=100):\n",
        "    if optimizer_name not in optimizers:\n",
        "        raise ValueError(f\"Optimizer '{optimizer_name}' not recognized. Choose from {list(optimizers.keys())}.\")\n",
        "\n",
        "    # Cas spécial pour LSTM\n",
        "    if model_name == 'LSTM':\n",
        "        train_loader, val_loader = prepare_data_for_lstm(train, test, batch_size=128)\n",
        "        model = models[model_name]\n",
        "        print(f\"Training {model_name} model with {optimizer_name} optimizer using full sequences...\")\n",
        "        accuracy = train_lstm_full_sequences(model, train_loader.dataset, val_loader.dataset, optimizer_name, lr, epochs, batch_size=128)\n",
        "        print(f\"Accuracy of {model_name}: {accuracy:.2f}\")\n",
        "        return accuracy\n",
        "\n",
        "    else:\n",
        "        train_loader, val_loader = prepare_data(X_train, y_train, X_val, y_val, batch_size=128)\n",
        "        model = models[model_name]\n",
        "        print(f\"Training {model_name} model with {optimizer_name} optimizer using window size {window_size}...\")\n",
        "        accuracy = train_model(model, train_loader, val_loader, optimizer_name, lr, epochs)\n",
        "        print(f\"Accuracy of {model_name}: {accuracy:.2f}\")\n",
        "        return accuracy"
      ],
      "metadata": {
        "id": "TDJYRuCP_UlK"
      },
      "id": "TDJYRuCP_UlK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# execute_model('MLP', 'adam', 0.001, 5, epochs=50)"
      ],
      "metadata": {
        "id": "I5q34WbxMcxR",
        "collapsed": true
      },
      "id": "I5q34WbxMcxR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# execute_model('RNN', 'adam', 0.001, 5, 50)"
      ],
      "metadata": {
        "id": "PxIU2diIPHPh",
        "collapsed": true
      },
      "id": "PxIU2diIPHPh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "execute_model('LSTM', 'adam', 0.001, 5, 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXSFdnk0y7wm",
        "outputId": "de0e6475-10fe-4c68-a962-72bbea127d37"
      },
      "id": "AXSFdnk0y7wm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "497\n",
            "461\n",
            "Training LSTM model with adam optimizer using full sequences...\n",
            "Epoch 1/30, Training Loss: 1.1012\n",
            "Epoch 2/30, Training Loss: 1.0117\n",
            "Epoch 3/30, Training Loss: 0.9596\n",
            "Epoch 4/30, Training Loss: 0.9368\n",
            "Epoch 5/30, Training Loss: 0.9274\n",
            "Epoch 6/30, Training Loss: 0.9162\n",
            "Epoch 7/30, Training Loss: 0.9033\n",
            "Epoch 8/30, Training Loss: 0.8930\n",
            "Epoch 9/30, Training Loss: 0.8851\n",
            "Epoch 10/30, Training Loss: 0.8776\n",
            "Epoch 11/30, Training Loss: 0.8694\n",
            "Epoch 12/30, Training Loss: 0.8603\n",
            "Epoch 13/30, Training Loss: 0.8500\n",
            "Epoch 14/30, Training Loss: 0.8385\n",
            "Epoch 15/30, Training Loss: 0.8277\n",
            "Epoch 16/30, Training Loss: 0.8207\n",
            "Epoch 17/30, Training Loss: 0.8165\n",
            "Epoch 18/30, Training Loss: 0.8112\n",
            "Epoch 19/30, Training Loss: 0.8045\n",
            "Epoch 20/30, Training Loss: 0.7976\n",
            "Epoch 21/30, Training Loss: 0.7927\n",
            "Epoch 22/30, Training Loss: 0.7882\n",
            "Epoch 23/30, Training Loss: 0.7835\n",
            "Epoch 24/30, Training Loss: 0.7780\n",
            "Epoch 25/30, Training Loss: 0.7727\n",
            "Epoch 26/30, Training Loss: 0.7669\n",
            "Epoch 27/30, Training Loss: 0.7607\n",
            "Epoch 28/30, Training Loss: 0.7543\n",
            "Epoch 29/30, Training Loss: 0.7480\n",
            "Epoch 30/30, Training Loss: 0.7413\n",
            "Accuracy of LSTM: 0.61\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6102272727272727"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Fonction pour faire plusiers exécutions et comparer\n",
        "def execute_models_and_compare():\n",
        "    models = ['MLP', 'RNN', 'LSTM']\n",
        "    optimizers = ['adam', 'sgd', 'rmsprop']\n",
        "    results = []\n",
        "\n",
        "    for model_name in models:\n",
        "        for optimizer_name in optimizers:\n",
        "            print(f\"Running {model_name} with {optimizer_name} optimizer...\")\n",
        "            accuracy = execute_model(model_name, optimizer_name, 0.001, 5, 20) # LE NOMBRE d'époch ne change rien pour le lstm\n",
        "            results.append({\n",
        "                'Model': model_name,\n",
        "                'Optimizer': optimizer_name,\n",
        "                'Accuracy': accuracy\n",
        "            })\n",
        "\n",
        "    # Création d'un DataFrame pour les résultats\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    # Visualisation avec seaborn\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(data=results_df, x='Model', y='Accuracy', hue='Optimizer')\n",
        "    plt.title('Comparative Accuracy of Models with Different Optimizers')\n",
        "    plt.xlabel('Model')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(title='Optimizer')\n",
        "    plt.show()\n",
        "\n",
        "    return results_df"
      ],
      "metadata": {
        "id": "OIFUh0HMcZxx"
      },
      "id": "OIFUh0HMcZxx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_multi_ex = execute_models_and_compare()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "j3aD64CDcb8K",
        "outputId": "bacfc7be-5d51-47a4-a4f0-88a1d31acecc"
      },
      "id": "j3aD64CDcb8K",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running MLP with adam optimizer...\n",
            "Training MLP model with adam optimizer using window size 5...\n",
            "Epoch [1/20], Step [1/142], Loss: 1.1189\n",
            "Epoch [1/20], Step [2/142], Loss: 1.1111\n",
            "Epoch [1/20], Step [3/142], Loss: 1.0994\n",
            "Epoch [1/20], Step [4/142], Loss: 1.1121\n",
            "Epoch [1/20], Step [5/142], Loss: 1.1044\n",
            "Epoch [1/20], Step [6/142], Loss: 1.0918\n",
            "Epoch [1/20], Step [7/142], Loss: 1.1001\n",
            "Epoch [1/20], Step [8/142], Loss: 1.0822\n",
            "Epoch [1/20], Step [9/142], Loss: 1.0829\n",
            "Epoch [1/20], Step [10/142], Loss: 1.0832\n",
            "Epoch [1/20], Step [11/142], Loss: 1.0769\n",
            "Epoch [1/20], Step [12/142], Loss: 1.0763\n",
            "Epoch [1/20], Step [13/142], Loss: 1.0590\n",
            "Epoch [1/20], Step [14/142], Loss: 1.0684\n",
            "Epoch [1/20], Step [15/142], Loss: 1.0734\n",
            "Epoch [1/20], Step [16/142], Loss: 1.0521\n",
            "Epoch [1/20], Step [17/142], Loss: 1.0727\n",
            "Epoch [1/20], Step [18/142], Loss: 1.0583\n",
            "Epoch [1/20], Step [19/142], Loss: 1.0550\n",
            "Epoch [1/20], Step [20/142], Loss: 1.0575\n",
            "Epoch [1/20], Step [21/142], Loss: 1.0388\n",
            "Epoch [1/20], Step [22/142], Loss: 1.0368\n",
            "Epoch [1/20], Step [23/142], Loss: 1.0026\n",
            "Epoch [1/20], Step [24/142], Loss: 1.0272\n",
            "Epoch [1/20], Step [25/142], Loss: 1.0335\n",
            "Epoch [1/20], Step [26/142], Loss: 1.0150\n",
            "Epoch [1/20], Step [27/142], Loss: 1.0296\n",
            "Epoch [1/20], Step [28/142], Loss: 0.9957\n",
            "Epoch [1/20], Step [29/142], Loss: 0.9969\n",
            "Epoch [1/20], Step [30/142], Loss: 1.0275\n",
            "Epoch [1/20], Step [31/142], Loss: 1.0264\n",
            "Epoch [1/20], Step [32/142], Loss: 1.0309\n",
            "Epoch [1/20], Step [33/142], Loss: 1.0216\n",
            "Epoch [1/20], Step [34/142], Loss: 1.0300\n",
            "Epoch [1/20], Step [35/142], Loss: 0.9592\n",
            "Epoch [1/20], Step [36/142], Loss: 1.0019\n",
            "Epoch [1/20], Step [37/142], Loss: 1.0050\n",
            "Epoch [1/20], Step [38/142], Loss: 0.9647\n",
            "Epoch [1/20], Step [39/142], Loss: 0.9742\n",
            "Epoch [1/20], Step [40/142], Loss: 0.9761\n",
            "Epoch [1/20], Step [41/142], Loss: 0.9675\n",
            "Epoch [1/20], Step [42/142], Loss: 0.9907\n",
            "Epoch [1/20], Step [43/142], Loss: 1.0395\n",
            "Epoch [1/20], Step [44/142], Loss: 1.0358\n",
            "Epoch [1/20], Step [45/142], Loss: 0.9610\n",
            "Epoch [1/20], Step [46/142], Loss: 0.9741\n",
            "Epoch [1/20], Step [47/142], Loss: 0.9465\n",
            "Epoch [1/20], Step [48/142], Loss: 0.9978\n",
            "Epoch [1/20], Step [49/142], Loss: 0.9921\n",
            "Epoch [1/20], Step [50/142], Loss: 0.9649\n",
            "Epoch [1/20], Step [51/142], Loss: 0.9662\n",
            "Epoch [1/20], Step [52/142], Loss: 0.9422\n",
            "Epoch [1/20], Step [53/142], Loss: 1.0150\n",
            "Epoch [1/20], Step [54/142], Loss: 0.9743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-40-867da078f61e>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
            "<ipython-input-40-867da078f61e>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mLe flux de sortie a été tronqué et ne contient que les 5000 dernières lignes.\u001b[0m\n",
            "Epoch [6/20], Step [52/142], Loss: 0.8815\n",
            "Epoch [6/20], Step [53/142], Loss: 0.7580\n",
            "Epoch [6/20], Step [54/142], Loss: 0.8420\n",
            "Epoch [6/20], Step [55/142], Loss: 0.8866\n",
            "Epoch [6/20], Step [56/142], Loss: 0.8073\n",
            "Epoch [6/20], Step [57/142], Loss: 0.7708\n",
            "Epoch [6/20], Step [58/142], Loss: 0.8505\n",
            "Epoch [6/20], Step [59/142], Loss: 0.8397\n",
            "Epoch [6/20], Step [60/142], Loss: 0.9297\n",
            "Epoch [6/20], Step [61/142], Loss: 0.9252\n",
            "Epoch [6/20], Step [62/142], Loss: 0.8777\n",
            "Epoch [6/20], Step [63/142], Loss: 0.9072\n",
            "Epoch [6/20], Step [64/142], Loss: 0.8245\n",
            "Epoch [6/20], Step [65/142], Loss: 0.8194\n",
            "Epoch [6/20], Step [66/142], Loss: 1.0560\n",
            "Epoch [6/20], Step [67/142], Loss: 0.8818\n",
            "Epoch [6/20], Step [68/142], Loss: 0.8255\n",
            "Epoch [6/20], Step [69/142], Loss: 0.7092\n",
            "Epoch [6/20], Step [70/142], Loss: 0.7841\n",
            "Epoch [6/20], Step [71/142], Loss: 0.9438\n",
            "Epoch [6/20], Step [72/142], Loss: 0.8390\n",
            "Epoch [6/20], Step [73/142], Loss: 0.8000\n",
            "Epoch [6/20], Step [74/142], Loss: 0.7975\n",
            "Epoch [6/20], Step [75/142], Loss: 0.9114\n",
            "Epoch [6/20], Step [76/142], Loss: 0.8426\n",
            "Epoch [6/20], Step [77/142], Loss: 0.7961\n",
            "Epoch [6/20], Step [78/142], Loss: 0.7204\n",
            "Epoch [6/20], Step [79/142], Loss: 0.8632\n",
            "Epoch [6/20], Step [80/142], Loss: 0.8771\n",
            "Epoch [6/20], Step [81/142], Loss: 0.7777\n",
            "Epoch [6/20], Step [82/142], Loss: 0.7376\n",
            "Epoch [6/20], Step [83/142], Loss: 0.8404\n",
            "Epoch [6/20], Step [84/142], Loss: 0.8712\n",
            "Epoch [6/20], Step [85/142], Loss: 0.7952\n",
            "Epoch [6/20], Step [86/142], Loss: 0.7889\n",
            "Epoch [6/20], Step [87/142], Loss: 0.8711\n",
            "Epoch [6/20], Step [88/142], Loss: 0.8307\n",
            "Epoch [6/20], Step [89/142], Loss: 0.7950\n",
            "Epoch [6/20], Step [90/142], Loss: 0.8857\n",
            "Epoch [6/20], Step [91/142], Loss: 0.7868\n",
            "Epoch [6/20], Step [92/142], Loss: 0.8227\n",
            "Epoch [6/20], Step [93/142], Loss: 0.8895\n",
            "Epoch [6/20], Step [94/142], Loss: 0.8096\n",
            "Epoch [6/20], Step [95/142], Loss: 0.7739\n",
            "Epoch [6/20], Step [96/142], Loss: 0.9194\n",
            "Epoch [6/20], Step [97/142], Loss: 0.9174\n",
            "Epoch [6/20], Step [98/142], Loss: 0.8157\n",
            "Epoch [6/20], Step [99/142], Loss: 0.7807\n",
            "Epoch [6/20], Step [100/142], Loss: 0.8422\n",
            "Epoch [6/20], Step [101/142], Loss: 0.8644\n",
            "Epoch [6/20], Step [102/142], Loss: 0.8560\n",
            "Epoch [6/20], Step [103/142], Loss: 0.9404\n",
            "Epoch [6/20], Step [104/142], Loss: 0.7773\n",
            "Epoch [6/20], Step [105/142], Loss: 0.7936\n",
            "Epoch [6/20], Step [106/142], Loss: 0.8818\n",
            "Epoch [6/20], Step [107/142], Loss: 0.8827\n",
            "Epoch [6/20], Step [108/142], Loss: 0.8116\n",
            "Epoch [6/20], Step [109/142], Loss: 0.8088\n",
            "Epoch [6/20], Step [110/142], Loss: 0.8692\n",
            "Epoch [6/20], Step [111/142], Loss: 0.8356\n",
            "Epoch [6/20], Step [112/142], Loss: 0.8003\n",
            "Epoch [6/20], Step [113/142], Loss: 0.8340\n",
            "Epoch [6/20], Step [114/142], Loss: 0.8132\n",
            "Epoch [6/20], Step [115/142], Loss: 0.8889\n",
            "Epoch [6/20], Step [116/142], Loss: 0.8144\n",
            "Epoch [6/20], Step [117/142], Loss: 0.7528\n",
            "Epoch [6/20], Step [118/142], Loss: 0.7997\n",
            "Epoch [6/20], Step [119/142], Loss: 0.8139\n",
            "Epoch [6/20], Step [120/142], Loss: 0.8437\n",
            "Epoch [6/20], Step [121/142], Loss: 0.8188\n",
            "Epoch [6/20], Step [122/142], Loss: 0.7877\n",
            "Epoch [6/20], Step [123/142], Loss: 0.9183\n",
            "Epoch [6/20], Step [124/142], Loss: 0.7667\n",
            "Epoch [6/20], Step [125/142], Loss: 0.8610\n",
            "Epoch [6/20], Step [126/142], Loss: 0.8704\n",
            "Epoch [6/20], Step [127/142], Loss: 0.8924\n",
            "Epoch [6/20], Step [128/142], Loss: 0.8177\n",
            "Epoch [6/20], Step [129/142], Loss: 0.7858\n",
            "Epoch [6/20], Step [130/142], Loss: 0.8959\n",
            "Epoch [6/20], Step [131/142], Loss: 0.8824\n",
            "Epoch [6/20], Step [132/142], Loss: 0.8711\n",
            "Epoch [6/20], Step [133/142], Loss: 0.7883\n",
            "Epoch [6/20], Step [134/142], Loss: 0.7774\n",
            "Epoch [6/20], Step [135/142], Loss: 0.8022\n",
            "Epoch [6/20], Step [136/142], Loss: 0.8247\n",
            "Epoch [6/20], Step [137/142], Loss: 0.8386\n",
            "Epoch [6/20], Step [138/142], Loss: 0.8048\n",
            "Epoch [6/20], Step [139/142], Loss: 0.7980\n",
            "Epoch [6/20], Step [140/142], Loss: 0.9030\n",
            "Epoch [6/20], Step [141/142], Loss: 0.8693\n",
            "Epoch [6/20], Step [142/142], Loss: 0.9487\n",
            "Epoch [7/20], Step [1/142], Loss: 0.8451\n",
            "Epoch [7/20], Step [2/142], Loss: 0.9014\n",
            "Epoch [7/20], Step [3/142], Loss: 0.8002\n",
            "Epoch [7/20], Step [4/142], Loss: 0.7729\n",
            "Epoch [7/20], Step [5/142], Loss: 0.8681\n",
            "Epoch [7/20], Step [6/142], Loss: 0.7950\n",
            "Epoch [7/20], Step [7/142], Loss: 0.8712\n",
            "Epoch [7/20], Step [8/142], Loss: 0.7606\n",
            "Epoch [7/20], Step [9/142], Loss: 0.8900\n",
            "Epoch [7/20], Step [10/142], Loss: 0.8771\n",
            "Epoch [7/20], Step [11/142], Loss: 0.8525\n",
            "Epoch [7/20], Step [12/142], Loss: 0.8351\n",
            "Epoch [7/20], Step [13/142], Loss: 0.8401\n",
            "Epoch [7/20], Step [14/142], Loss: 0.8665\n",
            "Epoch [7/20], Step [15/142], Loss: 0.9658\n",
            "Epoch [7/20], Step [16/142], Loss: 0.8684\n",
            "Epoch [7/20], Step [17/142], Loss: 0.7275\n",
            "Epoch [7/20], Step [18/142], Loss: 0.8360\n",
            "Epoch [7/20], Step [19/142], Loss: 0.7584\n",
            "Epoch [7/20], Step [20/142], Loss: 0.8454\n",
            "Epoch [7/20], Step [21/142], Loss: 0.8305\n",
            "Epoch [7/20], Step [22/142], Loss: 0.9535\n",
            "Epoch [7/20], Step [23/142], Loss: 0.8067\n",
            "Epoch [7/20], Step [24/142], Loss: 0.7784\n",
            "Epoch [7/20], Step [25/142], Loss: 0.7740\n",
            "Epoch [7/20], Step [26/142], Loss: 0.8191\n",
            "Epoch [7/20], Step [27/142], Loss: 0.8947\n",
            "Epoch [7/20], Step [28/142], Loss: 0.8341\n",
            "Epoch [7/20], Step [29/142], Loss: 0.7885\n",
            "Epoch [7/20], Step [30/142], Loss: 0.8273\n",
            "Epoch [7/20], Step [31/142], Loss: 0.9037\n",
            "Epoch [7/20], Step [32/142], Loss: 0.8259\n",
            "Epoch [7/20], Step [33/142], Loss: 0.8307\n",
            "Epoch [7/20], Step [34/142], Loss: 0.8167\n",
            "Epoch [7/20], Step [35/142], Loss: 0.8157\n",
            "Epoch [7/20], Step [36/142], Loss: 0.7897\n",
            "Epoch [7/20], Step [37/142], Loss: 0.8733\n",
            "Epoch [7/20], Step [38/142], Loss: 0.8213\n",
            "Epoch [7/20], Step [39/142], Loss: 0.8419\n",
            "Epoch [7/20], Step [40/142], Loss: 0.8837\n",
            "Epoch [7/20], Step [41/142], Loss: 0.8619\n",
            "Epoch [7/20], Step [42/142], Loss: 0.9099\n",
            "Epoch [7/20], Step [43/142], Loss: 0.8565\n",
            "Epoch [7/20], Step [44/142], Loss: 0.8660\n",
            "Epoch [7/20], Step [45/142], Loss: 0.8812\n",
            "Epoch [7/20], Step [46/142], Loss: 0.9033\n",
            "Epoch [7/20], Step [47/142], Loss: 0.8155\n",
            "Epoch [7/20], Step [48/142], Loss: 0.9820\n",
            "Epoch [7/20], Step [49/142], Loss: 0.7958\n",
            "Epoch [7/20], Step [50/142], Loss: 0.8339\n",
            "Epoch [7/20], Step [51/142], Loss: 0.7910\n",
            "Epoch [7/20], Step [52/142], Loss: 0.8063\n",
            "Epoch [7/20], Step [53/142], Loss: 0.8450\n",
            "Epoch [7/20], Step [54/142], Loss: 0.8919\n",
            "Epoch [7/20], Step [55/142], Loss: 0.9053\n",
            "Epoch [7/20], Step [56/142], Loss: 0.8387\n",
            "Epoch [7/20], Step [57/142], Loss: 0.8674\n",
            "Epoch [7/20], Step [58/142], Loss: 0.8519\n",
            "Epoch [7/20], Step [59/142], Loss: 0.8238\n",
            "Epoch [7/20], Step [60/142], Loss: 0.8745\n",
            "Epoch [7/20], Step [61/142], Loss: 0.7901\n",
            "Epoch [7/20], Step [62/142], Loss: 0.7221\n",
            "Epoch [7/20], Step [63/142], Loss: 0.9053\n",
            "Epoch [7/20], Step [64/142], Loss: 0.9479\n",
            "Epoch [7/20], Step [65/142], Loss: 0.8137\n",
            "Epoch [7/20], Step [66/142], Loss: 0.9124\n",
            "Epoch [7/20], Step [67/142], Loss: 0.9037\n",
            "Epoch [7/20], Step [68/142], Loss: 0.7825\n",
            "Epoch [7/20], Step [69/142], Loss: 0.8937\n",
            "Epoch [7/20], Step [70/142], Loss: 0.9327\n",
            "Epoch [7/20], Step [71/142], Loss: 0.8534\n",
            "Epoch [7/20], Step [72/142], Loss: 0.8034\n",
            "Epoch [7/20], Step [73/142], Loss: 0.8272\n",
            "Epoch [7/20], Step [74/142], Loss: 0.6758\n",
            "Epoch [7/20], Step [75/142], Loss: 0.7834\n",
            "Epoch [7/20], Step [76/142], Loss: 0.7892\n",
            "Epoch [7/20], Step [77/142], Loss: 0.9362\n",
            "Epoch [7/20], Step [78/142], Loss: 0.8203\n",
            "Epoch [7/20], Step [79/142], Loss: 0.8292\n",
            "Epoch [7/20], Step [80/142], Loss: 0.8844\n",
            "Epoch [7/20], Step [81/142], Loss: 0.8250\n",
            "Epoch [7/20], Step [82/142], Loss: 0.8599\n",
            "Epoch [7/20], Step [83/142], Loss: 0.8732\n",
            "Epoch [7/20], Step [84/142], Loss: 0.8982\n",
            "Epoch [7/20], Step [85/142], Loss: 0.8598\n",
            "Epoch [7/20], Step [86/142], Loss: 0.8466\n",
            "Epoch [7/20], Step [87/142], Loss: 0.7767\n",
            "Epoch [7/20], Step [88/142], Loss: 0.8785\n",
            "Epoch [7/20], Step [89/142], Loss: 0.8377\n",
            "Epoch [7/20], Step [90/142], Loss: 0.8981\n",
            "Epoch [7/20], Step [91/142], Loss: 0.7961\n",
            "Epoch [7/20], Step [92/142], Loss: 0.7802\n",
            "Epoch [7/20], Step [93/142], Loss: 0.9555\n",
            "Epoch [7/20], Step [94/142], Loss: 0.8478\n",
            "Epoch [7/20], Step [95/142], Loss: 0.8473\n",
            "Epoch [7/20], Step [96/142], Loss: 0.9080\n",
            "Epoch [7/20], Step [97/142], Loss: 0.8849\n",
            "Epoch [7/20], Step [98/142], Loss: 0.8456\n",
            "Epoch [7/20], Step [99/142], Loss: 0.8028\n",
            "Epoch [7/20], Step [100/142], Loss: 0.7626\n",
            "Epoch [7/20], Step [101/142], Loss: 0.8066\n",
            "Epoch [7/20], Step [102/142], Loss: 0.8104\n",
            "Epoch [7/20], Step [103/142], Loss: 0.8293\n",
            "Epoch [7/20], Step [104/142], Loss: 0.8710\n",
            "Epoch [7/20], Step [105/142], Loss: 0.7582\n",
            "Epoch [7/20], Step [106/142], Loss: 0.8218\n",
            "Epoch [7/20], Step [107/142], Loss: 0.7615\n",
            "Epoch [7/20], Step [108/142], Loss: 0.9420\n",
            "Epoch [7/20], Step [109/142], Loss: 0.7194\n",
            "Epoch [7/20], Step [110/142], Loss: 0.8224\n",
            "Epoch [7/20], Step [111/142], Loss: 0.9317\n",
            "Epoch [7/20], Step [112/142], Loss: 0.8476\n",
            "Epoch [7/20], Step [113/142], Loss: 0.8989\n",
            "Epoch [7/20], Step [114/142], Loss: 0.8293\n",
            "Epoch [7/20], Step [115/142], Loss: 0.8845\n",
            "Epoch [7/20], Step [116/142], Loss: 0.8012\n",
            "Epoch [7/20], Step [117/142], Loss: 0.9050\n",
            "Epoch [7/20], Step [118/142], Loss: 0.8500\n",
            "Epoch [7/20], Step [119/142], Loss: 0.8658\n",
            "Epoch [7/20], Step [120/142], Loss: 0.8618\n",
            "Epoch [7/20], Step [121/142], Loss: 0.7957\n",
            "Epoch [7/20], Step [122/142], Loss: 0.9070\n",
            "Epoch [7/20], Step [123/142], Loss: 0.8353\n",
            "Epoch [7/20], Step [124/142], Loss: 0.8910\n",
            "Epoch [7/20], Step [125/142], Loss: 0.9311\n",
            "Epoch [7/20], Step [126/142], Loss: 0.8311\n",
            "Epoch [7/20], Step [127/142], Loss: 0.8803\n",
            "Epoch [7/20], Step [128/142], Loss: 0.9139\n",
            "Epoch [7/20], Step [129/142], Loss: 0.8031\n",
            "Epoch [7/20], Step [130/142], Loss: 0.8280\n",
            "Epoch [7/20], Step [131/142], Loss: 0.8665\n",
            "Epoch [7/20], Step [132/142], Loss: 0.9168\n",
            "Epoch [7/20], Step [133/142], Loss: 0.8970\n",
            "Epoch [7/20], Step [134/142], Loss: 0.8920\n",
            "Epoch [7/20], Step [135/142], Loss: 0.7556\n",
            "Epoch [7/20], Step [136/142], Loss: 0.8180\n",
            "Epoch [7/20], Step [137/142], Loss: 0.8542\n",
            "Epoch [7/20], Step [138/142], Loss: 0.7541\n",
            "Epoch [7/20], Step [139/142], Loss: 0.8649\n",
            "Epoch [7/20], Step [140/142], Loss: 0.8542\n",
            "Epoch [7/20], Step [141/142], Loss: 0.7647\n",
            "Epoch [7/20], Step [142/142], Loss: 0.8195\n",
            "Epoch [8/20], Step [1/142], Loss: 0.9249\n",
            "Epoch [8/20], Step [2/142], Loss: 0.8555\n",
            "Epoch [8/20], Step [3/142], Loss: 0.8539\n",
            "Epoch [8/20], Step [4/142], Loss: 0.8211\n",
            "Epoch [8/20], Step [5/142], Loss: 0.8064\n",
            "Epoch [8/20], Step [6/142], Loss: 0.8095\n",
            "Epoch [8/20], Step [7/142], Loss: 0.8420\n",
            "Epoch [8/20], Step [8/142], Loss: 0.8966\n",
            "Epoch [8/20], Step [9/142], Loss: 0.7616\n",
            "Epoch [8/20], Step [10/142], Loss: 0.8586\n",
            "Epoch [8/20], Step [11/142], Loss: 0.8544\n",
            "Epoch [8/20], Step [12/142], Loss: 0.7629\n",
            "Epoch [8/20], Step [13/142], Loss: 0.7934\n",
            "Epoch [8/20], Step [14/142], Loss: 0.9019\n",
            "Epoch [8/20], Step [15/142], Loss: 0.8855\n",
            "Epoch [8/20], Step [16/142], Loss: 0.8533\n",
            "Epoch [8/20], Step [17/142], Loss: 0.8788\n",
            "Epoch [8/20], Step [18/142], Loss: 0.8356\n",
            "Epoch [8/20], Step [19/142], Loss: 0.8516\n",
            "Epoch [8/20], Step [20/142], Loss: 0.9228\n",
            "Epoch [8/20], Step [21/142], Loss: 0.7855\n",
            "Epoch [8/20], Step [22/142], Loss: 0.8979\n",
            "Epoch [8/20], Step [23/142], Loss: 0.8397\n",
            "Epoch [8/20], Step [24/142], Loss: 0.7879\n",
            "Epoch [8/20], Step [25/142], Loss: 0.8196\n",
            "Epoch [8/20], Step [26/142], Loss: 0.7874\n",
            "Epoch [8/20], Step [27/142], Loss: 0.8835\n",
            "Epoch [8/20], Step [28/142], Loss: 0.8527\n",
            "Epoch [8/20], Step [29/142], Loss: 0.9760\n",
            "Epoch [8/20], Step [30/142], Loss: 0.8670\n",
            "Epoch [8/20], Step [31/142], Loss: 0.8572\n",
            "Epoch [8/20], Step [32/142], Loss: 0.8418\n",
            "Epoch [8/20], Step [33/142], Loss: 0.8529\n",
            "Epoch [8/20], Step [34/142], Loss: 0.8271\n",
            "Epoch [8/20], Step [35/142], Loss: 0.7137\n",
            "Epoch [8/20], Step [36/142], Loss: 0.8560\n",
            "Epoch [8/20], Step [37/142], Loss: 0.8366\n",
            "Epoch [8/20], Step [38/142], Loss: 0.7883\n",
            "Epoch [8/20], Step [39/142], Loss: 0.8071\n",
            "Epoch [8/20], Step [40/142], Loss: 0.8680\n",
            "Epoch [8/20], Step [41/142], Loss: 0.9373\n",
            "Epoch [8/20], Step [42/142], Loss: 0.8922\n",
            "Epoch [8/20], Step [43/142], Loss: 0.8133\n",
            "Epoch [8/20], Step [44/142], Loss: 0.8618\n",
            "Epoch [8/20], Step [45/142], Loss: 0.7699\n",
            "Epoch [8/20], Step [46/142], Loss: 0.8666\n",
            "Epoch [8/20], Step [47/142], Loss: 0.7977\n",
            "Epoch [8/20], Step [48/142], Loss: 0.9039\n",
            "Epoch [8/20], Step [49/142], Loss: 0.8475\n",
            "Epoch [8/20], Step [50/142], Loss: 0.8355\n",
            "Epoch [8/20], Step [51/142], Loss: 0.7671\n",
            "Epoch [8/20], Step [52/142], Loss: 0.7942\n",
            "Epoch [8/20], Step [53/142], Loss: 0.8897\n",
            "Epoch [8/20], Step [54/142], Loss: 0.9007\n",
            "Epoch [8/20], Step [55/142], Loss: 0.8000\n",
            "Epoch [8/20], Step [56/142], Loss: 0.9462\n",
            "Epoch [8/20], Step [57/142], Loss: 0.7665\n",
            "Epoch [8/20], Step [58/142], Loss: 0.8287\n",
            "Epoch [8/20], Step [59/142], Loss: 0.8605\n",
            "Epoch [8/20], Step [60/142], Loss: 0.8740\n",
            "Epoch [8/20], Step [61/142], Loss: 0.8371\n",
            "Epoch [8/20], Step [62/142], Loss: 0.8567\n",
            "Epoch [8/20], Step [63/142], Loss: 0.8528\n",
            "Epoch [8/20], Step [64/142], Loss: 0.7876\n",
            "Epoch [8/20], Step [65/142], Loss: 0.7790\n",
            "Epoch [8/20], Step [66/142], Loss: 0.9300\n",
            "Epoch [8/20], Step [67/142], Loss: 0.8535\n",
            "Epoch [8/20], Step [68/142], Loss: 0.7881\n",
            "Epoch [8/20], Step [69/142], Loss: 0.8970\n",
            "Epoch [8/20], Step [70/142], Loss: 0.8717\n",
            "Epoch [8/20], Step [71/142], Loss: 0.8805\n",
            "Epoch [8/20], Step [72/142], Loss: 0.8575\n",
            "Epoch [8/20], Step [73/142], Loss: 0.9344\n",
            "Epoch [8/20], Step [74/142], Loss: 0.8713\n",
            "Epoch [8/20], Step [75/142], Loss: 0.8807\n",
            "Epoch [8/20], Step [76/142], Loss: 0.8619\n",
            "Epoch [8/20], Step [77/142], Loss: 0.8169\n",
            "Epoch [8/20], Step [78/142], Loss: 0.8845\n",
            "Epoch [8/20], Step [79/142], Loss: 0.7500\n",
            "Epoch [8/20], Step [80/142], Loss: 0.8551\n",
            "Epoch [8/20], Step [81/142], Loss: 0.9959\n",
            "Epoch [8/20], Step [82/142], Loss: 0.7877\n",
            "Epoch [8/20], Step [83/142], Loss: 0.7547\n",
            "Epoch [8/20], Step [84/142], Loss: 0.8454\n",
            "Epoch [8/20], Step [85/142], Loss: 0.7956\n",
            "Epoch [8/20], Step [86/142], Loss: 0.8382\n",
            "Epoch [8/20], Step [87/142], Loss: 0.9096\n",
            "Epoch [8/20], Step [88/142], Loss: 0.8630\n",
            "Epoch [8/20], Step [89/142], Loss: 0.8416\n",
            "Epoch [8/20], Step [90/142], Loss: 0.8732\n",
            "Epoch [8/20], Step [91/142], Loss: 0.9153\n",
            "Epoch [8/20], Step [92/142], Loss: 0.8946\n",
            "Epoch [8/20], Step [93/142], Loss: 0.8740\n",
            "Epoch [8/20], Step [94/142], Loss: 0.9168\n",
            "Epoch [8/20], Step [95/142], Loss: 0.7636\n",
            "Epoch [8/20], Step [96/142], Loss: 0.7562\n",
            "Epoch [8/20], Step [97/142], Loss: 0.8389\n",
            "Epoch [8/20], Step [98/142], Loss: 0.8696\n",
            "Epoch [8/20], Step [99/142], Loss: 0.8642\n",
            "Epoch [8/20], Step [100/142], Loss: 0.8049\n",
            "Epoch [8/20], Step [101/142], Loss: 0.8163\n",
            "Epoch [8/20], Step [102/142], Loss: 0.9188\n",
            "Epoch [8/20], Step [103/142], Loss: 0.8043\n",
            "Epoch [8/20], Step [104/142], Loss: 0.7466\n",
            "Epoch [8/20], Step [105/142], Loss: 0.7878\n",
            "Epoch [8/20], Step [106/142], Loss: 0.8856\n",
            "Epoch [8/20], Step [107/142], Loss: 0.7477\n",
            "Epoch [8/20], Step [108/142], Loss: 0.8392\n",
            "Epoch [8/20], Step [109/142], Loss: 0.8102\n",
            "Epoch [8/20], Step [110/142], Loss: 0.9201\n",
            "Epoch [8/20], Step [111/142], Loss: 0.8379\n",
            "Epoch [8/20], Step [112/142], Loss: 0.8438\n",
            "Epoch [8/20], Step [113/142], Loss: 0.7524\n",
            "Epoch [8/20], Step [114/142], Loss: 0.8316\n",
            "Epoch [8/20], Step [115/142], Loss: 0.8496\n",
            "Epoch [8/20], Step [116/142], Loss: 0.7997\n",
            "Epoch [8/20], Step [117/142], Loss: 0.8620\n",
            "Epoch [8/20], Step [118/142], Loss: 0.8931\n",
            "Epoch [8/20], Step [119/142], Loss: 0.9154\n",
            "Epoch [8/20], Step [120/142], Loss: 0.8587\n",
            "Epoch [8/20], Step [121/142], Loss: 0.8514\n",
            "Epoch [8/20], Step [122/142], Loss: 0.8685\n",
            "Epoch [8/20], Step [123/142], Loss: 0.7998\n",
            "Epoch [8/20], Step [124/142], Loss: 0.7966\n",
            "Epoch [8/20], Step [125/142], Loss: 0.8090\n",
            "Epoch [8/20], Step [126/142], Loss: 0.8172\n",
            "Epoch [8/20], Step [127/142], Loss: 0.9182\n",
            "Epoch [8/20], Step [128/142], Loss: 0.8486\n",
            "Epoch [8/20], Step [129/142], Loss: 0.7415\n",
            "Epoch [8/20], Step [130/142], Loss: 0.9330\n",
            "Epoch [8/20], Step [131/142], Loss: 0.8065\n",
            "Epoch [8/20], Step [132/142], Loss: 0.8470\n",
            "Epoch [8/20], Step [133/142], Loss: 0.7360\n",
            "Epoch [8/20], Step [134/142], Loss: 0.9562\n",
            "Epoch [8/20], Step [135/142], Loss: 0.7999\n",
            "Epoch [8/20], Step [136/142], Loss: 0.8289\n",
            "Epoch [8/20], Step [137/142], Loss: 0.8600\n",
            "Epoch [8/20], Step [138/142], Loss: 0.7561\n",
            "Epoch [8/20], Step [139/142], Loss: 0.8523\n",
            "Epoch [8/20], Step [140/142], Loss: 0.8680\n",
            "Epoch [8/20], Step [141/142], Loss: 0.9052\n",
            "Epoch [8/20], Step [142/142], Loss: 1.0313\n",
            "Epoch [9/20], Step [1/142], Loss: 0.8245\n",
            "Epoch [9/20], Step [2/142], Loss: 0.7009\n",
            "Epoch [9/20], Step [3/142], Loss: 0.8520\n",
            "Epoch [9/20], Step [4/142], Loss: 0.9753\n",
            "Epoch [9/20], Step [5/142], Loss: 0.7621\n",
            "Epoch [9/20], Step [6/142], Loss: 0.8232\n",
            "Epoch [9/20], Step [7/142], Loss: 0.8572\n",
            "Epoch [9/20], Step [8/142], Loss: 0.8557\n",
            "Epoch [9/20], Step [9/142], Loss: 0.7767\n",
            "Epoch [9/20], Step [10/142], Loss: 0.9058\n",
            "Epoch [9/20], Step [11/142], Loss: 0.8474\n",
            "Epoch [9/20], Step [12/142], Loss: 0.8452\n",
            "Epoch [9/20], Step [13/142], Loss: 0.8953\n",
            "Epoch [9/20], Step [14/142], Loss: 0.8669\n",
            "Epoch [9/20], Step [15/142], Loss: 0.9055\n",
            "Epoch [9/20], Step [16/142], Loss: 0.8063\n",
            "Epoch [9/20], Step [17/142], Loss: 0.8462\n",
            "Epoch [9/20], Step [18/142], Loss: 0.8479\n",
            "Epoch [9/20], Step [19/142], Loss: 0.7215\n",
            "Epoch [9/20], Step [20/142], Loss: 0.8065\n",
            "Epoch [9/20], Step [21/142], Loss: 0.8509\n",
            "Epoch [9/20], Step [22/142], Loss: 0.8058\n",
            "Epoch [9/20], Step [23/142], Loss: 0.9124\n",
            "Epoch [9/20], Step [24/142], Loss: 0.7918\n",
            "Epoch [9/20], Step [25/142], Loss: 0.8883\n",
            "Epoch [9/20], Step [26/142], Loss: 0.8085\n",
            "Epoch [9/20], Step [27/142], Loss: 0.8703\n",
            "Epoch [9/20], Step [28/142], Loss: 0.8165\n",
            "Epoch [9/20], Step [29/142], Loss: 0.8304\n",
            "Epoch [9/20], Step [30/142], Loss: 0.8407\n",
            "Epoch [9/20], Step [31/142], Loss: 0.9939\n",
            "Epoch [9/20], Step [32/142], Loss: 0.8422\n",
            "Epoch [9/20], Step [33/142], Loss: 0.9020\n",
            "Epoch [9/20], Step [34/142], Loss: 0.8671\n",
            "Epoch [9/20], Step [35/142], Loss: 0.8766\n",
            "Epoch [9/20], Step [36/142], Loss: 0.7322\n",
            "Epoch [9/20], Step [37/142], Loss: 0.7745\n",
            "Epoch [9/20], Step [38/142], Loss: 0.7949\n",
            "Epoch [9/20], Step [39/142], Loss: 0.8448\n",
            "Epoch [9/20], Step [40/142], Loss: 0.8703\n",
            "Epoch [9/20], Step [41/142], Loss: 0.8529\n",
            "Epoch [9/20], Step [42/142], Loss: 0.9039\n",
            "Epoch [9/20], Step [43/142], Loss: 0.8670\n",
            "Epoch [9/20], Step [44/142], Loss: 0.8320\n",
            "Epoch [9/20], Step [45/142], Loss: 0.8486\n",
            "Epoch [9/20], Step [46/142], Loss: 0.8395\n",
            "Epoch [9/20], Step [47/142], Loss: 0.8767\n",
            "Epoch [9/20], Step [48/142], Loss: 0.8515\n",
            "Epoch [9/20], Step [49/142], Loss: 0.9204\n",
            "Epoch [9/20], Step [50/142], Loss: 0.9125\n",
            "Epoch [9/20], Step [51/142], Loss: 0.9065\n",
            "Epoch [9/20], Step [52/142], Loss: 0.8337\n",
            "Epoch [9/20], Step [53/142], Loss: 0.8385\n",
            "Epoch [9/20], Step [54/142], Loss: 0.9283\n",
            "Epoch [9/20], Step [55/142], Loss: 0.7293\n",
            "Epoch [9/20], Step [56/142], Loss: 0.9187\n",
            "Epoch [9/20], Step [57/142], Loss: 0.8605\n",
            "Epoch [9/20], Step [58/142], Loss: 0.8443\n",
            "Epoch [9/20], Step [59/142], Loss: 0.7510\n",
            "Epoch [9/20], Step [60/142], Loss: 0.8849\n",
            "Epoch [9/20], Step [61/142], Loss: 0.7461\n",
            "Epoch [9/20], Step [62/142], Loss: 0.8575\n",
            "Epoch [9/20], Step [63/142], Loss: 0.9132\n",
            "Epoch [9/20], Step [64/142], Loss: 0.7393\n",
            "Epoch [9/20], Step [65/142], Loss: 0.7160\n",
            "Epoch [9/20], Step [66/142], Loss: 0.8572\n",
            "Epoch [9/20], Step [67/142], Loss: 0.9495\n",
            "Epoch [9/20], Step [68/142], Loss: 0.9019\n",
            "Epoch [9/20], Step [69/142], Loss: 0.9565\n",
            "Epoch [9/20], Step [70/142], Loss: 0.8382\n",
            "Epoch [9/20], Step [71/142], Loss: 0.8390\n",
            "Epoch [9/20], Step [72/142], Loss: 0.8580\n",
            "Epoch [9/20], Step [73/142], Loss: 0.7757\n",
            "Epoch [9/20], Step [74/142], Loss: 0.9398\n",
            "Epoch [9/20], Step [75/142], Loss: 0.8203\n",
            "Epoch [9/20], Step [76/142], Loss: 0.8123\n",
            "Epoch [9/20], Step [77/142], Loss: 0.8543\n",
            "Epoch [9/20], Step [78/142], Loss: 0.8731\n",
            "Epoch [9/20], Step [79/142], Loss: 0.7203\n",
            "Epoch [9/20], Step [80/142], Loss: 0.9109\n",
            "Epoch [9/20], Step [81/142], Loss: 0.8113\n",
            "Epoch [9/20], Step [82/142], Loss: 0.9121\n",
            "Epoch [9/20], Step [83/142], Loss: 0.8119\n",
            "Epoch [9/20], Step [84/142], Loss: 0.8230\n",
            "Epoch [9/20], Step [85/142], Loss: 0.8312\n",
            "Epoch [9/20], Step [86/142], Loss: 0.9058\n",
            "Epoch [9/20], Step [87/142], Loss: 0.8033\n",
            "Epoch [9/20], Step [88/142], Loss: 0.9083\n",
            "Epoch [9/20], Step [89/142], Loss: 0.7643\n",
            "Epoch [9/20], Step [90/142], Loss: 0.8245\n",
            "Epoch [9/20], Step [91/142], Loss: 0.7992\n",
            "Epoch [9/20], Step [92/142], Loss: 0.8922\n",
            "Epoch [9/20], Step [93/142], Loss: 0.8461\n",
            "Epoch [9/20], Step [94/142], Loss: 0.8375\n",
            "Epoch [9/20], Step [95/142], Loss: 0.8455\n",
            "Epoch [9/20], Step [96/142], Loss: 0.8228\n",
            "Epoch [9/20], Step [97/142], Loss: 0.9122\n",
            "Epoch [9/20], Step [98/142], Loss: 0.7791\n",
            "Epoch [9/20], Step [99/142], Loss: 0.9059\n",
            "Epoch [9/20], Step [100/142], Loss: 0.7883\n",
            "Epoch [9/20], Step [101/142], Loss: 0.7911\n",
            "Epoch [9/20], Step [102/142], Loss: 0.8019\n",
            "Epoch [9/20], Step [103/142], Loss: 0.8914\n",
            "Epoch [9/20], Step [104/142], Loss: 0.8609\n",
            "Epoch [9/20], Step [105/142], Loss: 0.8455\n",
            "Epoch [9/20], Step [106/142], Loss: 0.7789\n",
            "Epoch [9/20], Step [107/142], Loss: 0.8867\n",
            "Epoch [9/20], Step [108/142], Loss: 0.7492\n",
            "Epoch [9/20], Step [109/142], Loss: 0.8737\n",
            "Epoch [9/20], Step [110/142], Loss: 0.8496\n",
            "Epoch [9/20], Step [111/142], Loss: 0.8570\n",
            "Epoch [9/20], Step [112/142], Loss: 0.8185\n",
            "Epoch [9/20], Step [113/142], Loss: 0.8657\n",
            "Epoch [9/20], Step [114/142], Loss: 0.9216\n",
            "Epoch [9/20], Step [115/142], Loss: 0.8341\n",
            "Epoch [9/20], Step [116/142], Loss: 0.9459\n",
            "Epoch [9/20], Step [117/142], Loss: 0.8482\n",
            "Epoch [9/20], Step [118/142], Loss: 0.7863\n",
            "Epoch [9/20], Step [119/142], Loss: 0.8354\n",
            "Epoch [9/20], Step [120/142], Loss: 0.8133\n",
            "Epoch [9/20], Step [121/142], Loss: 0.8246\n",
            "Epoch [9/20], Step [122/142], Loss: 0.8456\n",
            "Epoch [9/20], Step [123/142], Loss: 0.8830\n",
            "Epoch [9/20], Step [124/142], Loss: 0.8610\n",
            "Epoch [9/20], Step [125/142], Loss: 0.8481\n",
            "Epoch [9/20], Step [126/142], Loss: 0.8838\n",
            "Epoch [9/20], Step [127/142], Loss: 0.7534\n",
            "Epoch [9/20], Step [128/142], Loss: 0.8091\n",
            "Epoch [9/20], Step [129/142], Loss: 0.8584\n",
            "Epoch [9/20], Step [130/142], Loss: 0.8568\n",
            "Epoch [9/20], Step [131/142], Loss: 0.7777\n",
            "Epoch [9/20], Step [132/142], Loss: 0.8209\n",
            "Epoch [9/20], Step [133/142], Loss: 0.8070\n",
            "Epoch [9/20], Step [134/142], Loss: 0.9238\n",
            "Epoch [9/20], Step [135/142], Loss: 0.8264\n",
            "Epoch [9/20], Step [136/142], Loss: 0.8104\n",
            "Epoch [9/20], Step [137/142], Loss: 0.8534\n",
            "Epoch [9/20], Step [138/142], Loss: 0.8662\n",
            "Epoch [9/20], Step [139/142], Loss: 0.8231\n",
            "Epoch [9/20], Step [140/142], Loss: 0.8105\n",
            "Epoch [9/20], Step [141/142], Loss: 0.8773\n",
            "Epoch [9/20], Step [142/142], Loss: 0.8785\n",
            "Epoch [10/20], Step [1/142], Loss: 0.8455\n",
            "Epoch [10/20], Step [2/142], Loss: 0.9231\n",
            "Epoch [10/20], Step [3/142], Loss: 0.7802\n",
            "Epoch [10/20], Step [4/142], Loss: 0.8327\n",
            "Epoch [10/20], Step [5/142], Loss: 0.9016\n",
            "Epoch [10/20], Step [6/142], Loss: 0.8915\n",
            "Epoch [10/20], Step [7/142], Loss: 0.8790\n",
            "Epoch [10/20], Step [8/142], Loss: 0.8236\n",
            "Epoch [10/20], Step [9/142], Loss: 0.7855\n",
            "Epoch [10/20], Step [10/142], Loss: 0.8280\n",
            "Epoch [10/20], Step [11/142], Loss: 0.8591\n",
            "Epoch [10/20], Step [12/142], Loss: 0.8173\n",
            "Epoch [10/20], Step [13/142], Loss: 0.9067\n",
            "Epoch [10/20], Step [14/142], Loss: 0.7965\n",
            "Epoch [10/20], Step [15/142], Loss: 0.9265\n",
            "Epoch [10/20], Step [16/142], Loss: 0.8565\n",
            "Epoch [10/20], Step [17/142], Loss: 0.8459\n",
            "Epoch [10/20], Step [18/142], Loss: 0.8547\n",
            "Epoch [10/20], Step [19/142], Loss: 0.9229\n",
            "Epoch [10/20], Step [20/142], Loss: 0.8696\n",
            "Epoch [10/20], Step [21/142], Loss: 0.7999\n",
            "Epoch [10/20], Step [22/142], Loss: 0.8030\n",
            "Epoch [10/20], Step [23/142], Loss: 0.8417\n",
            "Epoch [10/20], Step [24/142], Loss: 0.9199\n",
            "Epoch [10/20], Step [25/142], Loss: 0.7966\n",
            "Epoch [10/20], Step [26/142], Loss: 0.7686\n",
            "Epoch [10/20], Step [27/142], Loss: 0.8783\n",
            "Epoch [10/20], Step [28/142], Loss: 0.8577\n",
            "Epoch [10/20], Step [29/142], Loss: 0.8671\n",
            "Epoch [10/20], Step [30/142], Loss: 0.7806\n",
            "Epoch [10/20], Step [31/142], Loss: 0.7612\n",
            "Epoch [10/20], Step [32/142], Loss: 0.8165\n",
            "Epoch [10/20], Step [33/142], Loss: 0.8050\n",
            "Epoch [10/20], Step [34/142], Loss: 0.9033\n",
            "Epoch [10/20], Step [35/142], Loss: 0.8158\n",
            "Epoch [10/20], Step [36/142], Loss: 0.8746\n",
            "Epoch [10/20], Step [37/142], Loss: 0.8212\n",
            "Epoch [10/20], Step [38/142], Loss: 0.8720\n",
            "Epoch [10/20], Step [39/142], Loss: 0.8059\n",
            "Epoch [10/20], Step [40/142], Loss: 0.8990\n",
            "Epoch [10/20], Step [41/142], Loss: 0.8410\n",
            "Epoch [10/20], Step [42/142], Loss: 0.8511\n",
            "Epoch [10/20], Step [43/142], Loss: 0.8859\n",
            "Epoch [10/20], Step [44/142], Loss: 0.9635\n",
            "Epoch [10/20], Step [45/142], Loss: 0.8352\n",
            "Epoch [10/20], Step [46/142], Loss: 0.7089\n",
            "Epoch [10/20], Step [47/142], Loss: 0.7808\n",
            "Epoch [10/20], Step [48/142], Loss: 0.8208\n",
            "Epoch [10/20], Step [49/142], Loss: 0.9159\n",
            "Epoch [10/20], Step [50/142], Loss: 0.9010\n",
            "Epoch [10/20], Step [51/142], Loss: 0.8580\n",
            "Epoch [10/20], Step [52/142], Loss: 0.8126\n",
            "Epoch [10/20], Step [53/142], Loss: 0.7602\n",
            "Epoch [10/20], Step [54/142], Loss: 0.8448\n",
            "Epoch [10/20], Step [55/142], Loss: 0.8319\n",
            "Epoch [10/20], Step [56/142], Loss: 0.9544\n",
            "Epoch [10/20], Step [57/142], Loss: 0.8272\n",
            "Epoch [10/20], Step [58/142], Loss: 0.8462\n",
            "Epoch [10/20], Step [59/142], Loss: 0.8995\n",
            "Epoch [10/20], Step [60/142], Loss: 0.9020\n",
            "Epoch [10/20], Step [61/142], Loss: 0.9003\n",
            "Epoch [10/20], Step [62/142], Loss: 0.7541\n",
            "Epoch [10/20], Step [63/142], Loss: 0.9224\n",
            "Epoch [10/20], Step [64/142], Loss: 0.7892\n",
            "Epoch [10/20], Step [65/142], Loss: 0.8489\n",
            "Epoch [10/20], Step [66/142], Loss: 0.7805\n",
            "Epoch [10/20], Step [67/142], Loss: 0.8010\n",
            "Epoch [10/20], Step [68/142], Loss: 0.7442\n",
            "Epoch [10/20], Step [69/142], Loss: 0.8243\n",
            "Epoch [10/20], Step [70/142], Loss: 0.8499\n",
            "Epoch [10/20], Step [71/142], Loss: 0.7772\n",
            "Epoch [10/20], Step [72/142], Loss: 0.9205\n",
            "Epoch [10/20], Step [73/142], Loss: 0.8345\n",
            "Epoch [10/20], Step [74/142], Loss: 0.7955\n",
            "Epoch [10/20], Step [75/142], Loss: 0.9476\n",
            "Epoch [10/20], Step [76/142], Loss: 0.9013\n",
            "Epoch [10/20], Step [77/142], Loss: 0.8491\n",
            "Epoch [10/20], Step [78/142], Loss: 0.8450\n",
            "Epoch [10/20], Step [79/142], Loss: 0.7708\n",
            "Epoch [10/20], Step [80/142], Loss: 0.8822\n",
            "Epoch [10/20], Step [81/142], Loss: 0.8669\n",
            "Epoch [10/20], Step [82/142], Loss: 0.8717\n",
            "Epoch [10/20], Step [83/142], Loss: 0.9484\n",
            "Epoch [10/20], Step [84/142], Loss: 0.8076\n",
            "Epoch [10/20], Step [85/142], Loss: 0.8834\n",
            "Epoch [10/20], Step [86/142], Loss: 0.7981\n",
            "Epoch [10/20], Step [87/142], Loss: 0.8836\n",
            "Epoch [10/20], Step [88/142], Loss: 0.8560\n",
            "Epoch [10/20], Step [89/142], Loss: 0.8959\n",
            "Epoch [10/20], Step [90/142], Loss: 0.8827\n",
            "Epoch [10/20], Step [91/142], Loss: 0.8381\n",
            "Epoch [10/20], Step [92/142], Loss: 0.8247\n",
            "Epoch [10/20], Step [93/142], Loss: 0.7841\n",
            "Epoch [10/20], Step [94/142], Loss: 0.7926\n",
            "Epoch [10/20], Step [95/142], Loss: 0.9092\n",
            "Epoch [10/20], Step [96/142], Loss: 0.8993\n",
            "Epoch [10/20], Step [97/142], Loss: 0.9217\n",
            "Epoch [10/20], Step [98/142], Loss: 0.8856\n",
            "Epoch [10/20], Step [99/142], Loss: 0.8995\n",
            "Epoch [10/20], Step [100/142], Loss: 0.7432\n",
            "Epoch [10/20], Step [101/142], Loss: 0.8806\n",
            "Epoch [10/20], Step [102/142], Loss: 0.8555\n",
            "Epoch [10/20], Step [103/142], Loss: 0.8917\n",
            "Epoch [10/20], Step [104/142], Loss: 0.7684\n",
            "Epoch [10/20], Step [105/142], Loss: 0.8246\n",
            "Epoch [10/20], Step [106/142], Loss: 0.8459\n",
            "Epoch [10/20], Step [107/142], Loss: 0.8381\n",
            "Epoch [10/20], Step [108/142], Loss: 0.7845\n",
            "Epoch [10/20], Step [109/142], Loss: 0.8675\n",
            "Epoch [10/20], Step [110/142], Loss: 0.8583\n",
            "Epoch [10/20], Step [111/142], Loss: 0.8387\n",
            "Epoch [10/20], Step [112/142], Loss: 0.8486\n",
            "Epoch [10/20], Step [113/142], Loss: 0.8496\n",
            "Epoch [10/20], Step [114/142], Loss: 0.7389\n",
            "Epoch [10/20], Step [115/142], Loss: 0.9116\n",
            "Epoch [10/20], Step [116/142], Loss: 0.8231\n",
            "Epoch [10/20], Step [117/142], Loss: 0.8124\n",
            "Epoch [10/20], Step [118/142], Loss: 0.7905\n",
            "Epoch [10/20], Step [119/142], Loss: 0.8446\n",
            "Epoch [10/20], Step [120/142], Loss: 0.8470\n",
            "Epoch [10/20], Step [121/142], Loss: 0.8058\n",
            "Epoch [10/20], Step [122/142], Loss: 0.8211\n",
            "Epoch [10/20], Step [123/142], Loss: 0.7972\n",
            "Epoch [10/20], Step [124/142], Loss: 0.7509\n",
            "Epoch [10/20], Step [125/142], Loss: 0.7949\n",
            "Epoch [10/20], Step [126/142], Loss: 0.8097\n",
            "Epoch [10/20], Step [127/142], Loss: 0.9399\n",
            "Epoch [10/20], Step [128/142], Loss: 0.8864\n",
            "Epoch [10/20], Step [129/142], Loss: 0.8835\n",
            "Epoch [10/20], Step [130/142], Loss: 0.8586\n",
            "Epoch [10/20], Step [131/142], Loss: 0.7680\n",
            "Epoch [10/20], Step [132/142], Loss: 0.8637\n",
            "Epoch [10/20], Step [133/142], Loss: 0.8696\n",
            "Epoch [10/20], Step [134/142], Loss: 0.7739\n",
            "Epoch [10/20], Step [135/142], Loss: 0.8728\n",
            "Epoch [10/20], Step [136/142], Loss: 0.8859\n",
            "Epoch [10/20], Step [137/142], Loss: 0.8225\n",
            "Epoch [10/20], Step [138/142], Loss: 0.7520\n",
            "Epoch [10/20], Step [139/142], Loss: 0.8435\n",
            "Epoch [10/20], Step [140/142], Loss: 0.8027\n",
            "Epoch [10/20], Step [141/142], Loss: 0.8535\n",
            "Epoch [10/20], Step [142/142], Loss: 0.8205\n",
            "Epoch [11/20], Step [1/142], Loss: 0.7779\n",
            "Epoch [11/20], Step [2/142], Loss: 0.8870\n",
            "Epoch [11/20], Step [3/142], Loss: 0.8962\n",
            "Epoch [11/20], Step [4/142], Loss: 0.8584\n",
            "Epoch [11/20], Step [5/142], Loss: 0.7897\n",
            "Epoch [11/20], Step [6/142], Loss: 0.8637\n",
            "Epoch [11/20], Step [7/142], Loss: 0.8444\n",
            "Epoch [11/20], Step [8/142], Loss: 0.8043\n",
            "Epoch [11/20], Step [9/142], Loss: 0.8842\n",
            "Epoch [11/20], Step [10/142], Loss: 0.8543\n",
            "Epoch [11/20], Step [11/142], Loss: 0.7701\n",
            "Epoch [11/20], Step [12/142], Loss: 0.8515\n",
            "Epoch [11/20], Step [13/142], Loss: 0.7515\n",
            "Epoch [11/20], Step [14/142], Loss: 0.8077\n",
            "Epoch [11/20], Step [15/142], Loss: 0.9025\n",
            "Epoch [11/20], Step [16/142], Loss: 0.8331\n",
            "Epoch [11/20], Step [17/142], Loss: 0.7882\n",
            "Epoch [11/20], Step [18/142], Loss: 0.8348\n",
            "Epoch [11/20], Step [19/142], Loss: 0.9385\n",
            "Epoch [11/20], Step [20/142], Loss: 0.8741\n",
            "Epoch [11/20], Step [21/142], Loss: 0.8326\n",
            "Epoch [11/20], Step [22/142], Loss: 0.7511\n",
            "Epoch [11/20], Step [23/142], Loss: 0.8725\n",
            "Epoch [11/20], Step [24/142], Loss: 0.8903\n",
            "Epoch [11/20], Step [25/142], Loss: 0.8033\n",
            "Epoch [11/20], Step [26/142], Loss: 0.8744\n",
            "Epoch [11/20], Step [27/142], Loss: 0.8448\n",
            "Epoch [11/20], Step [28/142], Loss: 0.9069\n",
            "Epoch [11/20], Step [29/142], Loss: 0.7985\n",
            "Epoch [11/20], Step [30/142], Loss: 0.8294\n",
            "Epoch [11/20], Step [31/142], Loss: 0.8562\n",
            "Epoch [11/20], Step [32/142], Loss: 0.7679\n",
            "Epoch [11/20], Step [33/142], Loss: 0.8148\n",
            "Epoch [11/20], Step [34/142], Loss: 0.8385\n",
            "Epoch [11/20], Step [35/142], Loss: 0.8705\n",
            "Epoch [11/20], Step [36/142], Loss: 0.8590\n",
            "Epoch [11/20], Step [37/142], Loss: 0.7698\n",
            "Epoch [11/20], Step [38/142], Loss: 0.8927\n",
            "Epoch [11/20], Step [39/142], Loss: 0.9065\n",
            "Epoch [11/20], Step [40/142], Loss: 0.8133\n",
            "Epoch [11/20], Step [41/142], Loss: 0.8474\n",
            "Epoch [11/20], Step [42/142], Loss: 0.8342\n",
            "Epoch [11/20], Step [43/142], Loss: 0.7441\n",
            "Epoch [11/20], Step [44/142], Loss: 0.8505\n",
            "Epoch [11/20], Step [45/142], Loss: 0.8591\n",
            "Epoch [11/20], Step [46/142], Loss: 0.9153\n",
            "Epoch [11/20], Step [47/142], Loss: 0.7871\n",
            "Epoch [11/20], Step [48/142], Loss: 0.8356\n",
            "Epoch [11/20], Step [49/142], Loss: 0.8425\n",
            "Epoch [11/20], Step [50/142], Loss: 0.8710\n",
            "Epoch [11/20], Step [51/142], Loss: 0.8677\n",
            "Epoch [11/20], Step [52/142], Loss: 0.8491\n",
            "Epoch [11/20], Step [53/142], Loss: 0.9014\n",
            "Epoch [11/20], Step [54/142], Loss: 0.7918\n",
            "Epoch [11/20], Step [55/142], Loss: 0.8531\n",
            "Epoch [11/20], Step [56/142], Loss: 0.7491\n",
            "Epoch [11/20], Step [57/142], Loss: 0.7799\n",
            "Epoch [11/20], Step [58/142], Loss: 0.8305\n",
            "Epoch [11/20], Step [59/142], Loss: 0.8856\n",
            "Epoch [11/20], Step [60/142], Loss: 0.8018\n",
            "Epoch [11/20], Step [61/142], Loss: 0.8315\n",
            "Epoch [11/20], Step [62/142], Loss: 0.8060\n",
            "Epoch [11/20], Step [63/142], Loss: 0.8589\n",
            "Epoch [11/20], Step [64/142], Loss: 0.8026\n",
            "Epoch [11/20], Step [65/142], Loss: 0.8282\n",
            "Epoch [11/20], Step [66/142], Loss: 0.8606\n",
            "Epoch [11/20], Step [67/142], Loss: 0.8444\n",
            "Epoch [11/20], Step [68/142], Loss: 0.8890\n",
            "Epoch [11/20], Step [69/142], Loss: 0.8850\n",
            "Epoch [11/20], Step [70/142], Loss: 0.8326\n",
            "Epoch [11/20], Step [71/142], Loss: 0.8717\n",
            "Epoch [11/20], Step [72/142], Loss: 0.8163\n",
            "Epoch [11/20], Step [73/142], Loss: 0.8340\n",
            "Epoch [11/20], Step [74/142], Loss: 0.8876\n",
            "Epoch [11/20], Step [75/142], Loss: 0.8226\n",
            "Epoch [11/20], Step [76/142], Loss: 0.8010\n",
            "Epoch [11/20], Step [77/142], Loss: 0.8508\n",
            "Epoch [11/20], Step [78/142], Loss: 0.7727\n",
            "Epoch [11/20], Step [79/142], Loss: 0.7951\n",
            "Epoch [11/20], Step [80/142], Loss: 0.7763\n",
            "Epoch [11/20], Step [81/142], Loss: 0.8101\n",
            "Epoch [11/20], Step [82/142], Loss: 0.8766\n",
            "Epoch [11/20], Step [83/142], Loss: 0.9007\n",
            "Epoch [11/20], Step [84/142], Loss: 0.9146\n",
            "Epoch [11/20], Step [85/142], Loss: 0.8562\n",
            "Epoch [11/20], Step [86/142], Loss: 0.8998\n",
            "Epoch [11/20], Step [87/142], Loss: 0.8698\n",
            "Epoch [11/20], Step [88/142], Loss: 0.7911\n",
            "Epoch [11/20], Step [89/142], Loss: 0.9124\n",
            "Epoch [11/20], Step [90/142], Loss: 0.8320\n",
            "Epoch [11/20], Step [91/142], Loss: 0.8405\n",
            "Epoch [11/20], Step [92/142], Loss: 0.8163\n",
            "Epoch [11/20], Step [93/142], Loss: 0.7802\n",
            "Epoch [11/20], Step [94/142], Loss: 0.8607\n",
            "Epoch [11/20], Step [95/142], Loss: 0.9457\n",
            "Epoch [11/20], Step [96/142], Loss: 0.9388\n",
            "Epoch [11/20], Step [97/142], Loss: 0.8583\n",
            "Epoch [11/20], Step [98/142], Loss: 0.8480\n",
            "Epoch [11/20], Step [99/142], Loss: 0.8265\n",
            "Epoch [11/20], Step [100/142], Loss: 0.8982\n",
            "Epoch [11/20], Step [101/142], Loss: 0.7618\n",
            "Epoch [11/20], Step [102/142], Loss: 0.8092\n",
            "Epoch [11/20], Step [103/142], Loss: 0.9259\n",
            "Epoch [11/20], Step [104/142], Loss: 0.8341\n",
            "Epoch [11/20], Step [105/142], Loss: 0.8587\n",
            "Epoch [11/20], Step [106/142], Loss: 0.9806\n",
            "Epoch [11/20], Step [107/142], Loss: 0.7812\n",
            "Epoch [11/20], Step [108/142], Loss: 0.8718\n",
            "Epoch [11/20], Step [109/142], Loss: 0.7803\n",
            "Epoch [11/20], Step [110/142], Loss: 0.8897\n",
            "Epoch [11/20], Step [111/142], Loss: 0.8212\n",
            "Epoch [11/20], Step [112/142], Loss: 0.8715\n",
            "Epoch [11/20], Step [113/142], Loss: 0.8373\n",
            "Epoch [11/20], Step [114/142], Loss: 0.7711\n",
            "Epoch [11/20], Step [115/142], Loss: 0.8968\n",
            "Epoch [11/20], Step [116/142], Loss: 0.8077\n",
            "Epoch [11/20], Step [117/142], Loss: 0.8270\n",
            "Epoch [11/20], Step [118/142], Loss: 0.9016\n",
            "Epoch [11/20], Step [119/142], Loss: 0.8523\n",
            "Epoch [11/20], Step [120/142], Loss: 0.9370\n",
            "Epoch [11/20], Step [121/142], Loss: 0.8605\n",
            "Epoch [11/20], Step [122/142], Loss: 0.8526\n",
            "Epoch [11/20], Step [123/142], Loss: 0.7749\n",
            "Epoch [11/20], Step [124/142], Loss: 0.9331\n",
            "Epoch [11/20], Step [125/142], Loss: 0.8379\n",
            "Epoch [11/20], Step [126/142], Loss: 0.9785\n",
            "Epoch [11/20], Step [127/142], Loss: 0.7326\n",
            "Epoch [11/20], Step [128/142], Loss: 0.8975\n",
            "Epoch [11/20], Step [129/142], Loss: 0.8758\n",
            "Epoch [11/20], Step [130/142], Loss: 0.8240\n",
            "Epoch [11/20], Step [131/142], Loss: 0.7962\n",
            "Epoch [11/20], Step [132/142], Loss: 0.7793\n",
            "Epoch [11/20], Step [133/142], Loss: 0.7794\n",
            "Epoch [11/20], Step [134/142], Loss: 0.8992\n",
            "Epoch [11/20], Step [135/142], Loss: 0.8261\n",
            "Epoch [11/20], Step [136/142], Loss: 0.9718\n",
            "Epoch [11/20], Step [137/142], Loss: 0.9070\n",
            "Epoch [11/20], Step [138/142], Loss: 0.8812\n",
            "Epoch [11/20], Step [139/142], Loss: 0.8706\n",
            "Epoch [11/20], Step [140/142], Loss: 0.8535\n",
            "Epoch [11/20], Step [141/142], Loss: 0.7754\n",
            "Epoch [11/20], Step [142/142], Loss: 0.7538\n",
            "Epoch [12/20], Step [1/142], Loss: 0.8619\n",
            "Epoch [12/20], Step [2/142], Loss: 0.8602\n",
            "Epoch [12/20], Step [3/142], Loss: 0.8052\n",
            "Epoch [12/20], Step [4/142], Loss: 0.9067\n",
            "Epoch [12/20], Step [5/142], Loss: 0.7934\n",
            "Epoch [12/20], Step [6/142], Loss: 0.8992\n",
            "Epoch [12/20], Step [7/142], Loss: 0.8385\n",
            "Epoch [12/20], Step [8/142], Loss: 0.8143\n",
            "Epoch [12/20], Step [9/142], Loss: 0.8523\n",
            "Epoch [12/20], Step [10/142], Loss: 0.8330\n",
            "Epoch [12/20], Step [11/142], Loss: 0.8070\n",
            "Epoch [12/20], Step [12/142], Loss: 0.8366\n",
            "Epoch [12/20], Step [13/142], Loss: 0.9271\n",
            "Epoch [12/20], Step [14/142], Loss: 0.8731\n",
            "Epoch [12/20], Step [15/142], Loss: 0.8768\n",
            "Epoch [12/20], Step [16/142], Loss: 0.8217\n",
            "Epoch [12/20], Step [17/142], Loss: 0.9772\n",
            "Epoch [12/20], Step [18/142], Loss: 0.7486\n",
            "Epoch [12/20], Step [19/142], Loss: 0.8499\n",
            "Epoch [12/20], Step [20/142], Loss: 0.8934\n",
            "Epoch [12/20], Step [21/142], Loss: 0.8355\n",
            "Epoch [12/20], Step [22/142], Loss: 0.9603\n",
            "Epoch [12/20], Step [23/142], Loss: 0.8693\n",
            "Epoch [12/20], Step [24/142], Loss: 0.8441\n",
            "Epoch [12/20], Step [25/142], Loss: 0.9297\n",
            "Epoch [12/20], Step [26/142], Loss: 0.8552\n",
            "Epoch [12/20], Step [27/142], Loss: 0.9314\n",
            "Epoch [12/20], Step [28/142], Loss: 0.9029\n",
            "Epoch [12/20], Step [29/142], Loss: 0.9203\n",
            "Epoch [12/20], Step [30/142], Loss: 0.8292\n",
            "Epoch [12/20], Step [31/142], Loss: 0.8425\n",
            "Epoch [12/20], Step [32/142], Loss: 0.8247\n",
            "Epoch [12/20], Step [33/142], Loss: 0.7935\n",
            "Epoch [12/20], Step [34/142], Loss: 0.8039\n",
            "Epoch [12/20], Step [35/142], Loss: 0.9223\n",
            "Epoch [12/20], Step [36/142], Loss: 0.7923\n",
            "Epoch [12/20], Step [37/142], Loss: 0.8165\n",
            "Epoch [12/20], Step [38/142], Loss: 0.9585\n",
            "Epoch [12/20], Step [39/142], Loss: 0.7726\n",
            "Epoch [12/20], Step [40/142], Loss: 0.7755\n",
            "Epoch [12/20], Step [41/142], Loss: 0.8732\n",
            "Epoch [12/20], Step [42/142], Loss: 0.8971\n",
            "Epoch [12/20], Step [43/142], Loss: 0.7838\n",
            "Epoch [12/20], Step [44/142], Loss: 0.7980\n",
            "Epoch [12/20], Step [45/142], Loss: 0.7814\n",
            "Epoch [12/20], Step [46/142], Loss: 0.8382\n",
            "Epoch [12/20], Step [47/142], Loss: 0.8339\n",
            "Epoch [12/20], Step [48/142], Loss: 0.7419\n",
            "Epoch [12/20], Step [49/142], Loss: 0.8991\n",
            "Epoch [12/20], Step [50/142], Loss: 0.9129\n",
            "Epoch [12/20], Step [51/142], Loss: 0.8464\n",
            "Epoch [12/20], Step [52/142], Loss: 0.8205\n",
            "Epoch [12/20], Step [53/142], Loss: 0.9414\n",
            "Epoch [12/20], Step [54/142], Loss: 0.8159\n",
            "Epoch [12/20], Step [55/142], Loss: 0.7714\n",
            "Epoch [12/20], Step [56/142], Loss: 0.8516\n",
            "Epoch [12/20], Step [57/142], Loss: 0.7699\n",
            "Epoch [12/20], Step [58/142], Loss: 0.7433\n",
            "Epoch [12/20], Step [59/142], Loss: 0.8811\n",
            "Epoch [12/20], Step [60/142], Loss: 0.8770\n",
            "Epoch [12/20], Step [61/142], Loss: 0.8096\n",
            "Epoch [12/20], Step [62/142], Loss: 0.8838\n",
            "Epoch [12/20], Step [63/142], Loss: 0.7907\n",
            "Epoch [12/20], Step [64/142], Loss: 0.8318\n",
            "Epoch [12/20], Step [65/142], Loss: 0.7599\n",
            "Epoch [12/20], Step [66/142], Loss: 0.8233\n",
            "Epoch [12/20], Step [67/142], Loss: 0.8822\n",
            "Epoch [12/20], Step [68/142], Loss: 0.8357\n",
            "Epoch [12/20], Step [69/142], Loss: 0.8078\n",
            "Epoch [12/20], Step [70/142], Loss: 0.8227\n",
            "Epoch [12/20], Step [71/142], Loss: 0.8252\n",
            "Epoch [12/20], Step [72/142], Loss: 0.7766\n",
            "Epoch [12/20], Step [73/142], Loss: 0.8408\n",
            "Epoch [12/20], Step [74/142], Loss: 0.9890\n",
            "Epoch [12/20], Step [75/142], Loss: 0.9245\n",
            "Epoch [12/20], Step [76/142], Loss: 0.8612\n",
            "Epoch [12/20], Step [77/142], Loss: 0.8164\n",
            "Epoch [12/20], Step [78/142], Loss: 0.8189\n",
            "Epoch [12/20], Step [79/142], Loss: 0.7453\n",
            "Epoch [12/20], Step [80/142], Loss: 0.8209\n",
            "Epoch [12/20], Step [81/142], Loss: 0.8200\n",
            "Epoch [12/20], Step [82/142], Loss: 0.8569\n",
            "Epoch [12/20], Step [83/142], Loss: 0.7691\n",
            "Epoch [12/20], Step [84/142], Loss: 0.8256\n",
            "Epoch [12/20], Step [85/142], Loss: 0.7325\n",
            "Epoch [12/20], Step [86/142], Loss: 0.8469\n",
            "Epoch [12/20], Step [87/142], Loss: 0.7879\n",
            "Epoch [12/20], Step [88/142], Loss: 0.8742\n",
            "Epoch [12/20], Step [89/142], Loss: 0.8527\n",
            "Epoch [12/20], Step [90/142], Loss: 0.8351\n",
            "Epoch [12/20], Step [91/142], Loss: 0.8701\n",
            "Epoch [12/20], Step [92/142], Loss: 0.8618\n",
            "Epoch [12/20], Step [93/142], Loss: 0.8337\n",
            "Epoch [12/20], Step [94/142], Loss: 0.8103\n",
            "Epoch [12/20], Step [95/142], Loss: 0.7563\n",
            "Epoch [12/20], Step [96/142], Loss: 0.8490\n",
            "Epoch [12/20], Step [97/142], Loss: 0.7934\n",
            "Epoch [12/20], Step [98/142], Loss: 0.9622\n",
            "Epoch [12/20], Step [99/142], Loss: 0.7883\n",
            "Epoch [12/20], Step [100/142], Loss: 0.8522\n",
            "Epoch [12/20], Step [101/142], Loss: 0.8064\n",
            "Epoch [12/20], Step [102/142], Loss: 0.8761\n",
            "Epoch [12/20], Step [103/142], Loss: 0.8623\n",
            "Epoch [12/20], Step [104/142], Loss: 0.7749\n",
            "Epoch [12/20], Step [105/142], Loss: 0.8253\n",
            "Epoch [12/20], Step [106/142], Loss: 0.9446\n",
            "Epoch [12/20], Step [107/142], Loss: 0.8216\n",
            "Epoch [12/20], Step [108/142], Loss: 0.9158\n",
            "Epoch [12/20], Step [109/142], Loss: 0.9191\n",
            "Epoch [12/20], Step [110/142], Loss: 0.8494\n",
            "Epoch [12/20], Step [111/142], Loss: 0.7593\n",
            "Epoch [12/20], Step [112/142], Loss: 0.9092\n",
            "Epoch [12/20], Step [113/142], Loss: 0.8463\n",
            "Epoch [12/20], Step [114/142], Loss: 0.8485\n",
            "Epoch [12/20], Step [115/142], Loss: 0.8474\n",
            "Epoch [12/20], Step [116/142], Loss: 0.8143\n",
            "Epoch [12/20], Step [117/142], Loss: 0.9293\n",
            "Epoch [12/20], Step [118/142], Loss: 0.8108\n",
            "Epoch [12/20], Step [119/142], Loss: 0.8953\n",
            "Epoch [12/20], Step [120/142], Loss: 0.8867\n",
            "Epoch [12/20], Step [121/142], Loss: 0.9288\n",
            "Epoch [12/20], Step [122/142], Loss: 0.7921\n",
            "Epoch [12/20], Step [123/142], Loss: 0.7492\n",
            "Epoch [12/20], Step [124/142], Loss: 0.8776\n",
            "Epoch [12/20], Step [125/142], Loss: 0.8699\n",
            "Epoch [12/20], Step [126/142], Loss: 0.8239\n",
            "Epoch [12/20], Step [127/142], Loss: 0.8084\n",
            "Epoch [12/20], Step [128/142], Loss: 0.9100\n",
            "Epoch [12/20], Step [129/142], Loss: 0.8567\n",
            "Epoch [12/20], Step [130/142], Loss: 0.7964\n",
            "Epoch [12/20], Step [131/142], Loss: 0.9161\n",
            "Epoch [12/20], Step [132/142], Loss: 0.8477\n",
            "Epoch [12/20], Step [133/142], Loss: 0.7875\n",
            "Epoch [12/20], Step [134/142], Loss: 0.8544\n",
            "Epoch [12/20], Step [135/142], Loss: 0.8696\n",
            "Epoch [12/20], Step [136/142], Loss: 0.7572\n",
            "Epoch [12/20], Step [137/142], Loss: 0.8754\n",
            "Epoch [12/20], Step [138/142], Loss: 0.7415\n",
            "Epoch [12/20], Step [139/142], Loss: 0.8672\n",
            "Epoch [12/20], Step [140/142], Loss: 0.8369\n",
            "Epoch [12/20], Step [141/142], Loss: 0.9096\n",
            "Epoch [12/20], Step [142/142], Loss: 0.8309\n",
            "Epoch [13/20], Step [1/142], Loss: 0.9173\n",
            "Epoch [13/20], Step [2/142], Loss: 0.8562\n",
            "Epoch [13/20], Step [3/142], Loss: 0.7246\n",
            "Epoch [13/20], Step [4/142], Loss: 0.8545\n",
            "Epoch [13/20], Step [5/142], Loss: 0.8281\n",
            "Epoch [13/20], Step [6/142], Loss: 0.8306\n",
            "Epoch [13/20], Step [7/142], Loss: 0.9050\n",
            "Epoch [13/20], Step [8/142], Loss: 0.8029\n",
            "Epoch [13/20], Step [9/142], Loss: 0.9135\n",
            "Epoch [13/20], Step [10/142], Loss: 0.8512\n",
            "Epoch [13/20], Step [11/142], Loss: 0.8617\n",
            "Epoch [13/20], Step [12/142], Loss: 0.8801\n",
            "Epoch [13/20], Step [13/142], Loss: 0.7971\n",
            "Epoch [13/20], Step [14/142], Loss: 0.8759\n",
            "Epoch [13/20], Step [15/142], Loss: 0.8065\n",
            "Epoch [13/20], Step [16/142], Loss: 0.9456\n",
            "Epoch [13/20], Step [17/142], Loss: 0.7640\n",
            "Epoch [13/20], Step [18/142], Loss: 0.8885\n",
            "Epoch [13/20], Step [19/142], Loss: 0.8715\n",
            "Epoch [13/20], Step [20/142], Loss: 0.7788\n",
            "Epoch [13/20], Step [21/142], Loss: 0.7670\n",
            "Epoch [13/20], Step [22/142], Loss: 0.8962\n",
            "Epoch [13/20], Step [23/142], Loss: 0.7864\n",
            "Epoch [13/20], Step [24/142], Loss: 0.8386\n",
            "Epoch [13/20], Step [25/142], Loss: 0.8146\n",
            "Epoch [13/20], Step [26/142], Loss: 0.9142\n",
            "Epoch [13/20], Step [27/142], Loss: 0.9221\n",
            "Epoch [13/20], Step [28/142], Loss: 0.8325\n",
            "Epoch [13/20], Step [29/142], Loss: 0.8028\n",
            "Epoch [13/20], Step [30/142], Loss: 0.8585\n",
            "Epoch [13/20], Step [31/142], Loss: 0.8224\n",
            "Epoch [13/20], Step [32/142], Loss: 0.8150\n",
            "Epoch [13/20], Step [33/142], Loss: 0.8170\n",
            "Epoch [13/20], Step [34/142], Loss: 0.8414\n",
            "Epoch [13/20], Step [35/142], Loss: 0.7904\n",
            "Epoch [13/20], Step [36/142], Loss: 0.8647\n",
            "Epoch [13/20], Step [37/142], Loss: 0.7756\n",
            "Epoch [13/20], Step [38/142], Loss: 0.8991\n",
            "Epoch [13/20], Step [39/142], Loss: 0.8422\n",
            "Epoch [13/20], Step [40/142], Loss: 0.7948\n",
            "Epoch [13/20], Step [41/142], Loss: 0.7585\n",
            "Epoch [13/20], Step [42/142], Loss: 0.8454\n",
            "Epoch [13/20], Step [43/142], Loss: 0.7749\n",
            "Epoch [13/20], Step [44/142], Loss: 0.8946\n",
            "Epoch [13/20], Step [45/142], Loss: 0.7836\n",
            "Epoch [13/20], Step [46/142], Loss: 0.7541\n",
            "Epoch [13/20], Step [47/142], Loss: 0.8579\n",
            "Epoch [13/20], Step [48/142], Loss: 0.8120\n",
            "Epoch [13/20], Step [49/142], Loss: 0.8168\n",
            "Epoch [13/20], Step [50/142], Loss: 0.8250\n",
            "Epoch [13/20], Step [51/142], Loss: 0.8042\n",
            "Epoch [13/20], Step [52/142], Loss: 0.8760\n",
            "Epoch [13/20], Step [53/142], Loss: 0.8736\n",
            "Epoch [13/20], Step [54/142], Loss: 0.7502\n",
            "Epoch [13/20], Step [55/142], Loss: 0.8521\n",
            "Epoch [13/20], Step [56/142], Loss: 0.8127\n",
            "Epoch [13/20], Step [57/142], Loss: 0.7969\n",
            "Epoch [13/20], Step [58/142], Loss: 0.8139\n",
            "Epoch [13/20], Step [59/142], Loss: 0.8103\n",
            "Epoch [13/20], Step [60/142], Loss: 0.7943\n",
            "Epoch [13/20], Step [61/142], Loss: 0.7781\n",
            "Epoch [13/20], Step [62/142], Loss: 0.9062\n",
            "Epoch [13/20], Step [63/142], Loss: 0.8370\n",
            "Epoch [13/20], Step [64/142], Loss: 0.9009\n",
            "Epoch [13/20], Step [65/142], Loss: 0.9126\n",
            "Epoch [13/20], Step [66/142], Loss: 0.8120\n",
            "Epoch [13/20], Step [67/142], Loss: 0.8015\n",
            "Epoch [13/20], Step [68/142], Loss: 0.8436\n",
            "Epoch [13/20], Step [69/142], Loss: 0.7780\n",
            "Epoch [13/20], Step [70/142], Loss: 0.9523\n",
            "Epoch [13/20], Step [71/142], Loss: 0.9184\n",
            "Epoch [13/20], Step [72/142], Loss: 0.9012\n",
            "Epoch [13/20], Step [73/142], Loss: 0.8695\n",
            "Epoch [13/20], Step [74/142], Loss: 0.9040\n",
            "Epoch [13/20], Step [75/142], Loss: 0.8989\n",
            "Epoch [13/20], Step [76/142], Loss: 0.7860\n",
            "Epoch [13/20], Step [77/142], Loss: 0.8427\n",
            "Epoch [13/20], Step [78/142], Loss: 0.8103\n",
            "Epoch [13/20], Step [79/142], Loss: 0.8987\n",
            "Epoch [13/20], Step [80/142], Loss: 0.7186\n",
            "Epoch [13/20], Step [81/142], Loss: 0.9110\n",
            "Epoch [13/20], Step [82/142], Loss: 0.9419\n",
            "Epoch [13/20], Step [83/142], Loss: 0.9436\n",
            "Epoch [13/20], Step [84/142], Loss: 0.8558\n",
            "Epoch [13/20], Step [85/142], Loss: 0.8020\n",
            "Epoch [13/20], Step [86/142], Loss: 0.7496\n",
            "Epoch [13/20], Step [87/142], Loss: 0.8765\n",
            "Epoch [13/20], Step [88/142], Loss: 0.8343\n",
            "Epoch [13/20], Step [89/142], Loss: 0.9874\n",
            "Epoch [13/20], Step [90/142], Loss: 0.9453\n",
            "Epoch [13/20], Step [91/142], Loss: 0.8378\n",
            "Epoch [13/20], Step [92/142], Loss: 0.8475\n",
            "Epoch [13/20], Step [93/142], Loss: 0.8858\n",
            "Epoch [13/20], Step [94/142], Loss: 0.8753\n",
            "Epoch [13/20], Step [95/142], Loss: 0.8681\n",
            "Epoch [13/20], Step [96/142], Loss: 0.8271\n",
            "Epoch [13/20], Step [97/142], Loss: 0.7322\n",
            "Epoch [13/20], Step [98/142], Loss: 0.9147\n",
            "Epoch [13/20], Step [99/142], Loss: 0.8182\n",
            "Epoch [13/20], Step [100/142], Loss: 0.8223\n",
            "Epoch [13/20], Step [101/142], Loss: 0.8708\n",
            "Epoch [13/20], Step [102/142], Loss: 0.9001\n",
            "Epoch [13/20], Step [103/142], Loss: 0.8767\n",
            "Epoch [13/20], Step [104/142], Loss: 0.8806\n",
            "Epoch [13/20], Step [105/142], Loss: 0.9826\n",
            "Epoch [13/20], Step [106/142], Loss: 0.7800\n",
            "Epoch [13/20], Step [107/142], Loss: 0.7691\n",
            "Epoch [13/20], Step [108/142], Loss: 0.7841\n",
            "Epoch [13/20], Step [109/142], Loss: 0.7815\n",
            "Epoch [13/20], Step [110/142], Loss: 0.8687\n",
            "Epoch [13/20], Step [111/142], Loss: 0.7488\n",
            "Epoch [13/20], Step [112/142], Loss: 0.7693\n",
            "Epoch [13/20], Step [113/142], Loss: 0.8323\n",
            "Epoch [13/20], Step [114/142], Loss: 0.8265\n",
            "Epoch [13/20], Step [115/142], Loss: 0.8762\n",
            "Epoch [13/20], Step [116/142], Loss: 0.8682\n",
            "Epoch [13/20], Step [117/142], Loss: 0.9124\n",
            "Epoch [13/20], Step [118/142], Loss: 0.8533\n",
            "Epoch [13/20], Step [119/142], Loss: 0.8256\n",
            "Epoch [13/20], Step [120/142], Loss: 0.8854\n",
            "Epoch [13/20], Step [121/142], Loss: 0.9279\n",
            "Epoch [13/20], Step [122/142], Loss: 0.8233\n",
            "Epoch [13/20], Step [123/142], Loss: 0.7326\n",
            "Epoch [13/20], Step [124/142], Loss: 0.7986\n",
            "Epoch [13/20], Step [125/142], Loss: 0.8053\n",
            "Epoch [13/20], Step [126/142], Loss: 0.8993\n",
            "Epoch [13/20], Step [127/142], Loss: 0.7681\n",
            "Epoch [13/20], Step [128/142], Loss: 0.8520\n",
            "Epoch [13/20], Step [129/142], Loss: 0.7742\n",
            "Epoch [13/20], Step [130/142], Loss: 0.9253\n",
            "Epoch [13/20], Step [131/142], Loss: 0.8623\n",
            "Epoch [13/20], Step [132/142], Loss: 0.8875\n",
            "Epoch [13/20], Step [133/142], Loss: 0.8342\n",
            "Epoch [13/20], Step [134/142], Loss: 0.7798\n",
            "Epoch [13/20], Step [135/142], Loss: 0.9059\n",
            "Epoch [13/20], Step [136/142], Loss: 0.8799\n",
            "Epoch [13/20], Step [137/142], Loss: 0.7875\n",
            "Epoch [13/20], Step [138/142], Loss: 0.9087\n",
            "Epoch [13/20], Step [139/142], Loss: 0.7888\n",
            "Epoch [13/20], Step [140/142], Loss: 0.9072\n",
            "Epoch [13/20], Step [141/142], Loss: 0.8793\n",
            "Epoch [13/20], Step [142/142], Loss: 0.9030\n",
            "Epoch [14/20], Step [1/142], Loss: 0.8550\n",
            "Epoch [14/20], Step [2/142], Loss: 0.8617\n",
            "Epoch [14/20], Step [3/142], Loss: 0.7327\n",
            "Epoch [14/20], Step [4/142], Loss: 0.9356\n",
            "Epoch [14/20], Step [5/142], Loss: 0.8445\n",
            "Epoch [14/20], Step [6/142], Loss: 0.9321\n",
            "Epoch [14/20], Step [7/142], Loss: 0.8586\n",
            "Epoch [14/20], Step [8/142], Loss: 0.7920\n",
            "Epoch [14/20], Step [9/142], Loss: 0.8671\n",
            "Epoch [14/20], Step [10/142], Loss: 0.9312\n",
            "Epoch [14/20], Step [11/142], Loss: 0.7531\n",
            "Epoch [14/20], Step [12/142], Loss: 0.8256\n",
            "Epoch [14/20], Step [13/142], Loss: 0.7540\n",
            "Epoch [14/20], Step [14/142], Loss: 0.7606\n",
            "Epoch [14/20], Step [15/142], Loss: 0.8797\n",
            "Epoch [14/20], Step [16/142], Loss: 0.9210\n",
            "Epoch [14/20], Step [17/142], Loss: 0.8346\n",
            "Epoch [14/20], Step [18/142], Loss: 0.7660\n",
            "Epoch [14/20], Step [19/142], Loss: 0.8299\n",
            "Epoch [14/20], Step [20/142], Loss: 0.8690\n",
            "Epoch [14/20], Step [21/142], Loss: 0.7854\n",
            "Epoch [14/20], Step [22/142], Loss: 0.9074\n",
            "Epoch [14/20], Step [23/142], Loss: 0.8525\n",
            "Epoch [14/20], Step [24/142], Loss: 0.7907\n",
            "Epoch [14/20], Step [25/142], Loss: 0.8336\n",
            "Epoch [14/20], Step [26/142], Loss: 0.8436\n",
            "Epoch [14/20], Step [27/142], Loss: 0.8375\n",
            "Epoch [14/20], Step [28/142], Loss: 0.7502\n",
            "Epoch [14/20], Step [29/142], Loss: 0.8207\n",
            "Epoch [14/20], Step [30/142], Loss: 0.8546\n",
            "Epoch [14/20], Step [31/142], Loss: 0.9124\n",
            "Epoch [14/20], Step [32/142], Loss: 0.8652\n",
            "Epoch [14/20], Step [33/142], Loss: 0.8070\n",
            "Epoch [14/20], Step [34/142], Loss: 0.8166\n",
            "Epoch [14/20], Step [35/142], Loss: 0.9023\n",
            "Epoch [14/20], Step [36/142], Loss: 0.9045\n",
            "Epoch [14/20], Step [37/142], Loss: 0.8745\n",
            "Epoch [14/20], Step [38/142], Loss: 0.8350\n",
            "Epoch [14/20], Step [39/142], Loss: 0.8468\n",
            "Epoch [14/20], Step [40/142], Loss: 0.8161\n",
            "Epoch [14/20], Step [41/142], Loss: 0.8806\n",
            "Epoch [14/20], Step [42/142], Loss: 0.9335\n",
            "Epoch [14/20], Step [43/142], Loss: 0.8668\n",
            "Epoch [14/20], Step [44/142], Loss: 0.7959\n",
            "Epoch [14/20], Step [45/142], Loss: 0.8297\n",
            "Epoch [14/20], Step [46/142], Loss: 0.7642\n",
            "Epoch [14/20], Step [47/142], Loss: 0.9579\n",
            "Epoch [14/20], Step [48/142], Loss: 0.8663\n",
            "Epoch [14/20], Step [49/142], Loss: 0.8886\n",
            "Epoch [14/20], Step [50/142], Loss: 0.8282\n",
            "Epoch [14/20], Step [51/142], Loss: 0.8730\n",
            "Epoch [14/20], Step [52/142], Loss: 0.8443\n",
            "Epoch [14/20], Step [53/142], Loss: 0.8536\n",
            "Epoch [14/20], Step [54/142], Loss: 0.8659\n",
            "Epoch [14/20], Step [55/142], Loss: 0.8437\n",
            "Epoch [14/20], Step [56/142], Loss: 0.8353\n",
            "Epoch [14/20], Step [57/142], Loss: 0.8346\n",
            "Epoch [14/20], Step [58/142], Loss: 0.8484\n",
            "Epoch [14/20], Step [59/142], Loss: 0.7808\n",
            "Epoch [14/20], Step [60/142], Loss: 0.8010\n",
            "Epoch [14/20], Step [61/142], Loss: 0.7651\n",
            "Epoch [14/20], Step [62/142], Loss: 0.8494\n",
            "Epoch [14/20], Step [63/142], Loss: 0.8571\n",
            "Epoch [14/20], Step [64/142], Loss: 0.9042\n",
            "Epoch [14/20], Step [65/142], Loss: 0.9177\n",
            "Epoch [14/20], Step [66/142], Loss: 0.8629\n",
            "Epoch [14/20], Step [67/142], Loss: 0.8030\n",
            "Epoch [14/20], Step [68/142], Loss: 0.8570\n",
            "Epoch [14/20], Step [69/142], Loss: 0.8581\n",
            "Epoch [14/20], Step [70/142], Loss: 0.9001\n",
            "Epoch [14/20], Step [71/142], Loss: 0.9768\n",
            "Epoch [14/20], Step [72/142], Loss: 0.7854\n",
            "Epoch [14/20], Step [73/142], Loss: 0.8375\n",
            "Epoch [14/20], Step [74/142], Loss: 0.8442\n",
            "Epoch [14/20], Step [75/142], Loss: 0.8761\n",
            "Epoch [14/20], Step [76/142], Loss: 0.8998\n",
            "Epoch [14/20], Step [77/142], Loss: 0.8044\n",
            "Epoch [14/20], Step [78/142], Loss: 0.8140\n",
            "Epoch [14/20], Step [79/142], Loss: 0.8359\n",
            "Epoch [14/20], Step [80/142], Loss: 0.8503\n",
            "Epoch [14/20], Step [81/142], Loss: 0.8754\n",
            "Epoch [14/20], Step [82/142], Loss: 0.8518\n",
            "Epoch [14/20], Step [83/142], Loss: 0.8748\n",
            "Epoch [14/20], Step [84/142], Loss: 0.8246\n",
            "Epoch [14/20], Step [85/142], Loss: 0.8218\n",
            "Epoch [14/20], Step [86/142], Loss: 0.7980\n",
            "Epoch [14/20], Step [87/142], Loss: 0.8388\n",
            "Epoch [14/20], Step [88/142], Loss: 0.7909\n",
            "Epoch [14/20], Step [89/142], Loss: 0.8582\n",
            "Epoch [14/20], Step [90/142], Loss: 0.7832\n",
            "Epoch [14/20], Step [91/142], Loss: 0.8454\n",
            "Epoch [14/20], Step [92/142], Loss: 0.9167\n",
            "Epoch [14/20], Step [93/142], Loss: 0.7778\n",
            "Epoch [14/20], Step [94/142], Loss: 0.9306\n",
            "Epoch [14/20], Step [95/142], Loss: 0.8991\n",
            "Epoch [14/20], Step [96/142], Loss: 0.8955\n",
            "Epoch [14/20], Step [97/142], Loss: 0.8266\n",
            "Epoch [14/20], Step [98/142], Loss: 0.8668\n",
            "Epoch [14/20], Step [99/142], Loss: 0.8250\n",
            "Epoch [14/20], Step [100/142], Loss: 0.8466\n",
            "Epoch [14/20], Step [101/142], Loss: 0.7550\n",
            "Epoch [14/20], Step [102/142], Loss: 0.8022\n",
            "Epoch [14/20], Step [103/142], Loss: 0.8114\n",
            "Epoch [14/20], Step [104/142], Loss: 0.9258\n",
            "Epoch [14/20], Step [105/142], Loss: 0.7259\n",
            "Epoch [14/20], Step [106/142], Loss: 0.8817\n",
            "Epoch [14/20], Step [107/142], Loss: 0.8919\n",
            "Epoch [14/20], Step [108/142], Loss: 0.7812\n",
            "Epoch [14/20], Step [109/142], Loss: 0.8429\n",
            "Epoch [14/20], Step [110/142], Loss: 0.7644\n",
            "Epoch [14/20], Step [111/142], Loss: 0.8208\n",
            "Epoch [14/20], Step [112/142], Loss: 0.7820\n",
            "Epoch [14/20], Step [113/142], Loss: 0.8082\n",
            "Epoch [14/20], Step [114/142], Loss: 0.8974\n",
            "Epoch [14/20], Step [115/142], Loss: 0.8522\n",
            "Epoch [14/20], Step [116/142], Loss: 0.8480\n",
            "Epoch [14/20], Step [117/142], Loss: 0.8647\n",
            "Epoch [14/20], Step [118/142], Loss: 0.7941\n",
            "Epoch [14/20], Step [119/142], Loss: 0.9130\n",
            "Epoch [14/20], Step [120/142], Loss: 0.8663\n",
            "Epoch [14/20], Step [121/142], Loss: 0.8548\n",
            "Epoch [14/20], Step [122/142], Loss: 0.8001\n",
            "Epoch [14/20], Step [123/142], Loss: 0.9002\n",
            "Epoch [14/20], Step [124/142], Loss: 0.8369\n",
            "Epoch [14/20], Step [125/142], Loss: 0.9057\n",
            "Epoch [14/20], Step [126/142], Loss: 0.8880\n",
            "Epoch [14/20], Step [127/142], Loss: 0.9142\n",
            "Epoch [14/20], Step [128/142], Loss: 0.8045\n",
            "Epoch [14/20], Step [129/142], Loss: 0.8125\n",
            "Epoch [14/20], Step [130/142], Loss: 0.8348\n",
            "Epoch [14/20], Step [131/142], Loss: 0.7899\n",
            "Epoch [14/20], Step [132/142], Loss: 0.9220\n",
            "Epoch [14/20], Step [133/142], Loss: 0.8540\n",
            "Epoch [14/20], Step [134/142], Loss: 0.8600\n",
            "Epoch [14/20], Step [135/142], Loss: 0.8489\n",
            "Epoch [14/20], Step [136/142], Loss: 0.7841\n",
            "Epoch [14/20], Step [137/142], Loss: 0.9110\n",
            "Epoch [14/20], Step [138/142], Loss: 0.8087\n",
            "Epoch [14/20], Step [139/142], Loss: 0.7187\n",
            "Epoch [14/20], Step [140/142], Loss: 0.7924\n",
            "Epoch [14/20], Step [141/142], Loss: 0.8962\n",
            "Epoch [14/20], Step [142/142], Loss: 0.9291\n",
            "Epoch [15/20], Step [1/142], Loss: 0.7787\n",
            "Epoch [15/20], Step [2/142], Loss: 0.8557\n",
            "Epoch [15/20], Step [3/142], Loss: 0.8835\n",
            "Epoch [15/20], Step [4/142], Loss: 0.8143\n",
            "Epoch [15/20], Step [5/142], Loss: 0.8239\n",
            "Epoch [15/20], Step [6/142], Loss: 0.8454\n",
            "Epoch [15/20], Step [7/142], Loss: 0.8810\n",
            "Epoch [15/20], Step [8/142], Loss: 0.8309\n",
            "Epoch [15/20], Step [9/142], Loss: 0.8416\n",
            "Epoch [15/20], Step [10/142], Loss: 0.8477\n",
            "Epoch [15/20], Step [11/142], Loss: 0.7986\n",
            "Epoch [15/20], Step [12/142], Loss: 0.8028\n",
            "Epoch [15/20], Step [13/142], Loss: 0.9001\n",
            "Epoch [15/20], Step [14/142], Loss: 0.8086\n",
            "Epoch [15/20], Step [15/142], Loss: 0.9134\n",
            "Epoch [15/20], Step [16/142], Loss: 0.8204\n",
            "Epoch [15/20], Step [17/142], Loss: 0.8623\n",
            "Epoch [15/20], Step [18/142], Loss: 0.8167\n",
            "Epoch [15/20], Step [19/142], Loss: 0.9135\n",
            "Epoch [15/20], Step [20/142], Loss: 0.8242\n",
            "Epoch [15/20], Step [21/142], Loss: 0.8334\n",
            "Epoch [15/20], Step [22/142], Loss: 0.8171\n",
            "Epoch [15/20], Step [23/142], Loss: 0.8572\n",
            "Epoch [15/20], Step [24/142], Loss: 0.8145\n",
            "Epoch [15/20], Step [25/142], Loss: 0.8723\n",
            "Epoch [15/20], Step [26/142], Loss: 0.8224\n",
            "Epoch [15/20], Step [27/142], Loss: 0.8245\n",
            "Epoch [15/20], Step [28/142], Loss: 0.8447\n",
            "Epoch [15/20], Step [29/142], Loss: 0.8972\n",
            "Epoch [15/20], Step [30/142], Loss: 0.8822\n",
            "Epoch [15/20], Step [31/142], Loss: 0.8299\n",
            "Epoch [15/20], Step [32/142], Loss: 0.8877\n",
            "Epoch [15/20], Step [33/142], Loss: 0.9260\n",
            "Epoch [15/20], Step [34/142], Loss: 0.8165\n",
            "Epoch [15/20], Step [35/142], Loss: 0.8446\n",
            "Epoch [15/20], Step [36/142], Loss: 0.8476\n",
            "Epoch [15/20], Step [37/142], Loss: 0.9764\n",
            "Epoch [15/20], Step [38/142], Loss: 0.7193\n",
            "Epoch [15/20], Step [39/142], Loss: 0.8334\n",
            "Epoch [15/20], Step [40/142], Loss: 0.9108\n",
            "Epoch [15/20], Step [41/142], Loss: 0.8420\n",
            "Epoch [15/20], Step [42/142], Loss: 0.8975\n",
            "Epoch [15/20], Step [43/142], Loss: 0.8963\n",
            "Epoch [15/20], Step [44/142], Loss: 0.9644\n",
            "Epoch [15/20], Step [45/142], Loss: 0.8856\n",
            "Epoch [15/20], Step [46/142], Loss: 0.8606\n",
            "Epoch [15/20], Step [47/142], Loss: 0.7704\n",
            "Epoch [15/20], Step [48/142], Loss: 0.8270\n",
            "Epoch [15/20], Step [49/142], Loss: 0.8127\n",
            "Epoch [15/20], Step [50/142], Loss: 0.8793\n",
            "Epoch [15/20], Step [51/142], Loss: 0.9663\n",
            "Epoch [15/20], Step [52/142], Loss: 0.8404\n",
            "Epoch [15/20], Step [53/142], Loss: 0.7156\n",
            "Epoch [15/20], Step [54/142], Loss: 0.7650\n",
            "Epoch [15/20], Step [55/142], Loss: 0.8083\n",
            "Epoch [15/20], Step [56/142], Loss: 0.8046\n",
            "Epoch [15/20], Step [57/142], Loss: 0.8503\n",
            "Epoch [15/20], Step [58/142], Loss: 0.7363\n",
            "Epoch [15/20], Step [59/142], Loss: 0.8199\n",
            "Epoch [15/20], Step [60/142], Loss: 0.9755\n",
            "Epoch [15/20], Step [61/142], Loss: 0.8279\n",
            "Epoch [15/20], Step [62/142], Loss: 0.7887\n",
            "Epoch [15/20], Step [63/142], Loss: 0.8539\n",
            "Epoch [15/20], Step [64/142], Loss: 0.9085\n",
            "Epoch [15/20], Step [65/142], Loss: 0.8270\n",
            "Epoch [15/20], Step [66/142], Loss: 0.8244\n",
            "Epoch [15/20], Step [67/142], Loss: 0.8526\n",
            "Epoch [15/20], Step [68/142], Loss: 0.8839\n",
            "Epoch [15/20], Step [69/142], Loss: 0.8283\n",
            "Epoch [15/20], Step [70/142], Loss: 0.8012\n",
            "Epoch [15/20], Step [71/142], Loss: 0.7885\n",
            "Epoch [15/20], Step [72/142], Loss: 0.8777\n",
            "Epoch [15/20], Step [73/142], Loss: 0.7853\n",
            "Epoch [15/20], Step [74/142], Loss: 0.9380\n",
            "Epoch [15/20], Step [75/142], Loss: 0.8505\n",
            "Epoch [15/20], Step [76/142], Loss: 0.8152\n",
            "Epoch [15/20], Step [77/142], Loss: 0.7822\n",
            "Epoch [15/20], Step [78/142], Loss: 0.8101\n",
            "Epoch [15/20], Step [79/142], Loss: 0.7981\n",
            "Epoch [15/20], Step [80/142], Loss: 0.8183\n",
            "Epoch [15/20], Step [81/142], Loss: 0.8327\n",
            "Epoch [15/20], Step [82/142], Loss: 0.9271\n",
            "Epoch [15/20], Step [83/142], Loss: 0.8931\n",
            "Epoch [15/20], Step [84/142], Loss: 0.7557\n",
            "Epoch [15/20], Step [85/142], Loss: 0.8670\n",
            "Epoch [15/20], Step [86/142], Loss: 0.6936\n",
            "Epoch [15/20], Step [87/142], Loss: 0.8521\n",
            "Epoch [15/20], Step [88/142], Loss: 0.7136\n",
            "Epoch [15/20], Step [89/142], Loss: 0.8707\n",
            "Epoch [15/20], Step [90/142], Loss: 0.8606\n",
            "Epoch [15/20], Step [91/142], Loss: 0.8776\n",
            "Epoch [15/20], Step [92/142], Loss: 0.8580\n",
            "Epoch [15/20], Step [93/142], Loss: 0.8235\n",
            "Epoch [15/20], Step [94/142], Loss: 0.8024\n",
            "Epoch [15/20], Step [95/142], Loss: 0.8662\n",
            "Epoch [15/20], Step [96/142], Loss: 0.8481\n",
            "Epoch [15/20], Step [97/142], Loss: 1.0012\n",
            "Epoch [15/20], Step [98/142], Loss: 0.8594\n",
            "Epoch [15/20], Step [99/142], Loss: 0.8105\n",
            "Epoch [15/20], Step [100/142], Loss: 0.8467\n",
            "Epoch [15/20], Step [101/142], Loss: 0.9149\n",
            "Epoch [15/20], Step [102/142], Loss: 0.7962\n",
            "Epoch [15/20], Step [103/142], Loss: 0.8668\n",
            "Epoch [15/20], Step [104/142], Loss: 0.7595\n",
            "Epoch [15/20], Step [105/142], Loss: 0.7871\n",
            "Epoch [15/20], Step [106/142], Loss: 0.7811\n",
            "Epoch [15/20], Step [107/142], Loss: 0.7823\n",
            "Epoch [15/20], Step [108/142], Loss: 0.7540\n",
            "Epoch [15/20], Step [109/142], Loss: 0.8363\n",
            "Epoch [15/20], Step [110/142], Loss: 0.8141\n",
            "Epoch [15/20], Step [111/142], Loss: 0.8952\n",
            "Epoch [15/20], Step [112/142], Loss: 0.9146\n",
            "Epoch [15/20], Step [113/142], Loss: 0.8639\n",
            "Epoch [15/20], Step [114/142], Loss: 0.8934\n",
            "Epoch [15/20], Step [115/142], Loss: 0.8320\n",
            "Epoch [15/20], Step [116/142], Loss: 0.7612\n",
            "Epoch [15/20], Step [117/142], Loss: 0.8853\n",
            "Epoch [15/20], Step [118/142], Loss: 0.9847\n",
            "Epoch [15/20], Step [119/142], Loss: 0.9296\n",
            "Epoch [15/20], Step [120/142], Loss: 0.7839\n",
            "Epoch [15/20], Step [121/142], Loss: 0.8294\n",
            "Epoch [15/20], Step [122/142], Loss: 0.7502\n",
            "Epoch [15/20], Step [123/142], Loss: 0.8151\n",
            "Epoch [15/20], Step [124/142], Loss: 0.8871\n",
            "Epoch [15/20], Step [125/142], Loss: 0.8887\n",
            "Epoch [15/20], Step [126/142], Loss: 0.8091\n",
            "Epoch [15/20], Step [127/142], Loss: 0.8030\n",
            "Epoch [15/20], Step [128/142], Loss: 0.8346\n",
            "Epoch [15/20], Step [129/142], Loss: 0.8302\n",
            "Epoch [15/20], Step [130/142], Loss: 0.9420\n",
            "Epoch [15/20], Step [131/142], Loss: 0.8140\n",
            "Epoch [15/20], Step [132/142], Loss: 0.8472\n",
            "Epoch [15/20], Step [133/142], Loss: 0.8632\n",
            "Epoch [15/20], Step [134/142], Loss: 0.8524\n",
            "Epoch [15/20], Step [135/142], Loss: 0.8645\n",
            "Epoch [15/20], Step [136/142], Loss: 0.8533\n",
            "Epoch [15/20], Step [137/142], Loss: 0.8217\n",
            "Epoch [15/20], Step [138/142], Loss: 0.8104\n",
            "Epoch [15/20], Step [139/142], Loss: 0.8851\n",
            "Epoch [15/20], Step [140/142], Loss: 0.8171\n",
            "Epoch [15/20], Step [141/142], Loss: 0.9867\n",
            "Epoch [15/20], Step [142/142], Loss: 0.7946\n",
            "Epoch [16/20], Step [1/142], Loss: 0.8719\n",
            "Epoch [16/20], Step [2/142], Loss: 0.8135\n",
            "Epoch [16/20], Step [3/142], Loss: 0.8280\n",
            "Epoch [16/20], Step [4/142], Loss: 0.8229\n",
            "Epoch [16/20], Step [5/142], Loss: 0.8874\n",
            "Epoch [16/20], Step [6/142], Loss: 0.8338\n",
            "Epoch [16/20], Step [7/142], Loss: 0.8350\n",
            "Epoch [16/20], Step [8/142], Loss: 0.7928\n",
            "Epoch [16/20], Step [9/142], Loss: 0.9213\n",
            "Epoch [16/20], Step [10/142], Loss: 0.8758\n",
            "Epoch [16/20], Step [11/142], Loss: 0.8470\n",
            "Epoch [16/20], Step [12/142], Loss: 0.7801\n",
            "Epoch [16/20], Step [13/142], Loss: 0.8213\n",
            "Epoch [16/20], Step [14/142], Loss: 0.8369\n",
            "Epoch [16/20], Step [15/142], Loss: 0.7928\n",
            "Epoch [16/20], Step [16/142], Loss: 0.8512\n",
            "Epoch [16/20], Step [17/142], Loss: 0.8451\n",
            "Epoch [16/20], Step [18/142], Loss: 0.8922\n",
            "Epoch [16/20], Step [19/142], Loss: 0.8878\n",
            "Epoch [16/20], Step [20/142], Loss: 0.8716\n",
            "Epoch [16/20], Step [21/142], Loss: 0.8858\n",
            "Epoch [16/20], Step [22/142], Loss: 0.8658\n",
            "Epoch [16/20], Step [23/142], Loss: 0.8397\n",
            "Epoch [16/20], Step [24/142], Loss: 0.9103\n",
            "Epoch [16/20], Step [25/142], Loss: 0.9024\n",
            "Epoch [16/20], Step [26/142], Loss: 0.7842\n",
            "Epoch [16/20], Step [27/142], Loss: 0.8857\n",
            "Epoch [16/20], Step [28/142], Loss: 0.8165\n",
            "Epoch [16/20], Step [29/142], Loss: 0.8184\n",
            "Epoch [16/20], Step [30/142], Loss: 0.8421\n",
            "Epoch [16/20], Step [31/142], Loss: 0.8468\n",
            "Epoch [16/20], Step [32/142], Loss: 0.7874\n",
            "Epoch [16/20], Step [33/142], Loss: 0.8406\n",
            "Epoch [16/20], Step [34/142], Loss: 0.8764\n",
            "Epoch [16/20], Step [35/142], Loss: 0.8705\n",
            "Epoch [16/20], Step [36/142], Loss: 0.8534\n",
            "Epoch [16/20], Step [37/142], Loss: 0.8514\n",
            "Epoch [16/20], Step [38/142], Loss: 0.7184\n",
            "Epoch [16/20], Step [39/142], Loss: 0.8243\n",
            "Epoch [16/20], Step [40/142], Loss: 0.7568\n",
            "Epoch [16/20], Step [41/142], Loss: 0.9002\n",
            "Epoch [16/20], Step [42/142], Loss: 0.8004\n",
            "Epoch [16/20], Step [43/142], Loss: 0.8580\n",
            "Epoch [16/20], Step [44/142], Loss: 0.8827\n",
            "Epoch [16/20], Step [45/142], Loss: 0.8590\n",
            "Epoch [16/20], Step [46/142], Loss: 0.9116\n",
            "Epoch [16/20], Step [47/142], Loss: 0.7935\n",
            "Epoch [16/20], Step [48/142], Loss: 0.8071\n",
            "Epoch [16/20], Step [49/142], Loss: 0.8290\n",
            "Epoch [16/20], Step [50/142], Loss: 0.8499\n",
            "Epoch [16/20], Step [51/142], Loss: 0.9144\n",
            "Epoch [16/20], Step [52/142], Loss: 0.8433\n",
            "Epoch [16/20], Step [53/142], Loss: 0.8755\n",
            "Epoch [16/20], Step [54/142], Loss: 0.7639\n",
            "Epoch [16/20], Step [55/142], Loss: 0.9012\n",
            "Epoch [16/20], Step [56/142], Loss: 0.8835\n",
            "Epoch [16/20], Step [57/142], Loss: 0.8102\n",
            "Epoch [16/20], Step [58/142], Loss: 0.8038\n",
            "Epoch [16/20], Step [59/142], Loss: 0.9007\n",
            "Epoch [16/20], Step [60/142], Loss: 0.7868\n",
            "Epoch [16/20], Step [61/142], Loss: 0.9050\n",
            "Epoch [16/20], Step [62/142], Loss: 0.7665\n",
            "Epoch [16/20], Step [63/142], Loss: 0.8526\n",
            "Epoch [16/20], Step [64/142], Loss: 0.7613\n",
            "Epoch [16/20], Step [65/142], Loss: 0.8899\n",
            "Epoch [16/20], Step [66/142], Loss: 0.8867\n",
            "Epoch [16/20], Step [67/142], Loss: 0.8588\n",
            "Epoch [16/20], Step [68/142], Loss: 0.8150\n",
            "Epoch [16/20], Step [69/142], Loss: 0.8532\n",
            "Epoch [16/20], Step [70/142], Loss: 0.9029\n",
            "Epoch [16/20], Step [71/142], Loss: 0.8220\n",
            "Epoch [16/20], Step [72/142], Loss: 0.8436\n",
            "Epoch [16/20], Step [73/142], Loss: 0.8654\n",
            "Epoch [16/20], Step [74/142], Loss: 0.9235\n",
            "Epoch [16/20], Step [75/142], Loss: 0.8885\n",
            "Epoch [16/20], Step [76/142], Loss: 0.8868\n",
            "Epoch [16/20], Step [77/142], Loss: 0.9013\n",
            "Epoch [16/20], Step [78/142], Loss: 0.8635\n",
            "Epoch [16/20], Step [79/142], Loss: 0.8618\n",
            "Epoch [16/20], Step [80/142], Loss: 0.8911\n",
            "Epoch [16/20], Step [81/142], Loss: 0.8555\n",
            "Epoch [16/20], Step [82/142], Loss: 0.9731\n",
            "Epoch [16/20], Step [83/142], Loss: 0.8754\n",
            "Epoch [16/20], Step [84/142], Loss: 0.8041\n",
            "Epoch [16/20], Step [85/142], Loss: 0.9210\n",
            "Epoch [16/20], Step [86/142], Loss: 0.8240\n",
            "Epoch [16/20], Step [87/142], Loss: 0.8690\n",
            "Epoch [16/20], Step [88/142], Loss: 0.8722\n",
            "Epoch [16/20], Step [89/142], Loss: 0.7796\n",
            "Epoch [16/20], Step [90/142], Loss: 0.8198\n",
            "Epoch [16/20], Step [91/142], Loss: 0.7702\n",
            "Epoch [16/20], Step [92/142], Loss: 0.8333\n",
            "Epoch [16/20], Step [93/142], Loss: 0.8554\n",
            "Epoch [16/20], Step [94/142], Loss: 0.8309\n",
            "Epoch [16/20], Step [95/142], Loss: 0.8608\n",
            "Epoch [16/20], Step [96/142], Loss: 0.8440\n",
            "Epoch [16/20], Step [97/142], Loss: 0.9112\n",
            "Epoch [16/20], Step [98/142], Loss: 0.8229\n",
            "Epoch [16/20], Step [99/142], Loss: 0.7368\n",
            "Epoch [16/20], Step [100/142], Loss: 0.8214\n",
            "Epoch [16/20], Step [101/142], Loss: 0.8951\n",
            "Epoch [16/20], Step [102/142], Loss: 0.8172\n",
            "Epoch [16/20], Step [103/142], Loss: 0.8965\n",
            "Epoch [16/20], Step [104/142], Loss: 0.8485\n",
            "Epoch [16/20], Step [105/142], Loss: 0.7422\n",
            "Epoch [16/20], Step [106/142], Loss: 0.8361\n",
            "Epoch [16/20], Step [107/142], Loss: 0.7663\n",
            "Epoch [16/20], Step [108/142], Loss: 0.8329\n",
            "Epoch [16/20], Step [109/142], Loss: 0.7569\n",
            "Epoch [16/20], Step [110/142], Loss: 0.9306\n",
            "Epoch [16/20], Step [111/142], Loss: 0.9154\n",
            "Epoch [16/20], Step [112/142], Loss: 0.8299\n",
            "Epoch [16/20], Step [113/142], Loss: 0.7486\n",
            "Epoch [16/20], Step [114/142], Loss: 0.7850\n",
            "Epoch [16/20], Step [115/142], Loss: 0.8496\n",
            "Epoch [16/20], Step [116/142], Loss: 0.8173\n",
            "Epoch [16/20], Step [117/142], Loss: 0.8576\n",
            "Epoch [16/20], Step [118/142], Loss: 0.8481\n",
            "Epoch [16/20], Step [119/142], Loss: 0.8639\n",
            "Epoch [16/20], Step [120/142], Loss: 0.8366\n",
            "Epoch [16/20], Step [121/142], Loss: 0.7523\n",
            "Epoch [16/20], Step [122/142], Loss: 0.8913\n",
            "Epoch [16/20], Step [123/142], Loss: 0.8317\n",
            "Epoch [16/20], Step [124/142], Loss: 0.7704\n",
            "Epoch [16/20], Step [125/142], Loss: 0.8142\n",
            "Epoch [16/20], Step [126/142], Loss: 0.8011\n",
            "Epoch [16/20], Step [127/142], Loss: 0.9011\n",
            "Epoch [16/20], Step [128/142], Loss: 0.8779\n",
            "Epoch [16/20], Step [129/142], Loss: 0.9152\n",
            "Epoch [16/20], Step [130/142], Loss: 0.8486\n",
            "Epoch [16/20], Step [131/142], Loss: 0.8530\n",
            "Epoch [16/20], Step [132/142], Loss: 0.8945\n",
            "Epoch [16/20], Step [133/142], Loss: 0.7187\n",
            "Epoch [16/20], Step [134/142], Loss: 0.8371\n",
            "Epoch [16/20], Step [135/142], Loss: 0.8746\n",
            "Epoch [16/20], Step [136/142], Loss: 0.8351\n",
            "Epoch [16/20], Step [137/142], Loss: 0.8112\n",
            "Epoch [16/20], Step [138/142], Loss: 0.9118\n",
            "Epoch [16/20], Step [139/142], Loss: 0.8176\n",
            "Epoch [16/20], Step [140/142], Loss: 0.8466\n",
            "Epoch [16/20], Step [141/142], Loss: 0.8606\n",
            "Epoch [16/20], Step [142/142], Loss: 0.6661\n",
            "Epoch [17/20], Step [1/142], Loss: 0.8192\n",
            "Epoch [17/20], Step [2/142], Loss: 0.8407\n",
            "Epoch [17/20], Step [3/142], Loss: 0.8551\n",
            "Epoch [17/20], Step [4/142], Loss: 0.8553\n",
            "Epoch [17/20], Step [5/142], Loss: 0.8015\n",
            "Epoch [17/20], Step [6/142], Loss: 0.7832\n",
            "Epoch [17/20], Step [7/142], Loss: 0.8087\n",
            "Epoch [17/20], Step [8/142], Loss: 0.8683\n",
            "Epoch [17/20], Step [9/142], Loss: 0.8047\n",
            "Epoch [17/20], Step [10/142], Loss: 0.8272\n",
            "Epoch [17/20], Step [11/142], Loss: 0.8196\n",
            "Epoch [17/20], Step [12/142], Loss: 0.8824\n",
            "Epoch [17/20], Step [13/142], Loss: 0.8282\n",
            "Epoch [17/20], Step [14/142], Loss: 0.7850\n",
            "Epoch [17/20], Step [15/142], Loss: 0.8499\n",
            "Epoch [17/20], Step [16/142], Loss: 0.8774\n",
            "Epoch [17/20], Step [17/142], Loss: 0.7917\n",
            "Epoch [17/20], Step [18/142], Loss: 0.8681\n",
            "Epoch [17/20], Step [19/142], Loss: 0.8571\n",
            "Epoch [17/20], Step [20/142], Loss: 0.9127\n",
            "Epoch [17/20], Step [21/142], Loss: 0.8886\n",
            "Epoch [17/20], Step [22/142], Loss: 0.8686\n",
            "Epoch [17/20], Step [23/142], Loss: 0.8101\n",
            "Epoch [17/20], Step [24/142], Loss: 0.7831\n",
            "Epoch [17/20], Step [25/142], Loss: 0.7983\n",
            "Epoch [17/20], Step [26/142], Loss: 0.8569\n",
            "Epoch [17/20], Step [27/142], Loss: 0.8277\n",
            "Epoch [17/20], Step [28/142], Loss: 0.8737\n",
            "Epoch [17/20], Step [29/142], Loss: 0.9203\n",
            "Epoch [17/20], Step [30/142], Loss: 0.8342\n",
            "Epoch [17/20], Step [31/142], Loss: 0.8299\n",
            "Epoch [17/20], Step [32/142], Loss: 0.7598\n",
            "Epoch [17/20], Step [33/142], Loss: 0.8411\n",
            "Epoch [17/20], Step [34/142], Loss: 0.9183\n",
            "Epoch [17/20], Step [35/142], Loss: 0.9098\n",
            "Epoch [17/20], Step [36/142], Loss: 0.8469\n",
            "Epoch [17/20], Step [37/142], Loss: 0.8366\n",
            "Epoch [17/20], Step [38/142], Loss: 0.8579\n",
            "Epoch [17/20], Step [39/142], Loss: 0.8481\n",
            "Epoch [17/20], Step [40/142], Loss: 0.8634\n",
            "Epoch [17/20], Step [41/142], Loss: 0.7908\n",
            "Epoch [17/20], Step [42/142], Loss: 0.9127\n",
            "Epoch [17/20], Step [43/142], Loss: 0.8586\n",
            "Epoch [17/20], Step [44/142], Loss: 0.7821\n",
            "Epoch [17/20], Step [45/142], Loss: 0.9979\n",
            "Epoch [17/20], Step [46/142], Loss: 0.7174\n",
            "Epoch [17/20], Step [47/142], Loss: 0.8861\n",
            "Epoch [17/20], Step [48/142], Loss: 0.8865\n",
            "Epoch [17/20], Step [49/142], Loss: 0.8982\n",
            "Epoch [17/20], Step [50/142], Loss: 0.7509\n",
            "Epoch [17/20], Step [51/142], Loss: 0.9253\n",
            "Epoch [17/20], Step [52/142], Loss: 0.8314\n",
            "Epoch [17/20], Step [53/142], Loss: 0.8678\n",
            "Epoch [17/20], Step [54/142], Loss: 0.8600\n",
            "Epoch [17/20], Step [55/142], Loss: 0.8397\n",
            "Epoch [17/20], Step [56/142], Loss: 0.9741\n",
            "Epoch [17/20], Step [57/142], Loss: 0.8739\n",
            "Epoch [17/20], Step [58/142], Loss: 0.7926\n",
            "Epoch [17/20], Step [59/142], Loss: 0.8664\n",
            "Epoch [17/20], Step [60/142], Loss: 0.8110\n",
            "Epoch [17/20], Step [61/142], Loss: 0.8014\n",
            "Epoch [17/20], Step [62/142], Loss: 0.8691\n",
            "Epoch [17/20], Step [63/142], Loss: 0.8074\n",
            "Epoch [17/20], Step [64/142], Loss: 0.9123\n",
            "Epoch [17/20], Step [65/142], Loss: 0.8510\n",
            "Epoch [17/20], Step [66/142], Loss: 0.8146\n",
            "Epoch [17/20], Step [67/142], Loss: 0.9177\n",
            "Epoch [17/20], Step [68/142], Loss: 0.8820\n",
            "Epoch [17/20], Step [69/142], Loss: 0.7916\n",
            "Epoch [17/20], Step [70/142], Loss: 0.8739\n",
            "Epoch [17/20], Step [71/142], Loss: 0.7726\n",
            "Epoch [17/20], Step [72/142], Loss: 0.9329\n",
            "Epoch [17/20], Step [73/142], Loss: 0.8874\n",
            "Epoch [17/20], Step [74/142], Loss: 0.8424\n",
            "Epoch [17/20], Step [75/142], Loss: 0.8946\n",
            "Epoch [17/20], Step [76/142], Loss: 0.8476\n",
            "Epoch [17/20], Step [77/142], Loss: 0.7932\n",
            "Epoch [17/20], Step [78/142], Loss: 0.8047\n",
            "Epoch [17/20], Step [79/142], Loss: 0.7896\n",
            "Epoch [17/20], Step [80/142], Loss: 0.8747\n",
            "Epoch [17/20], Step [81/142], Loss: 0.8937\n",
            "Epoch [17/20], Step [82/142], Loss: 1.0117\n",
            "Epoch [17/20], Step [83/142], Loss: 0.7021\n",
            "Epoch [17/20], Step [84/142], Loss: 0.9041\n",
            "Epoch [17/20], Step [85/142], Loss: 0.8399\n",
            "Epoch [17/20], Step [86/142], Loss: 0.7948\n",
            "Epoch [17/20], Step [87/142], Loss: 0.7456\n",
            "Epoch [17/20], Step [88/142], Loss: 0.8556\n",
            "Epoch [17/20], Step [89/142], Loss: 0.8082\n",
            "Epoch [17/20], Step [90/142], Loss: 0.9108\n",
            "Epoch [17/20], Step [91/142], Loss: 0.7653\n",
            "Epoch [17/20], Step [92/142], Loss: 0.8837\n",
            "Epoch [17/20], Step [93/142], Loss: 0.8056\n",
            "Epoch [17/20], Step [94/142], Loss: 0.8216\n",
            "Epoch [17/20], Step [95/142], Loss: 0.8280\n",
            "Epoch [17/20], Step [96/142], Loss: 0.9125\n",
            "Epoch [17/20], Step [97/142], Loss: 0.8649\n",
            "Epoch [17/20], Step [98/142], Loss: 0.8815\n",
            "Epoch [17/20], Step [99/142], Loss: 0.8472\n",
            "Epoch [17/20], Step [100/142], Loss: 0.8642\n",
            "Epoch [17/20], Step [101/142], Loss: 0.7216\n",
            "Epoch [17/20], Step [102/142], Loss: 0.8247\n",
            "Epoch [17/20], Step [103/142], Loss: 0.9220\n",
            "Epoch [17/20], Step [104/142], Loss: 0.7752\n",
            "Epoch [17/20], Step [105/142], Loss: 0.8803\n",
            "Epoch [17/20], Step [106/142], Loss: 0.8196\n",
            "Epoch [17/20], Step [107/142], Loss: 0.8484\n",
            "Epoch [17/20], Step [108/142], Loss: 0.9021\n",
            "Epoch [17/20], Step [109/142], Loss: 0.7632\n",
            "Epoch [17/20], Step [110/142], Loss: 0.8631\n",
            "Epoch [17/20], Step [111/142], Loss: 0.8077\n",
            "Epoch [17/20], Step [112/142], Loss: 0.8139\n",
            "Epoch [17/20], Step [113/142], Loss: 0.9134\n",
            "Epoch [17/20], Step [114/142], Loss: 0.8337\n",
            "Epoch [17/20], Step [115/142], Loss: 0.7696\n",
            "Epoch [17/20], Step [116/142], Loss: 0.7988\n",
            "Epoch [17/20], Step [117/142], Loss: 0.7156\n",
            "Epoch [17/20], Step [118/142], Loss: 0.8308\n",
            "Epoch [17/20], Step [119/142], Loss: 0.8754\n",
            "Epoch [17/20], Step [120/142], Loss: 0.8624\n",
            "Epoch [17/20], Step [121/142], Loss: 0.8720\n",
            "Epoch [17/20], Step [122/142], Loss: 0.8133\n",
            "Epoch [17/20], Step [123/142], Loss: 0.8549\n",
            "Epoch [17/20], Step [124/142], Loss: 0.8416\n",
            "Epoch [17/20], Step [125/142], Loss: 0.8878\n",
            "Epoch [17/20], Step [126/142], Loss: 0.8225\n",
            "Epoch [17/20], Step [127/142], Loss: 0.7518\n",
            "Epoch [17/20], Step [128/142], Loss: 0.8452\n",
            "Epoch [17/20], Step [129/142], Loss: 0.9461\n",
            "Epoch [17/20], Step [130/142], Loss: 0.8091\n",
            "Epoch [17/20], Step [131/142], Loss: 0.9119\n",
            "Epoch [17/20], Step [132/142], Loss: 0.7802\n",
            "Epoch [17/20], Step [133/142], Loss: 0.7906\n",
            "Epoch [17/20], Step [134/142], Loss: 0.8484\n",
            "Epoch [17/20], Step [135/142], Loss: 0.8461\n",
            "Epoch [17/20], Step [136/142], Loss: 0.8755\n",
            "Epoch [17/20], Step [137/142], Loss: 0.8316\n",
            "Epoch [17/20], Step [138/142], Loss: 0.7662\n",
            "Epoch [17/20], Step [139/142], Loss: 0.8348\n",
            "Epoch [17/20], Step [140/142], Loss: 0.9881\n",
            "Epoch [17/20], Step [141/142], Loss: 0.8835\n",
            "Epoch [17/20], Step [142/142], Loss: 0.8702\n",
            "Epoch [18/20], Step [1/142], Loss: 0.8182\n",
            "Epoch [18/20], Step [2/142], Loss: 0.8234\n",
            "Epoch [18/20], Step [3/142], Loss: 0.9470\n",
            "Epoch [18/20], Step [4/142], Loss: 0.8595\n",
            "Epoch [18/20], Step [5/142], Loss: 0.8132\n",
            "Epoch [18/20], Step [6/142], Loss: 0.9118\n",
            "Epoch [18/20], Step [7/142], Loss: 0.6802\n",
            "Epoch [18/20], Step [8/142], Loss: 0.8523\n",
            "Epoch [18/20], Step [9/142], Loss: 0.8361\n",
            "Epoch [18/20], Step [10/142], Loss: 0.8188\n",
            "Epoch [18/20], Step [11/142], Loss: 0.8085\n",
            "Epoch [18/20], Step [12/142], Loss: 0.8487\n",
            "Epoch [18/20], Step [13/142], Loss: 0.8419\n",
            "Epoch [18/20], Step [14/142], Loss: 0.8229\n",
            "Epoch [18/20], Step [15/142], Loss: 0.8429\n",
            "Epoch [18/20], Step [16/142], Loss: 0.8398\n",
            "Epoch [18/20], Step [17/142], Loss: 0.8883\n",
            "Epoch [18/20], Step [18/142], Loss: 0.8655\n",
            "Epoch [18/20], Step [19/142], Loss: 0.7564\n",
            "Epoch [18/20], Step [20/142], Loss: 0.7355\n",
            "Epoch [18/20], Step [21/142], Loss: 0.9212\n",
            "Epoch [18/20], Step [22/142], Loss: 0.8768\n",
            "Epoch [18/20], Step [23/142], Loss: 0.8004\n",
            "Epoch [18/20], Step [24/142], Loss: 0.8258\n",
            "Epoch [18/20], Step [25/142], Loss: 0.8669\n",
            "Epoch [18/20], Step [26/142], Loss: 0.8217\n",
            "Epoch [18/20], Step [27/142], Loss: 0.8968\n",
            "Epoch [18/20], Step [28/142], Loss: 0.9198\n",
            "Epoch [18/20], Step [29/142], Loss: 0.8135\n",
            "Epoch [18/20], Step [30/142], Loss: 0.8121\n",
            "Epoch [18/20], Step [31/142], Loss: 0.8372\n",
            "Epoch [18/20], Step [32/142], Loss: 0.7793\n",
            "Epoch [18/20], Step [33/142], Loss: 0.7658\n",
            "Epoch [18/20], Step [34/142], Loss: 0.8862\n",
            "Epoch [18/20], Step [35/142], Loss: 0.8374\n",
            "Epoch [18/20], Step [36/142], Loss: 0.9333\n",
            "Epoch [18/20], Step [37/142], Loss: 0.8758\n",
            "Epoch [18/20], Step [38/142], Loss: 0.8298\n",
            "Epoch [18/20], Step [39/142], Loss: 0.8316\n",
            "Epoch [18/20], Step [40/142], Loss: 0.7704\n",
            "Epoch [18/20], Step [41/142], Loss: 0.8453\n",
            "Epoch [18/20], Step [42/142], Loss: 0.8892\n",
            "Epoch [18/20], Step [43/142], Loss: 0.8509\n",
            "Epoch [18/20], Step [44/142], Loss: 0.8604\n",
            "Epoch [18/20], Step [45/142], Loss: 0.8686\n",
            "Epoch [18/20], Step [46/142], Loss: 0.8534\n",
            "Epoch [18/20], Step [47/142], Loss: 0.9441\n",
            "Epoch [18/20], Step [48/142], Loss: 0.8167\n",
            "Epoch [18/20], Step [49/142], Loss: 0.8596\n",
            "Epoch [18/20], Step [50/142], Loss: 0.8954\n",
            "Epoch [18/20], Step [51/142], Loss: 0.9038\n",
            "Epoch [18/20], Step [52/142], Loss: 0.8179\n",
            "Epoch [18/20], Step [53/142], Loss: 0.8149\n",
            "Epoch [18/20], Step [54/142], Loss: 0.9768\n",
            "Epoch [18/20], Step [55/142], Loss: 0.7538\n",
            "Epoch [18/20], Step [56/142], Loss: 0.8591\n",
            "Epoch [18/20], Step [57/142], Loss: 0.8300\n",
            "Epoch [18/20], Step [58/142], Loss: 0.9599\n",
            "Epoch [18/20], Step [59/142], Loss: 0.8442\n",
            "Epoch [18/20], Step [60/142], Loss: 0.7783\n",
            "Epoch [18/20], Step [61/142], Loss: 0.8837\n",
            "Epoch [18/20], Step [62/142], Loss: 0.8987\n",
            "Epoch [18/20], Step [63/142], Loss: 0.8547\n",
            "Epoch [18/20], Step [64/142], Loss: 0.8661\n",
            "Epoch [18/20], Step [65/142], Loss: 0.8382\n",
            "Epoch [18/20], Step [66/142], Loss: 0.8930\n",
            "Epoch [18/20], Step [67/142], Loss: 0.7908\n",
            "Epoch [18/20], Step [68/142], Loss: 0.8105\n",
            "Epoch [18/20], Step [69/142], Loss: 0.8484\n",
            "Epoch [18/20], Step [70/142], Loss: 0.8287\n",
            "Epoch [18/20], Step [71/142], Loss: 0.7907\n",
            "Epoch [18/20], Step [72/142], Loss: 0.8438\n",
            "Epoch [18/20], Step [73/142], Loss: 0.9590\n",
            "Epoch [18/20], Step [74/142], Loss: 0.9463\n",
            "Epoch [18/20], Step [75/142], Loss: 0.8211\n",
            "Epoch [18/20], Step [76/142], Loss: 0.8705\n",
            "Epoch [18/20], Step [77/142], Loss: 0.9447\n",
            "Epoch [18/20], Step [78/142], Loss: 0.8412\n",
            "Epoch [18/20], Step [79/142], Loss: 0.8924\n",
            "Epoch [18/20], Step [80/142], Loss: 0.7720\n",
            "Epoch [18/20], Step [81/142], Loss: 0.8820\n",
            "Epoch [18/20], Step [82/142], Loss: 0.7880\n",
            "Epoch [18/20], Step [83/142], Loss: 0.9818\n",
            "Epoch [18/20], Step [84/142], Loss: 0.7916\n",
            "Epoch [18/20], Step [85/142], Loss: 0.8253\n",
            "Epoch [18/20], Step [86/142], Loss: 0.7330\n",
            "Epoch [18/20], Step [87/142], Loss: 0.8003\n",
            "Epoch [18/20], Step [88/142], Loss: 0.8977\n",
            "Epoch [18/20], Step [89/142], Loss: 0.7950\n",
            "Epoch [18/20], Step [90/142], Loss: 0.8333\n",
            "Epoch [18/20], Step [91/142], Loss: 0.7974\n",
            "Epoch [18/20], Step [92/142], Loss: 0.7827\n",
            "Epoch [18/20], Step [93/142], Loss: 0.8854\n",
            "Epoch [18/20], Step [94/142], Loss: 0.8710\n",
            "Epoch [18/20], Step [95/142], Loss: 0.7931\n",
            "Epoch [18/20], Step [96/142], Loss: 0.7870\n",
            "Epoch [18/20], Step [97/142], Loss: 0.8097\n",
            "Epoch [18/20], Step [98/142], Loss: 0.8602\n",
            "Epoch [18/20], Step [99/142], Loss: 0.8397\n",
            "Epoch [18/20], Step [100/142], Loss: 0.8374\n",
            "Epoch [18/20], Step [101/142], Loss: 0.8193\n",
            "Epoch [18/20], Step [102/142], Loss: 0.9124\n",
            "Epoch [18/20], Step [103/142], Loss: 0.7690\n",
            "Epoch [18/20], Step [104/142], Loss: 0.8456\n",
            "Epoch [18/20], Step [105/142], Loss: 0.8223\n",
            "Epoch [18/20], Step [106/142], Loss: 0.8822\n",
            "Epoch [18/20], Step [107/142], Loss: 0.9764\n",
            "Epoch [18/20], Step [108/142], Loss: 0.8641\n",
            "Epoch [18/20], Step [109/142], Loss: 0.8667\n",
            "Epoch [18/20], Step [110/142], Loss: 0.8341\n",
            "Epoch [18/20], Step [111/142], Loss: 0.7962\n",
            "Epoch [18/20], Step [112/142], Loss: 0.7640\n",
            "Epoch [18/20], Step [113/142], Loss: 0.8428\n",
            "Epoch [18/20], Step [114/142], Loss: 0.8807\n",
            "Epoch [18/20], Step [115/142], Loss: 0.8377\n",
            "Epoch [18/20], Step [116/142], Loss: 0.7529\n",
            "Epoch [18/20], Step [117/142], Loss: 0.8675\n",
            "Epoch [18/20], Step [118/142], Loss: 0.8015\n",
            "Epoch [18/20], Step [119/142], Loss: 0.8159\n",
            "Epoch [18/20], Step [120/142], Loss: 0.9434\n",
            "Epoch [18/20], Step [121/142], Loss: 0.8012\n",
            "Epoch [18/20], Step [122/142], Loss: 0.7647\n",
            "Epoch [18/20], Step [123/142], Loss: 0.8730\n",
            "Epoch [18/20], Step [124/142], Loss: 0.8068\n",
            "Epoch [18/20], Step [125/142], Loss: 0.8827\n",
            "Epoch [18/20], Step [126/142], Loss: 0.8508\n",
            "Epoch [18/20], Step [127/142], Loss: 0.7563\n",
            "Epoch [18/20], Step [128/142], Loss: 0.7800\n",
            "Epoch [18/20], Step [129/142], Loss: 0.8325\n",
            "Epoch [18/20], Step [130/142], Loss: 0.9003\n",
            "Epoch [18/20], Step [131/142], Loss: 0.8731\n",
            "Epoch [18/20], Step [132/142], Loss: 0.8003\n",
            "Epoch [18/20], Step [133/142], Loss: 0.8696\n",
            "Epoch [18/20], Step [134/142], Loss: 0.8347\n",
            "Epoch [18/20], Step [135/142], Loss: 0.9205\n",
            "Epoch [18/20], Step [136/142], Loss: 0.9533\n",
            "Epoch [18/20], Step [137/142], Loss: 0.7547\n",
            "Epoch [18/20], Step [138/142], Loss: 0.8259\n",
            "Epoch [18/20], Step [139/142], Loss: 0.8354\n",
            "Epoch [18/20], Step [140/142], Loss: 0.9188\n",
            "Epoch [18/20], Step [141/142], Loss: 0.8113\n",
            "Epoch [18/20], Step [142/142], Loss: 0.8322\n",
            "Epoch [19/20], Step [1/142], Loss: 0.8736\n",
            "Epoch [19/20], Step [2/142], Loss: 0.8869\n",
            "Epoch [19/20], Step [3/142], Loss: 0.9027\n",
            "Epoch [19/20], Step [4/142], Loss: 0.8717\n",
            "Epoch [19/20], Step [5/142], Loss: 0.8050\n",
            "Epoch [19/20], Step [6/142], Loss: 0.9682\n",
            "Epoch [19/20], Step [7/142], Loss: 0.8442\n",
            "Epoch [19/20], Step [8/142], Loss: 0.7968\n",
            "Epoch [19/20], Step [9/142], Loss: 0.8040\n",
            "Epoch [19/20], Step [10/142], Loss: 0.8544\n",
            "Epoch [19/20], Step [11/142], Loss: 0.8849\n",
            "Epoch [19/20], Step [12/142], Loss: 0.7509\n",
            "Epoch [19/20], Step [13/142], Loss: 0.8244\n",
            "Epoch [19/20], Step [14/142], Loss: 0.8133\n",
            "Epoch [19/20], Step [15/142], Loss: 0.8191\n",
            "Epoch [19/20], Step [16/142], Loss: 0.8189\n",
            "Epoch [19/20], Step [17/142], Loss: 0.7966\n",
            "Epoch [19/20], Step [18/142], Loss: 0.7955\n",
            "Epoch [19/20], Step [19/142], Loss: 0.8242\n",
            "Epoch [19/20], Step [20/142], Loss: 0.8543\n",
            "Epoch [19/20], Step [21/142], Loss: 0.8767\n",
            "Epoch [19/20], Step [22/142], Loss: 0.8142\n",
            "Epoch [19/20], Step [23/142], Loss: 0.8801\n",
            "Epoch [19/20], Step [24/142], Loss: 0.8407\n",
            "Epoch [19/20], Step [25/142], Loss: 0.8534\n",
            "Epoch [19/20], Step [26/142], Loss: 0.8629\n",
            "Epoch [19/20], Step [27/142], Loss: 0.9498\n",
            "Epoch [19/20], Step [28/142], Loss: 0.8251\n",
            "Epoch [19/20], Step [29/142], Loss: 0.8469\n",
            "Epoch [19/20], Step [30/142], Loss: 0.8934\n",
            "Epoch [19/20], Step [31/142], Loss: 0.7983\n",
            "Epoch [19/20], Step [32/142], Loss: 0.7844\n",
            "Epoch [19/20], Step [33/142], Loss: 0.8895\n",
            "Epoch [19/20], Step [34/142], Loss: 0.8122\n",
            "Epoch [19/20], Step [35/142], Loss: 0.8177\n",
            "Epoch [19/20], Step [36/142], Loss: 0.8612\n",
            "Epoch [19/20], Step [37/142], Loss: 0.8821\n",
            "Epoch [19/20], Step [38/142], Loss: 0.8900\n",
            "Epoch [19/20], Step [39/142], Loss: 0.8129\n",
            "Epoch [19/20], Step [40/142], Loss: 0.8205\n",
            "Epoch [19/20], Step [41/142], Loss: 0.8483\n",
            "Epoch [19/20], Step [42/142], Loss: 0.8045\n",
            "Epoch [19/20], Step [43/142], Loss: 0.8275\n",
            "Epoch [19/20], Step [44/142], Loss: 0.8638\n",
            "Epoch [19/20], Step [45/142], Loss: 0.8564\n",
            "Epoch [19/20], Step [46/142], Loss: 0.8733\n",
            "Epoch [19/20], Step [47/142], Loss: 0.8474\n",
            "Epoch [19/20], Step [48/142], Loss: 0.8470\n",
            "Epoch [19/20], Step [49/142], Loss: 0.8580\n",
            "Epoch [19/20], Step [50/142], Loss: 0.9224\n",
            "Epoch [19/20], Step [51/142], Loss: 0.7509\n",
            "Epoch [19/20], Step [52/142], Loss: 0.8588\n",
            "Epoch [19/20], Step [53/142], Loss: 0.8077\n",
            "Epoch [19/20], Step [54/142], Loss: 0.8234\n",
            "Epoch [19/20], Step [55/142], Loss: 0.8505\n",
            "Epoch [19/20], Step [56/142], Loss: 0.8777\n",
            "Epoch [19/20], Step [57/142], Loss: 0.8241\n",
            "Epoch [19/20], Step [58/142], Loss: 0.8780\n",
            "Epoch [19/20], Step [59/142], Loss: 0.8861\n",
            "Epoch [19/20], Step [60/142], Loss: 0.8321\n",
            "Epoch [19/20], Step [61/142], Loss: 0.8884\n",
            "Epoch [19/20], Step [62/142], Loss: 0.7849\n",
            "Epoch [19/20], Step [63/142], Loss: 0.7996\n",
            "Epoch [19/20], Step [64/142], Loss: 0.9556\n",
            "Epoch [19/20], Step [65/142], Loss: 0.7674\n",
            "Epoch [19/20], Step [66/142], Loss: 0.8750\n",
            "Epoch [19/20], Step [67/142], Loss: 0.7790\n",
            "Epoch [19/20], Step [68/142], Loss: 0.7889\n",
            "Epoch [19/20], Step [69/142], Loss: 0.7570\n",
            "Epoch [19/20], Step [70/142], Loss: 0.8239\n",
            "Epoch [19/20], Step [71/142], Loss: 0.8142\n",
            "Epoch [19/20], Step [72/142], Loss: 0.7995\n",
            "Epoch [19/20], Step [73/142], Loss: 0.9713\n",
            "Epoch [19/20], Step [74/142], Loss: 0.8274\n",
            "Epoch [19/20], Step [75/142], Loss: 0.7910\n",
            "Epoch [19/20], Step [76/142], Loss: 0.8160\n",
            "Epoch [19/20], Step [77/142], Loss: 0.7897\n",
            "Epoch [19/20], Step [78/142], Loss: 0.9224\n",
            "Epoch [19/20], Step [79/142], Loss: 0.8395\n",
            "Epoch [19/20], Step [80/142], Loss: 0.8162\n",
            "Epoch [19/20], Step [81/142], Loss: 0.8449\n",
            "Epoch [19/20], Step [82/142], Loss: 0.7873\n",
            "Epoch [19/20], Step [83/142], Loss: 0.8087\n",
            "Epoch [19/20], Step [84/142], Loss: 0.8377\n",
            "Epoch [19/20], Step [85/142], Loss: 0.8742\n",
            "Epoch [19/20], Step [86/142], Loss: 0.9254\n",
            "Epoch [19/20], Step [87/142], Loss: 0.8563\n",
            "Epoch [19/20], Step [88/142], Loss: 0.8668\n",
            "Epoch [19/20], Step [89/142], Loss: 0.8274\n",
            "Epoch [19/20], Step [90/142], Loss: 0.8821\n",
            "Epoch [19/20], Step [91/142], Loss: 0.8545\n",
            "Epoch [19/20], Step [92/142], Loss: 0.8486\n",
            "Epoch [19/20], Step [93/142], Loss: 0.8873\n",
            "Epoch [19/20], Step [94/142], Loss: 0.8407\n",
            "Epoch [19/20], Step [95/142], Loss: 0.8181\n",
            "Epoch [19/20], Step [96/142], Loss: 0.8064\n",
            "Epoch [19/20], Step [97/142], Loss: 0.7231\n",
            "Epoch [19/20], Step [98/142], Loss: 0.7971\n",
            "Epoch [19/20], Step [99/142], Loss: 0.8046\n",
            "Epoch [19/20], Step [100/142], Loss: 0.8161\n",
            "Epoch [19/20], Step [101/142], Loss: 0.8524\n",
            "Epoch [19/20], Step [102/142], Loss: 0.7800\n",
            "Epoch [19/20], Step [103/142], Loss: 0.8522\n",
            "Epoch [19/20], Step [104/142], Loss: 0.8144\n",
            "Epoch [19/20], Step [105/142], Loss: 0.8529\n",
            "Epoch [19/20], Step [106/142], Loss: 0.8966\n",
            "Epoch [19/20], Step [107/142], Loss: 0.8314\n",
            "Epoch [19/20], Step [108/142], Loss: 0.8338\n",
            "Epoch [19/20], Step [109/142], Loss: 0.8667\n",
            "Epoch [19/20], Step [110/142], Loss: 0.8018\n",
            "Epoch [19/20], Step [111/142], Loss: 0.7895\n",
            "Epoch [19/20], Step [112/142], Loss: 0.9132\n",
            "Epoch [19/20], Step [113/142], Loss: 0.8390\n",
            "Epoch [19/20], Step [114/142], Loss: 0.7839\n",
            "Epoch [19/20], Step [115/142], Loss: 0.8808\n",
            "Epoch [19/20], Step [116/142], Loss: 0.8167\n",
            "Epoch [19/20], Step [117/142], Loss: 0.8325\n",
            "Epoch [19/20], Step [118/142], Loss: 0.8923\n",
            "Epoch [19/20], Step [119/142], Loss: 0.8808\n",
            "Epoch [19/20], Step [120/142], Loss: 0.8766\n",
            "Epoch [19/20], Step [121/142], Loss: 0.8524\n",
            "Epoch [19/20], Step [122/142], Loss: 0.7782\n",
            "Epoch [19/20], Step [123/142], Loss: 0.8912\n",
            "Epoch [19/20], Step [124/142], Loss: 0.8369\n",
            "Epoch [19/20], Step [125/142], Loss: 0.9536\n",
            "Epoch [19/20], Step [126/142], Loss: 0.7068\n",
            "Epoch [19/20], Step [127/142], Loss: 0.8562\n",
            "Epoch [19/20], Step [128/142], Loss: 0.8743\n",
            "Epoch [19/20], Step [129/142], Loss: 0.8840\n",
            "Epoch [19/20], Step [130/142], Loss: 0.8210\n",
            "Epoch [19/20], Step [131/142], Loss: 0.8559\n",
            "Epoch [19/20], Step [132/142], Loss: 0.8233\n",
            "Epoch [19/20], Step [133/142], Loss: 0.8114\n",
            "Epoch [19/20], Step [134/142], Loss: 0.8771\n",
            "Epoch [19/20], Step [135/142], Loss: 0.9853\n",
            "Epoch [19/20], Step [136/142], Loss: 0.8282\n",
            "Epoch [19/20], Step [137/142], Loss: 0.8726\n",
            "Epoch [19/20], Step [138/142], Loss: 0.8533\n",
            "Epoch [19/20], Step [139/142], Loss: 0.9108\n",
            "Epoch [19/20], Step [140/142], Loss: 0.9184\n",
            "Epoch [19/20], Step [141/142], Loss: 0.8280\n",
            "Epoch [19/20], Step [142/142], Loss: 0.8774\n",
            "Epoch [20/20], Step [1/142], Loss: 0.8358\n",
            "Epoch [20/20], Step [2/142], Loss: 0.7820\n",
            "Epoch [20/20], Step [3/142], Loss: 0.8629\n",
            "Epoch [20/20], Step [4/142], Loss: 0.9172\n",
            "Epoch [20/20], Step [5/142], Loss: 0.9173\n",
            "Epoch [20/20], Step [6/142], Loss: 0.7876\n",
            "Epoch [20/20], Step [7/142], Loss: 0.8534\n",
            "Epoch [20/20], Step [8/142], Loss: 0.7324\n",
            "Epoch [20/20], Step [9/142], Loss: 0.8096\n",
            "Epoch [20/20], Step [10/142], Loss: 0.8201\n",
            "Epoch [20/20], Step [11/142], Loss: 0.9231\n",
            "Epoch [20/20], Step [12/142], Loss: 0.8928\n",
            "Epoch [20/20], Step [13/142], Loss: 0.9253\n",
            "Epoch [20/20], Step [14/142], Loss: 0.9027\n",
            "Epoch [20/20], Step [15/142], Loss: 0.9140\n",
            "Epoch [20/20], Step [16/142], Loss: 0.9072\n",
            "Epoch [20/20], Step [17/142], Loss: 0.8772\n",
            "Epoch [20/20], Step [18/142], Loss: 0.8396\n",
            "Epoch [20/20], Step [19/142], Loss: 0.8602\n",
            "Epoch [20/20], Step [20/142], Loss: 0.8627\n",
            "Epoch [20/20], Step [21/142], Loss: 0.8482\n",
            "Epoch [20/20], Step [22/142], Loss: 0.9568\n",
            "Epoch [20/20], Step [23/142], Loss: 0.8609\n",
            "Epoch [20/20], Step [24/142], Loss: 0.8576\n",
            "Epoch [20/20], Step [25/142], Loss: 0.8486\n",
            "Epoch [20/20], Step [26/142], Loss: 0.8623\n",
            "Epoch [20/20], Step [27/142], Loss: 0.8165\n",
            "Epoch [20/20], Step [28/142], Loss: 0.7605\n",
            "Epoch [20/20], Step [29/142], Loss: 0.8648\n",
            "Epoch [20/20], Step [30/142], Loss: 0.8381\n",
            "Epoch [20/20], Step [31/142], Loss: 0.9119\n",
            "Epoch [20/20], Step [32/142], Loss: 0.8544\n",
            "Epoch [20/20], Step [33/142], Loss: 0.9762\n",
            "Epoch [20/20], Step [34/142], Loss: 0.8024\n",
            "Epoch [20/20], Step [35/142], Loss: 0.7679\n",
            "Epoch [20/20], Step [36/142], Loss: 0.8807\n",
            "Epoch [20/20], Step [37/142], Loss: 0.7798\n",
            "Epoch [20/20], Step [38/142], Loss: 0.8644\n",
            "Epoch [20/20], Step [39/142], Loss: 0.8444\n",
            "Epoch [20/20], Step [40/142], Loss: 0.8909\n",
            "Epoch [20/20], Step [41/142], Loss: 0.8925\n",
            "Epoch [20/20], Step [42/142], Loss: 0.8540\n",
            "Epoch [20/20], Step [43/142], Loss: 0.8251\n",
            "Epoch [20/20], Step [44/142], Loss: 0.8107\n",
            "Epoch [20/20], Step [45/142], Loss: 0.8371\n",
            "Epoch [20/20], Step [46/142], Loss: 0.7887\n",
            "Epoch [20/20], Step [47/142], Loss: 0.8369\n",
            "Epoch [20/20], Step [48/142], Loss: 0.8931\n",
            "Epoch [20/20], Step [49/142], Loss: 0.8793\n",
            "Epoch [20/20], Step [50/142], Loss: 0.8214\n",
            "Epoch [20/20], Step [51/142], Loss: 0.8298\n",
            "Epoch [20/20], Step [52/142], Loss: 0.8410\n",
            "Epoch [20/20], Step [53/142], Loss: 0.7977\n",
            "Epoch [20/20], Step [54/142], Loss: 0.8022\n",
            "Epoch [20/20], Step [55/142], Loss: 0.8162\n",
            "Epoch [20/20], Step [56/142], Loss: 0.8179\n",
            "Epoch [20/20], Step [57/142], Loss: 0.8330\n",
            "Epoch [20/20], Step [58/142], Loss: 0.9187\n",
            "Epoch [20/20], Step [59/142], Loss: 0.7982\n",
            "Epoch [20/20], Step [60/142], Loss: 0.8861\n",
            "Epoch [20/20], Step [61/142], Loss: 0.8356\n",
            "Epoch [20/20], Step [62/142], Loss: 0.9009\n",
            "Epoch [20/20], Step [63/142], Loss: 0.8766\n",
            "Epoch [20/20], Step [64/142], Loss: 0.8289\n",
            "Epoch [20/20], Step [65/142], Loss: 0.5982\n",
            "Epoch [20/20], Step [66/142], Loss: 0.7602\n",
            "Epoch [20/20], Step [67/142], Loss: 0.8408\n",
            "Epoch [20/20], Step [68/142], Loss: 0.9452\n",
            "Epoch [20/20], Step [69/142], Loss: 0.7864\n",
            "Epoch [20/20], Step [70/142], Loss: 0.8077\n",
            "Epoch [20/20], Step [71/142], Loss: 0.9843\n",
            "Epoch [20/20], Step [72/142], Loss: 0.8600\n",
            "Epoch [20/20], Step [73/142], Loss: 0.8159\n",
            "Epoch [20/20], Step [74/142], Loss: 0.8609\n",
            "Epoch [20/20], Step [75/142], Loss: 0.7529\n",
            "Epoch [20/20], Step [76/142], Loss: 0.7994\n",
            "Epoch [20/20], Step [77/142], Loss: 0.9125\n",
            "Epoch [20/20], Step [78/142], Loss: 0.9363\n",
            "Epoch [20/20], Step [79/142], Loss: 0.8694\n",
            "Epoch [20/20], Step [80/142], Loss: 0.9861\n",
            "Epoch [20/20], Step [81/142], Loss: 0.8412\n",
            "Epoch [20/20], Step [82/142], Loss: 0.7996\n",
            "Epoch [20/20], Step [83/142], Loss: 0.8371\n",
            "Epoch [20/20], Step [84/142], Loss: 0.8426\n",
            "Epoch [20/20], Step [85/142], Loss: 0.8272\n",
            "Epoch [20/20], Step [86/142], Loss: 0.8834\n",
            "Epoch [20/20], Step [87/142], Loss: 0.8951\n",
            "Epoch [20/20], Step [88/142], Loss: 0.8060\n",
            "Epoch [20/20], Step [89/142], Loss: 0.8463\n",
            "Epoch [20/20], Step [90/142], Loss: 0.7058\n",
            "Epoch [20/20], Step [91/142], Loss: 0.7780\n",
            "Epoch [20/20], Step [92/142], Loss: 0.8652\n",
            "Epoch [20/20], Step [93/142], Loss: 0.8471\n",
            "Epoch [20/20], Step [94/142], Loss: 0.9146\n",
            "Epoch [20/20], Step [95/142], Loss: 0.8079\n",
            "Epoch [20/20], Step [96/142], Loss: 0.8459\n",
            "Epoch [20/20], Step [97/142], Loss: 0.8887\n",
            "Epoch [20/20], Step [98/142], Loss: 0.8464\n",
            "Epoch [20/20], Step [99/142], Loss: 0.8136\n",
            "Epoch [20/20], Step [100/142], Loss: 0.8324\n",
            "Epoch [20/20], Step [101/142], Loss: 0.8951\n",
            "Epoch [20/20], Step [102/142], Loss: 0.9230\n",
            "Epoch [20/20], Step [103/142], Loss: 0.8188\n",
            "Epoch [20/20], Step [104/142], Loss: 0.8738\n",
            "Epoch [20/20], Step [105/142], Loss: 0.8620\n",
            "Epoch [20/20], Step [106/142], Loss: 0.8786\n",
            "Epoch [20/20], Step [107/142], Loss: 0.8666\n",
            "Epoch [20/20], Step [108/142], Loss: 0.8899\n",
            "Epoch [20/20], Step [109/142], Loss: 0.8626\n",
            "Epoch [20/20], Step [110/142], Loss: 0.7829\n",
            "Epoch [20/20], Step [111/142], Loss: 0.7260\n",
            "Epoch [20/20], Step [112/142], Loss: 0.8444\n",
            "Epoch [20/20], Step [113/142], Loss: 0.8088\n",
            "Epoch [20/20], Step [114/142], Loss: 0.9270\n",
            "Epoch [20/20], Step [115/142], Loss: 0.8837\n",
            "Epoch [20/20], Step [116/142], Loss: 0.8271\n",
            "Epoch [20/20], Step [117/142], Loss: 0.7400\n",
            "Epoch [20/20], Step [118/142], Loss: 0.8674\n",
            "Epoch [20/20], Step [119/142], Loss: 0.8016\n",
            "Epoch [20/20], Step [120/142], Loss: 0.9454\n",
            "Epoch [20/20], Step [121/142], Loss: 0.8654\n",
            "Epoch [20/20], Step [122/142], Loss: 0.8136\n",
            "Epoch [20/20], Step [123/142], Loss: 0.8503\n",
            "Epoch [20/20], Step [124/142], Loss: 0.7765\n",
            "Epoch [20/20], Step [125/142], Loss: 0.8352\n",
            "Epoch [20/20], Step [126/142], Loss: 0.8026\n",
            "Epoch [20/20], Step [127/142], Loss: 0.8604\n",
            "Epoch [20/20], Step [128/142], Loss: 0.7183\n",
            "Epoch [20/20], Step [129/142], Loss: 0.8967\n",
            "Epoch [20/20], Step [130/142], Loss: 0.8168\n",
            "Epoch [20/20], Step [131/142], Loss: 0.7910\n",
            "Epoch [20/20], Step [132/142], Loss: 0.8554\n",
            "Epoch [20/20], Step [133/142], Loss: 0.7848\n",
            "Epoch [20/20], Step [134/142], Loss: 0.7892\n",
            "Epoch [20/20], Step [135/142], Loss: 0.8462\n",
            "Epoch [20/20], Step [136/142], Loss: 0.7863\n",
            "Epoch [20/20], Step [137/142], Loss: 0.7975\n",
            "Epoch [20/20], Step [138/142], Loss: 0.7827\n",
            "Epoch [20/20], Step [139/142], Loss: 0.8280\n",
            "Epoch [20/20], Step [140/142], Loss: 0.9303\n",
            "Epoch [20/20], Step [141/142], Loss: 0.9234\n",
            "Epoch [20/20], Step [142/142], Loss: 0.8938\n",
            "Validation Accuracy: 0.6222\n",
            "Accuracy of RNN: 0.62\n",
            "Running RNN with rmsprop optimizer...\n",
            "Training RNN model with rmsprop optimizer using window size 5...\n",
            "Epoch [1/20], Step [1/142], Loss: 0.8830\n",
            "Epoch [1/20], Step [2/142], Loss: 0.8652\n",
            "Epoch [1/20], Step [3/142], Loss: 0.9559\n",
            "Epoch [1/20], Step [4/142], Loss: 0.9364\n",
            "Epoch [1/20], Step [5/142], Loss: 1.0063\n",
            "Epoch [1/20], Step [6/142], Loss: 0.8646\n",
            "Epoch [1/20], Step [7/142], Loss: 0.7863\n",
            "Epoch [1/20], Step [8/142], Loss: 0.8095\n",
            "Epoch [1/20], Step [9/142], Loss: 0.8508\n",
            "Epoch [1/20], Step [10/142], Loss: 0.8587\n",
            "Epoch [1/20], Step [11/142], Loss: 0.8392\n",
            "Epoch [1/20], Step [12/142], Loss: 0.8019\n",
            "Epoch [1/20], Step [13/142], Loss: 0.8894\n",
            "Epoch [1/20], Step [14/142], Loss: 0.8036\n",
            "Epoch [1/20], Step [15/142], Loss: 0.9430\n",
            "Epoch [1/20], Step [16/142], Loss: 0.8808\n",
            "Epoch [1/20], Step [17/142], Loss: 0.8579\n",
            "Epoch [1/20], Step [18/142], Loss: 0.8489\n",
            "Epoch [1/20], Step [19/142], Loss: 0.8321\n",
            "Epoch [1/20], Step [20/142], Loss: 0.9258\n",
            "Epoch [1/20], Step [21/142], Loss: 0.9640\n",
            "Epoch [1/20], Step [22/142], Loss: 0.8619\n",
            "Epoch [1/20], Step [23/142], Loss: 0.8765\n",
            "Epoch [1/20], Step [24/142], Loss: 0.8312\n",
            "Epoch [1/20], Step [25/142], Loss: 0.8645\n",
            "Epoch [1/20], Step [26/142], Loss: 0.8831\n",
            "Epoch [1/20], Step [27/142], Loss: 0.7691\n",
            "Epoch [1/20], Step [28/142], Loss: 0.8926\n",
            "Epoch [1/20], Step [29/142], Loss: 0.9885\n",
            "Epoch [1/20], Step [30/142], Loss: 0.8767\n",
            "Epoch [1/20], Step [31/142], Loss: 0.8567\n",
            "Epoch [1/20], Step [32/142], Loss: 0.8943\n",
            "Epoch [1/20], Step [33/142], Loss: 0.8351\n",
            "Epoch [1/20], Step [34/142], Loss: 0.8249\n",
            "Epoch [1/20], Step [35/142], Loss: 0.8300\n",
            "Epoch [1/20], Step [36/142], Loss: 0.8257\n",
            "Epoch [1/20], Step [37/142], Loss: 0.8544\n",
            "Epoch [1/20], Step [38/142], Loss: 0.7707\n",
            "Epoch [1/20], Step [39/142], Loss: 0.8284\n",
            "Epoch [1/20], Step [40/142], Loss: 0.8328\n",
            "Epoch [1/20], Step [41/142], Loss: 0.8587\n",
            "Epoch [1/20], Step [42/142], Loss: 0.9046\n",
            "Epoch [1/20], Step [43/142], Loss: 0.7882\n",
            "Epoch [1/20], Step [44/142], Loss: 0.8705\n",
            "Epoch [1/20], Step [45/142], Loss: 0.8796\n",
            "Epoch [1/20], Step [46/142], Loss: 0.8821\n",
            "Epoch [1/20], Step [47/142], Loss: 0.9054\n",
            "Epoch [1/20], Step [48/142], Loss: 0.9297\n",
            "Epoch [1/20], Step [49/142], Loss: 0.8495\n",
            "Epoch [1/20], Step [50/142], Loss: 0.8070\n",
            "Epoch [1/20], Step [51/142], Loss: 0.9325\n",
            "Epoch [1/20], Step [52/142], Loss: 0.7949\n",
            "Epoch [1/20], Step [53/142], Loss: 0.8872\n",
            "Epoch [1/20], Step [54/142], Loss: 0.8463\n",
            "Epoch [1/20], Step [55/142], Loss: 0.7805\n",
            "Epoch [1/20], Step [56/142], Loss: 0.8869\n",
            "Epoch [1/20], Step [57/142], Loss: 0.7950\n",
            "Epoch [1/20], Step [58/142], Loss: 0.8031\n",
            "Epoch [1/20], Step [59/142], Loss: 0.8820\n",
            "Epoch [1/20], Step [60/142], Loss: 0.7945\n",
            "Epoch [1/20], Step [61/142], Loss: 0.7620\n",
            "Epoch [1/20], Step [62/142], Loss: 0.8969\n",
            "Epoch [1/20], Step [63/142], Loss: 0.8273\n",
            "Epoch [1/20], Step [64/142], Loss: 0.9407\n",
            "Epoch [1/20], Step [65/142], Loss: 0.8552\n",
            "Epoch [1/20], Step [66/142], Loss: 0.8770\n",
            "Epoch [1/20], Step [67/142], Loss: 0.9066\n",
            "Epoch [1/20], Step [68/142], Loss: 0.8766\n",
            "Epoch [1/20], Step [69/142], Loss: 0.8023\n",
            "Epoch [1/20], Step [70/142], Loss: 0.8807\n",
            "Epoch [1/20], Step [71/142], Loss: 0.8432\n",
            "Epoch [1/20], Step [72/142], Loss: 0.8468\n",
            "Epoch [1/20], Step [73/142], Loss: 0.8042\n",
            "Epoch [1/20], Step [74/142], Loss: 0.8256\n",
            "Epoch [1/20], Step [75/142], Loss: 0.8644\n",
            "Epoch [1/20], Step [76/142], Loss: 0.8070\n",
            "Epoch [1/20], Step [77/142], Loss: 0.8553\n",
            "Epoch [1/20], Step [78/142], Loss: 0.8688\n",
            "Epoch [1/20], Step [79/142], Loss: 0.9239\n",
            "Epoch [1/20], Step [80/142], Loss: 0.8145\n",
            "Epoch [1/20], Step [81/142], Loss: 0.8062\n",
            "Epoch [1/20], Step [82/142], Loss: 0.8316\n",
            "Epoch [1/20], Step [83/142], Loss: 0.8222\n",
            "Epoch [1/20], Step [84/142], Loss: 0.9489\n",
            "Epoch [1/20], Step [85/142], Loss: 0.8194\n",
            "Epoch [1/20], Step [86/142], Loss: 0.9048\n",
            "Epoch [1/20], Step [87/142], Loss: 0.8493\n",
            "Epoch [1/20], Step [88/142], Loss: 0.8594\n",
            "Epoch [1/20], Step [89/142], Loss: 0.7538\n",
            "Epoch [1/20], Step [90/142], Loss: 0.8287\n",
            "Epoch [1/20], Step [91/142], Loss: 0.9486\n",
            "Epoch [1/20], Step [92/142], Loss: 0.8288\n",
            "Epoch [1/20], Step [93/142], Loss: 0.9386\n",
            "Epoch [1/20], Step [94/142], Loss: 0.8224\n",
            "Epoch [1/20], Step [95/142], Loss: 0.8161\n",
            "Epoch [1/20], Step [96/142], Loss: 0.8482\n",
            "Epoch [1/20], Step [97/142], Loss: 0.7904\n",
            "Epoch [1/20], Step [98/142], Loss: 0.8158\n",
            "Epoch [1/20], Step [99/142], Loss: 0.7825\n",
            "Epoch [1/20], Step [100/142], Loss: 0.8361\n",
            "Epoch [1/20], Step [101/142], Loss: 0.8273\n",
            "Epoch [1/20], Step [102/142], Loss: 0.8923\n",
            "Epoch [1/20], Step [103/142], Loss: 0.8500\n",
            "Epoch [1/20], Step [104/142], Loss: 0.9439\n",
            "Epoch [1/20], Step [105/142], Loss: 0.8746\n",
            "Epoch [1/20], Step [106/142], Loss: 0.8231\n",
            "Epoch [1/20], Step [107/142], Loss: 0.8632\n",
            "Epoch [1/20], Step [108/142], Loss: 0.8936\n",
            "Epoch [1/20], Step [109/142], Loss: 0.8714\n",
            "Epoch [1/20], Step [110/142], Loss: 0.8176\n",
            "Epoch [1/20], Step [111/142], Loss: 0.8742\n",
            "Epoch [1/20], Step [112/142], Loss: 0.7969\n",
            "Epoch [1/20], Step [113/142], Loss: 0.8500\n",
            "Epoch [1/20], Step [114/142], Loss: 0.8473\n",
            "Epoch [1/20], Step [115/142], Loss: 0.8576\n",
            "Epoch [1/20], Step [116/142], Loss: 0.8292\n",
            "Epoch [1/20], Step [117/142], Loss: 0.8220\n",
            "Epoch [1/20], Step [118/142], Loss: 0.8995\n",
            "Epoch [1/20], Step [119/142], Loss: 0.8722\n",
            "Epoch [1/20], Step [120/142], Loss: 0.7962\n",
            "Epoch [1/20], Step [121/142], Loss: 0.8346\n",
            "Epoch [1/20], Step [122/142], Loss: 0.7676\n",
            "Epoch [1/20], Step [123/142], Loss: 0.8199\n",
            "Epoch [1/20], Step [124/142], Loss: 0.8720\n",
            "Epoch [1/20], Step [125/142], Loss: 0.8358\n",
            "Epoch [1/20], Step [126/142], Loss: 0.9571\n",
            "Epoch [1/20], Step [127/142], Loss: 0.8762\n",
            "Epoch [1/20], Step [128/142], Loss: 0.8670\n",
            "Epoch [1/20], Step [129/142], Loss: 0.7687\n",
            "Epoch [1/20], Step [130/142], Loss: 0.8416\n",
            "Epoch [1/20], Step [131/142], Loss: 0.8384\n",
            "Epoch [1/20], Step [132/142], Loss: 0.9255\n",
            "Epoch [1/20], Step [133/142], Loss: 0.9267\n",
            "Epoch [1/20], Step [134/142], Loss: 0.8464\n",
            "Epoch [1/20], Step [135/142], Loss: 0.8771\n",
            "Epoch [1/20], Step [136/142], Loss: 0.8554\n",
            "Epoch [1/20], Step [137/142], Loss: 0.8097\n",
            "Epoch [1/20], Step [138/142], Loss: 0.8450\n",
            "Epoch [1/20], Step [139/142], Loss: 0.8665\n",
            "Epoch [1/20], Step [140/142], Loss: 0.8818\n",
            "Epoch [1/20], Step [141/142], Loss: 0.8177\n",
            "Epoch [1/20], Step [142/142], Loss: 0.7685\n",
            "Epoch [2/20], Step [1/142], Loss: 0.8953\n",
            "Epoch [2/20], Step [2/142], Loss: 0.8547\n",
            "Epoch [2/20], Step [3/142], Loss: 0.8245\n",
            "Epoch [2/20], Step [4/142], Loss: 0.8852\n",
            "Epoch [2/20], Step [5/142], Loss: 0.8915\n",
            "Epoch [2/20], Step [6/142], Loss: 0.8811\n",
            "Epoch [2/20], Step [7/142], Loss: 0.9469\n",
            "Epoch [2/20], Step [8/142], Loss: 0.8434\n",
            "Epoch [2/20], Step [9/142], Loss: 0.7363\n",
            "Epoch [2/20], Step [10/142], Loss: 0.8324\n",
            "Epoch [2/20], Step [11/142], Loss: 0.8434\n",
            "Epoch [2/20], Step [12/142], Loss: 0.7612\n",
            "Epoch [2/20], Step [13/142], Loss: 0.9170\n",
            "Epoch [2/20], Step [14/142], Loss: 0.9002\n",
            "Epoch [2/20], Step [15/142], Loss: 0.8981\n",
            "Epoch [2/20], Step [16/142], Loss: 0.8948\n",
            "Epoch [2/20], Step [17/142], Loss: 0.7954\n",
            "Epoch [2/20], Step [18/142], Loss: 0.8048\n",
            "Epoch [2/20], Step [19/142], Loss: 0.8517\n",
            "Epoch [2/20], Step [20/142], Loss: 0.9429\n",
            "Epoch [2/20], Step [21/142], Loss: 0.8432\n",
            "Epoch [2/20], Step [22/142], Loss: 0.8126\n",
            "Epoch [2/20], Step [23/142], Loss: 0.8599\n",
            "Epoch [2/20], Step [24/142], Loss: 0.8434\n",
            "Epoch [2/20], Step [25/142], Loss: 0.8193\n",
            "Epoch [2/20], Step [26/142], Loss: 0.8294\n",
            "Epoch [2/20], Step [27/142], Loss: 0.7105\n",
            "Epoch [2/20], Step [28/142], Loss: 0.8124\n",
            "Epoch [2/20], Step [29/142], Loss: 0.7525\n",
            "Epoch [2/20], Step [30/142], Loss: 0.8157\n",
            "Epoch [2/20], Step [31/142], Loss: 0.7674\n",
            "Epoch [2/20], Step [32/142], Loss: 0.7415\n",
            "Epoch [2/20], Step [33/142], Loss: 0.8423\n",
            "Epoch [2/20], Step [34/142], Loss: 0.8619\n",
            "Epoch [2/20], Step [35/142], Loss: 0.7721\n",
            "Epoch [2/20], Step [36/142], Loss: 0.9713\n",
            "Epoch [2/20], Step [37/142], Loss: 0.9113\n",
            "Epoch [2/20], Step [38/142], Loss: 0.8048\n",
            "Epoch [2/20], Step [39/142], Loss: 0.8062\n",
            "Epoch [2/20], Step [40/142], Loss: 1.0185\n",
            "Epoch [2/20], Step [41/142], Loss: 0.8184\n",
            "Epoch [2/20], Step [42/142], Loss: 0.9769\n",
            "Epoch [2/20], Step [43/142], Loss: 0.9262\n",
            "Epoch [2/20], Step [44/142], Loss: 0.8250\n",
            "Epoch [2/20], Step [45/142], Loss: 0.8466\n",
            "Epoch [2/20], Step [46/142], Loss: 0.8306\n",
            "Epoch [2/20], Step [47/142], Loss: 0.8859\n",
            "Epoch [2/20], Step [48/142], Loss: 0.7856\n",
            "Epoch [2/20], Step [49/142], Loss: 0.9117\n",
            "Epoch [2/20], Step [50/142], Loss: 0.8810\n",
            "Epoch [2/20], Step [51/142], Loss: 0.8001\n",
            "Epoch [2/20], Step [52/142], Loss: 0.8023\n",
            "Epoch [2/20], Step [53/142], Loss: 0.8553\n",
            "Epoch [2/20], Step [54/142], Loss: 0.7756\n",
            "Epoch [2/20], Step [55/142], Loss: 0.8458\n",
            "Epoch [2/20], Step [56/142], Loss: 0.8188\n",
            "Epoch [2/20], Step [57/142], Loss: 0.7683\n",
            "Epoch [2/20], Step [58/142], Loss: 0.8879\n",
            "Epoch [2/20], Step [59/142], Loss: 0.8537\n",
            "Epoch [2/20], Step [60/142], Loss: 0.8284\n",
            "Epoch [2/20], Step [61/142], Loss: 0.9053\n",
            "Epoch [2/20], Step [62/142], Loss: 0.8571\n",
            "Epoch [2/20], Step [63/142], Loss: 0.7807\n",
            "Epoch [2/20], Step [64/142], Loss: 0.9243\n",
            "Epoch [2/20], Step [65/142], Loss: 0.8483\n",
            "Epoch [2/20], Step [66/142], Loss: 0.8769\n",
            "Epoch [2/20], Step [67/142], Loss: 0.8595\n",
            "Epoch [2/20], Step [68/142], Loss: 0.9291\n",
            "Epoch [2/20], Step [69/142], Loss: 0.8884\n",
            "Epoch [2/20], Step [70/142], Loss: 0.8853\n",
            "Epoch [2/20], Step [71/142], Loss: 0.7894\n",
            "Epoch [2/20], Step [72/142], Loss: 0.7462\n",
            "Epoch [2/20], Step [73/142], Loss: 0.7591\n",
            "Epoch [2/20], Step [74/142], Loss: 0.9067\n",
            "Epoch [2/20], Step [75/142], Loss: 0.8463\n",
            "Epoch [2/20], Step [76/142], Loss: 0.8848\n",
            "Epoch [2/20], Step [77/142], Loss: 0.7880\n",
            "Epoch [2/20], Step [78/142], Loss: 0.8099\n",
            "Epoch [2/20], Step [79/142], Loss: 0.9232\n",
            "Epoch [2/20], Step [80/142], Loss: 0.9309\n",
            "Epoch [2/20], Step [81/142], Loss: 0.7377\n",
            "Epoch [2/20], Step [82/142], Loss: 0.9750\n",
            "Epoch [2/20], Step [83/142], Loss: 0.8326\n",
            "Epoch [2/20], Step [84/142], Loss: 0.8559\n",
            "Epoch [2/20], Step [85/142], Loss: 0.9011\n",
            "Epoch [2/20], Step [86/142], Loss: 0.8583\n",
            "Epoch [2/20], Step [87/142], Loss: 0.7626\n",
            "Epoch [2/20], Step [88/142], Loss: 0.8283\n",
            "Epoch [2/20], Step [89/142], Loss: 0.9617\n",
            "Epoch [2/20], Step [90/142], Loss: 0.8568\n",
            "Epoch [2/20], Step [91/142], Loss: 0.9066\n",
            "Epoch [2/20], Step [92/142], Loss: 0.7936\n",
            "Epoch [2/20], Step [93/142], Loss: 0.8277\n",
            "Epoch [2/20], Step [94/142], Loss: 0.7759\n",
            "Epoch [2/20], Step [95/142], Loss: 0.8722\n",
            "Epoch [2/20], Step [96/142], Loss: 0.7595\n",
            "Epoch [2/20], Step [97/142], Loss: 0.6653\n",
            "Epoch [2/20], Step [98/142], Loss: 0.8386\n",
            "Epoch [2/20], Step [99/142], Loss: 0.9067\n",
            "Epoch [2/20], Step [100/142], Loss: 0.8231\n",
            "Epoch [2/20], Step [101/142], Loss: 0.8730\n",
            "Epoch [2/20], Step [102/142], Loss: 0.8531\n",
            "Epoch [2/20], Step [103/142], Loss: 0.9044\n",
            "Epoch [2/20], Step [104/142], Loss: 0.8718\n",
            "Epoch [2/20], Step [105/142], Loss: 0.9479\n",
            "Epoch [2/20], Step [106/142], Loss: 1.0004\n",
            "Epoch [2/20], Step [107/142], Loss: 0.8474\n",
            "Epoch [2/20], Step [108/142], Loss: 0.8703\n",
            "Epoch [2/20], Step [109/142], Loss: 0.8828\n",
            "Epoch [2/20], Step [110/142], Loss: 0.8517\n",
            "Epoch [2/20], Step [111/142], Loss: 0.8702\n",
            "Epoch [2/20], Step [112/142], Loss: 0.9150\n",
            "Epoch [2/20], Step [113/142], Loss: 0.8330\n",
            "Epoch [2/20], Step [114/142], Loss: 0.9017\n",
            "Epoch [2/20], Step [115/142], Loss: 0.7354\n",
            "Epoch [2/20], Step [116/142], Loss: 0.8500\n",
            "Epoch [2/20], Step [117/142], Loss: 0.9034\n",
            "Epoch [2/20], Step [118/142], Loss: 0.8746\n",
            "Epoch [2/20], Step [119/142], Loss: 0.9129\n",
            "Epoch [2/20], Step [120/142], Loss: 0.8794\n",
            "Epoch [2/20], Step [121/142], Loss: 0.8685\n",
            "Epoch [2/20], Step [122/142], Loss: 0.7839\n",
            "Epoch [2/20], Step [123/142], Loss: 0.8553\n",
            "Epoch [2/20], Step [124/142], Loss: 0.7952\n",
            "Epoch [2/20], Step [125/142], Loss: 0.8178\n",
            "Epoch [2/20], Step [126/142], Loss: 0.8198\n",
            "Epoch [2/20], Step [127/142], Loss: 0.8652\n",
            "Epoch [2/20], Step [128/142], Loss: 0.9176\n",
            "Epoch [2/20], Step [129/142], Loss: 0.8668\n",
            "Epoch [2/20], Step [130/142], Loss: 0.7745\n",
            "Epoch [2/20], Step [131/142], Loss: 0.8931\n",
            "Epoch [2/20], Step [132/142], Loss: 0.8740\n",
            "Epoch [2/20], Step [133/142], Loss: 0.8681\n",
            "Epoch [2/20], Step [134/142], Loss: 0.8354\n",
            "Epoch [2/20], Step [135/142], Loss: 0.9241\n",
            "Epoch [2/20], Step [136/142], Loss: 0.8920\n",
            "Epoch [2/20], Step [137/142], Loss: 0.8195\n",
            "Epoch [2/20], Step [138/142], Loss: 0.8359\n",
            "Epoch [2/20], Step [139/142], Loss: 0.7577\n",
            "Epoch [2/20], Step [140/142], Loss: 0.8568\n",
            "Epoch [2/20], Step [141/142], Loss: 0.8420\n",
            "Epoch [2/20], Step [142/142], Loss: 0.8405\n",
            "Epoch [3/20], Step [1/142], Loss: 0.8928\n",
            "Epoch [3/20], Step [2/142], Loss: 0.7831\n",
            "Epoch [3/20], Step [3/142], Loss: 0.7495\n",
            "Epoch [3/20], Step [4/142], Loss: 0.8811\n",
            "Epoch [3/20], Step [5/142], Loss: 0.8875\n",
            "Epoch [3/20], Step [6/142], Loss: 0.8538\n",
            "Epoch [3/20], Step [7/142], Loss: 0.8958\n",
            "Epoch [3/20], Step [8/142], Loss: 0.8189\n",
            "Epoch [3/20], Step [9/142], Loss: 0.7929\n",
            "Epoch [3/20], Step [10/142], Loss: 0.9712\n",
            "Epoch [3/20], Step [11/142], Loss: 0.8350\n",
            "Epoch [3/20], Step [12/142], Loss: 0.9565\n",
            "Epoch [3/20], Step [13/142], Loss: 0.8035\n",
            "Epoch [3/20], Step [14/142], Loss: 0.8638\n",
            "Epoch [3/20], Step [15/142], Loss: 0.8881\n",
            "Epoch [3/20], Step [16/142], Loss: 0.9300\n",
            "Epoch [3/20], Step [17/142], Loss: 0.8447\n",
            "Epoch [3/20], Step [18/142], Loss: 0.8241\n",
            "Epoch [3/20], Step [19/142], Loss: 0.8279\n",
            "Epoch [3/20], Step [20/142], Loss: 0.9021\n",
            "Epoch [3/20], Step [21/142], Loss: 0.8577\n",
            "Epoch [3/20], Step [22/142], Loss: 0.8634\n",
            "Epoch [3/20], Step [23/142], Loss: 0.8156\n",
            "Epoch [3/20], Step [24/142], Loss: 0.7861\n",
            "Epoch [3/20], Step [25/142], Loss: 0.7948\n",
            "Epoch [3/20], Step [26/142], Loss: 0.8190\n",
            "Epoch [3/20], Step [27/142], Loss: 0.8305\n",
            "Epoch [3/20], Step [28/142], Loss: 0.8619\n",
            "Epoch [3/20], Step [29/142], Loss: 0.8592\n",
            "Epoch [3/20], Step [30/142], Loss: 0.8831\n",
            "Epoch [3/20], Step [31/142], Loss: 0.8509\n",
            "Epoch [3/20], Step [32/142], Loss: 0.8231\n",
            "Epoch [3/20], Step [33/142], Loss: 0.8922\n",
            "Epoch [3/20], Step [34/142], Loss: 0.8804\n",
            "Epoch [3/20], Step [35/142], Loss: 0.9059\n",
            "Epoch [3/20], Step [36/142], Loss: 0.9172\n",
            "Epoch [3/20], Step [37/142], Loss: 0.8589\n",
            "Epoch [3/20], Step [38/142], Loss: 0.8272\n",
            "Epoch [3/20], Step [39/142], Loss: 0.8898\n",
            "Epoch [3/20], Step [40/142], Loss: 0.8662\n",
            "Epoch [3/20], Step [41/142], Loss: 0.8374\n",
            "Epoch [3/20], Step [42/142], Loss: 0.8579\n",
            "Epoch [3/20], Step [43/142], Loss: 0.9007\n",
            "Epoch [3/20], Step [44/142], Loss: 0.7908\n",
            "Epoch [3/20], Step [45/142], Loss: 0.9144\n",
            "Epoch [3/20], Step [46/142], Loss: 0.8201\n",
            "Epoch [3/20], Step [47/142], Loss: 0.7831\n",
            "Epoch [3/20], Step [48/142], Loss: 0.8787\n",
            "Epoch [3/20], Step [49/142], Loss: 0.9132\n",
            "Epoch [3/20], Step [50/142], Loss: 0.7969\n",
            "Epoch [3/20], Step [51/142], Loss: 0.7931\n",
            "Epoch [3/20], Step [52/142], Loss: 0.7833\n",
            "Epoch [3/20], Step [53/142], Loss: 0.8448\n",
            "Epoch [3/20], Step [54/142], Loss: 0.7824\n",
            "Epoch [3/20], Step [55/142], Loss: 0.8232\n",
            "Epoch [3/20], Step [56/142], Loss: 0.9193\n",
            "Epoch [3/20], Step [57/142], Loss: 0.8940\n",
            "Epoch [3/20], Step [58/142], Loss: 0.9484\n",
            "Epoch [3/20], Step [59/142], Loss: 0.8282\n",
            "Epoch [3/20], Step [60/142], Loss: 0.8375\n",
            "Epoch [3/20], Step [61/142], Loss: 0.9227\n",
            "Epoch [3/20], Step [62/142], Loss: 0.8307\n",
            "Epoch [3/20], Step [63/142], Loss: 0.8620\n",
            "Epoch [3/20], Step [64/142], Loss: 0.7383\n",
            "Epoch [3/20], Step [65/142], Loss: 0.9418\n",
            "Epoch [3/20], Step [66/142], Loss: 0.7687\n",
            "Epoch [3/20], Step [67/142], Loss: 0.9002\n",
            "Epoch [3/20], Step [68/142], Loss: 0.8048\n",
            "Epoch [3/20], Step [69/142], Loss: 0.8431\n",
            "Epoch [3/20], Step [70/142], Loss: 0.9058\n",
            "Epoch [3/20], Step [71/142], Loss: 0.8174\n",
            "Epoch [3/20], Step [72/142], Loss: 0.8672\n",
            "Epoch [3/20], Step [73/142], Loss: 0.9568\n",
            "Epoch [3/20], Step [74/142], Loss: 0.8366\n",
            "Epoch [3/20], Step [75/142], Loss: 0.8861\n",
            "Epoch [3/20], Step [76/142], Loss: 0.8096\n",
            "Epoch [3/20], Step [77/142], Loss: 0.9447\n",
            "Epoch [3/20], Step [78/142], Loss: 0.8382\n",
            "Epoch [3/20], Step [79/142], Loss: 0.8714\n",
            "Epoch [3/20], Step [80/142], Loss: 0.8502\n",
            "Epoch [3/20], Step [81/142], Loss: 0.8109\n",
            "Epoch [3/20], Step [82/142], Loss: 0.8346\n",
            "Epoch [3/20], Step [83/142], Loss: 0.8713\n",
            "Epoch [3/20], Step [84/142], Loss: 0.9037\n",
            "Epoch [3/20], Step [85/142], Loss: 0.8062\n",
            "Epoch [3/20], Step [86/142], Loss: 0.8453\n",
            "Epoch [3/20], Step [87/142], Loss: 0.8279\n",
            "Epoch [3/20], Step [88/142], Loss: 0.7842\n",
            "Epoch [3/20], Step [89/142], Loss: 0.8305\n",
            "Epoch [3/20], Step [90/142], Loss: 0.8113\n",
            "Epoch [3/20], Step [91/142], Loss: 0.8232\n",
            "Epoch [3/20], Step [92/142], Loss: 0.8892\n",
            "Epoch [3/20], Step [93/142], Loss: 0.6910\n",
            "Epoch [3/20], Step [94/142], Loss: 0.8780\n",
            "Epoch [3/20], Step [95/142], Loss: 0.9134\n",
            "Epoch [3/20], Step [96/142], Loss: 0.7919\n",
            "Epoch [3/20], Step [97/142], Loss: 0.8506\n",
            "Epoch [3/20], Step [98/142], Loss: 0.8808\n",
            "Epoch [3/20], Step [99/142], Loss: 0.7652\n",
            "Epoch [3/20], Step [100/142], Loss: 0.7720\n",
            "Epoch [3/20], Step [101/142], Loss: 0.9110\n",
            "Epoch [3/20], Step [102/142], Loss: 0.8277\n",
            "Epoch [3/20], Step [103/142], Loss: 0.8023\n",
            "Epoch [3/20], Step [104/142], Loss: 0.7687\n",
            "Epoch [3/20], Step [105/142], Loss: 0.8846\n",
            "Epoch [3/20], Step [106/142], Loss: 0.7990\n",
            "Epoch [3/20], Step [107/142], Loss: 0.8049\n",
            "Epoch [3/20], Step [108/142], Loss: 0.8226\n",
            "Epoch [3/20], Step [109/142], Loss: 0.9049\n",
            "Epoch [3/20], Step [110/142], Loss: 0.7314\n",
            "Epoch [3/20], Step [111/142], Loss: 0.8149\n",
            "Epoch [3/20], Step [112/142], Loss: 0.7449\n",
            "Epoch [3/20], Step [113/142], Loss: 0.8102\n",
            "Epoch [3/20], Step [114/142], Loss: 0.6980\n",
            "Epoch [3/20], Step [115/142], Loss: 0.9360\n",
            "Epoch [3/20], Step [116/142], Loss: 0.7551\n",
            "Epoch [3/20], Step [117/142], Loss: 0.9001\n",
            "Epoch [3/20], Step [118/142], Loss: 0.8213\n",
            "Epoch [3/20], Step [119/142], Loss: 0.8167\n",
            "Epoch [3/20], Step [120/142], Loss: 0.8255\n",
            "Epoch [3/20], Step [121/142], Loss: 0.7881\n",
            "Epoch [3/20], Step [122/142], Loss: 0.9499\n",
            "Epoch [3/20], Step [123/142], Loss: 0.8824\n",
            "Epoch [3/20], Step [124/142], Loss: 0.8818\n",
            "Epoch [3/20], Step [125/142], Loss: 0.8163\n",
            "Epoch [3/20], Step [126/142], Loss: 0.8849\n",
            "Epoch [3/20], Step [127/142], Loss: 0.7884\n",
            "Epoch [3/20], Step [128/142], Loss: 0.9093\n",
            "Epoch [3/20], Step [129/142], Loss: 0.8714\n",
            "Epoch [3/20], Step [130/142], Loss: 0.7992\n",
            "Epoch [3/20], Step [131/142], Loss: 0.9453\n",
            "Epoch [3/20], Step [132/142], Loss: 0.8967\n",
            "Epoch [3/20], Step [133/142], Loss: 0.8699\n",
            "Epoch [3/20], Step [134/142], Loss: 0.9414\n",
            "Epoch [3/20], Step [135/142], Loss: 0.8341\n",
            "Epoch [3/20], Step [136/142], Loss: 0.8640\n",
            "Epoch [3/20], Step [137/142], Loss: 0.8707\n",
            "Epoch [3/20], Step [138/142], Loss: 0.8967\n",
            "Epoch [3/20], Step [139/142], Loss: 0.8495\n",
            "Epoch [3/20], Step [140/142], Loss: 0.7809\n",
            "Epoch [3/20], Step [141/142], Loss: 0.9457\n",
            "Epoch [3/20], Step [142/142], Loss: 0.8319\n",
            "Epoch [4/20], Step [1/142], Loss: 0.9901\n",
            "Epoch [4/20], Step [2/142], Loss: 0.8496\n",
            "Epoch [4/20], Step [3/142], Loss: 0.8044\n",
            "Epoch [4/20], Step [4/142], Loss: 0.8975\n",
            "Epoch [4/20], Step [5/142], Loss: 0.9208\n",
            "Epoch [4/20], Step [6/142], Loss: 0.9033\n",
            "Epoch [4/20], Step [7/142], Loss: 0.7849\n",
            "Epoch [4/20], Step [8/142], Loss: 0.8608\n",
            "Epoch [4/20], Step [9/142], Loss: 0.7999\n",
            "Epoch [4/20], Step [10/142], Loss: 0.7793\n",
            "Epoch [4/20], Step [11/142], Loss: 0.9254\n",
            "Epoch [4/20], Step [12/142], Loss: 0.7648\n",
            "Epoch [4/20], Step [13/142], Loss: 0.8849\n",
            "Epoch [4/20], Step [14/142], Loss: 0.8432\n",
            "Epoch [4/20], Step [15/142], Loss: 0.8320\n",
            "Epoch [4/20], Step [16/142], Loss: 0.8381\n",
            "Epoch [4/20], Step [17/142], Loss: 0.8688\n",
            "Epoch [4/20], Step [18/142], Loss: 0.9076\n",
            "Epoch [4/20], Step [19/142], Loss: 0.8100\n",
            "Epoch [4/20], Step [20/142], Loss: 0.8044\n",
            "Epoch [4/20], Step [21/142], Loss: 0.8173\n",
            "Epoch [4/20], Step [22/142], Loss: 0.7909\n",
            "Epoch [4/20], Step [23/142], Loss: 0.7781\n",
            "Epoch [4/20], Step [24/142], Loss: 0.9531\n",
            "Epoch [4/20], Step [25/142], Loss: 0.8005\n",
            "Epoch [4/20], Step [26/142], Loss: 0.8409\n",
            "Epoch [4/20], Step [27/142], Loss: 0.8002\n",
            "Epoch [4/20], Step [28/142], Loss: 0.8499\n",
            "Epoch [4/20], Step [29/142], Loss: 0.7928\n",
            "Epoch [4/20], Step [30/142], Loss: 0.8826\n",
            "Epoch [4/20], Step [31/142], Loss: 0.8415\n",
            "Epoch [4/20], Step [32/142], Loss: 0.7657\n",
            "Epoch [4/20], Step [33/142], Loss: 0.9232\n",
            "Epoch [4/20], Step [34/142], Loss: 0.8346\n",
            "Epoch [4/20], Step [35/142], Loss: 0.8484\n",
            "Epoch [4/20], Step [36/142], Loss: 0.8273\n",
            "Epoch [4/20], Step [37/142], Loss: 0.8443\n",
            "Epoch [4/20], Step [38/142], Loss: 0.8361\n",
            "Epoch [4/20], Step [39/142], Loss: 0.8050\n",
            "Epoch [4/20], Step [40/142], Loss: 0.9200\n",
            "Epoch [4/20], Step [41/142], Loss: 0.8834\n",
            "Epoch [4/20], Step [42/142], Loss: 0.7792\n",
            "Epoch [4/20], Step [43/142], Loss: 0.8487\n",
            "Epoch [4/20], Step [44/142], Loss: 0.8272\n",
            "Epoch [4/20], Step [45/142], Loss: 0.8049\n",
            "Epoch [4/20], Step [46/142], Loss: 0.7761\n",
            "Epoch [4/20], Step [47/142], Loss: 0.8461\n",
            "Epoch [4/20], Step [48/142], Loss: 0.8453\n",
            "Epoch [4/20], Step [49/142], Loss: 0.8468\n",
            "Epoch [4/20], Step [50/142], Loss: 0.8317\n",
            "Epoch [4/20], Step [51/142], Loss: 0.8375\n",
            "Epoch [4/20], Step [52/142], Loss: 0.8197\n",
            "Epoch [4/20], Step [53/142], Loss: 0.8014\n",
            "Epoch [4/20], Step [54/142], Loss: 0.8941\n",
            "Epoch [4/20], Step [55/142], Loss: 0.8758\n",
            "Epoch [4/20], Step [56/142], Loss: 0.8773\n",
            "Epoch [4/20], Step [57/142], Loss: 0.8580\n",
            "Epoch [4/20], Step [58/142], Loss: 0.8409\n",
            "Epoch [4/20], Step [59/142], Loss: 0.9341\n",
            "Epoch [4/20], Step [60/142], Loss: 0.9430\n",
            "Epoch [4/20], Step [61/142], Loss: 0.8273\n",
            "Epoch [4/20], Step [62/142], Loss: 0.8070\n",
            "Epoch [4/20], Step [63/142], Loss: 0.7827\n",
            "Epoch [4/20], Step [64/142], Loss: 0.8761\n",
            "Epoch [4/20], Step [65/142], Loss: 0.8925\n",
            "Epoch [4/20], Step [66/142], Loss: 0.8582\n",
            "Epoch [4/20], Step [67/142], Loss: 0.7888\n",
            "Epoch [4/20], Step [68/142], Loss: 0.7937\n",
            "Epoch [4/20], Step [69/142], Loss: 0.8689\n",
            "Epoch [4/20], Step [70/142], Loss: 0.9235\n",
            "Epoch [4/20], Step [71/142], Loss: 0.8746\n",
            "Epoch [4/20], Step [72/142], Loss: 0.9059\n",
            "Epoch [4/20], Step [73/142], Loss: 0.8015\n",
            "Epoch [4/20], Step [74/142], Loss: 0.9035\n",
            "Epoch [4/20], Step [75/142], Loss: 0.8501\n",
            "Epoch [4/20], Step [76/142], Loss: 0.8676\n",
            "Epoch [4/20], Step [77/142], Loss: 0.8468\n",
            "Epoch [4/20], Step [78/142], Loss: 0.7846\n",
            "Epoch [4/20], Step [79/142], Loss: 0.8741\n",
            "Epoch [4/20], Step [80/142], Loss: 0.8893\n",
            "Epoch [4/20], Step [81/142], Loss: 0.8028\n",
            "Epoch [4/20], Step [82/142], Loss: 0.8679\n",
            "Epoch [4/20], Step [83/142], Loss: 0.8742\n",
            "Epoch [4/20], Step [84/142], Loss: 0.9293\n",
            "Epoch [4/20], Step [85/142], Loss: 0.8426\n",
            "Epoch [4/20], Step [86/142], Loss: 0.9558\n",
            "Epoch [4/20], Step [87/142], Loss: 0.7989\n",
            "Epoch [4/20], Step [88/142], Loss: 0.8142\n",
            "Epoch [4/20], Step [89/142], Loss: 0.8112\n",
            "Epoch [4/20], Step [90/142], Loss: 0.9002\n",
            "Epoch [4/20], Step [91/142], Loss: 0.8886\n",
            "Epoch [4/20], Step [92/142], Loss: 0.7984\n",
            "Epoch [4/20], Step [93/142], Loss: 0.9477\n",
            "Epoch [4/20], Step [94/142], Loss: 0.8199\n",
            "Epoch [4/20], Step [95/142], Loss: 0.8261\n",
            "Epoch [4/20], Step [96/142], Loss: 0.9610\n",
            "Epoch [4/20], Step [97/142], Loss: 0.8628\n",
            "Epoch [4/20], Step [98/142], Loss: 0.7889\n",
            "Epoch [4/20], Step [99/142], Loss: 0.8215\n",
            "Epoch [4/20], Step [100/142], Loss: 0.8742\n",
            "Epoch [4/20], Step [101/142], Loss: 0.8558\n",
            "Epoch [4/20], Step [102/142], Loss: 0.7560\n",
            "Epoch [4/20], Step [103/142], Loss: 0.7385\n",
            "Epoch [4/20], Step [104/142], Loss: 0.8955\n",
            "Epoch [4/20], Step [105/142], Loss: 0.8597\n",
            "Epoch [4/20], Step [106/142], Loss: 0.7778\n",
            "Epoch [4/20], Step [107/142], Loss: 0.8228\n",
            "Epoch [4/20], Step [108/142], Loss: 0.6914\n",
            "Epoch [4/20], Step [109/142], Loss: 0.8609\n",
            "Epoch [4/20], Step [110/142], Loss: 0.8210\n",
            "Epoch [4/20], Step [111/142], Loss: 0.9051\n",
            "Epoch [4/20], Step [112/142], Loss: 0.7255\n",
            "Epoch [4/20], Step [113/142], Loss: 0.8041\n",
            "Epoch [4/20], Step [114/142], Loss: 0.8482\n",
            "Epoch [4/20], Step [115/142], Loss: 0.8305\n",
            "Epoch [4/20], Step [116/142], Loss: 0.8661\n",
            "Epoch [4/20], Step [117/142], Loss: 0.8511\n",
            "Epoch [4/20], Step [118/142], Loss: 0.7627\n",
            "Epoch [4/20], Step [119/142], Loss: 0.8279\n",
            "Epoch [4/20], Step [120/142], Loss: 0.8444\n",
            "Epoch [4/20], Step [121/142], Loss: 0.9198\n",
            "Epoch [4/20], Step [122/142], Loss: 0.8559\n",
            "Epoch [4/20], Step [123/142], Loss: 0.8474\n",
            "Epoch [4/20], Step [124/142], Loss: 0.7873\n",
            "Epoch [4/20], Step [125/142], Loss: 0.9692\n",
            "Epoch [4/20], Step [126/142], Loss: 0.8669\n",
            "Epoch [4/20], Step [127/142], Loss: 0.8140\n",
            "Epoch [4/20], Step [128/142], Loss: 0.8901\n",
            "Epoch [4/20], Step [129/142], Loss: 0.9497\n",
            "Epoch [4/20], Step [130/142], Loss: 0.8376\n",
            "Epoch [4/20], Step [131/142], Loss: 0.8081\n",
            "Epoch [4/20], Step [132/142], Loss: 0.8018\n",
            "Epoch [4/20], Step [133/142], Loss: 0.8642\n",
            "Epoch [4/20], Step [134/142], Loss: 0.9916\n",
            "Epoch [4/20], Step [135/142], Loss: 0.9600\n",
            "Epoch [4/20], Step [136/142], Loss: 0.8442\n",
            "Epoch [4/20], Step [137/142], Loss: 0.8594\n",
            "Epoch [4/20], Step [138/142], Loss: 0.8253\n",
            "Epoch [4/20], Step [139/142], Loss: 0.9185\n",
            "Epoch [4/20], Step [140/142], Loss: 0.8898\n",
            "Epoch [4/20], Step [141/142], Loss: 0.7997\n",
            "Epoch [4/20], Step [142/142], Loss: 0.6806\n",
            "Epoch [5/20], Step [1/142], Loss: 0.7368\n",
            "Epoch [5/20], Step [2/142], Loss: 0.9208\n",
            "Epoch [5/20], Step [3/142], Loss: 0.8346\n",
            "Epoch [5/20], Step [4/142], Loss: 0.7435\n",
            "Epoch [5/20], Step [5/142], Loss: 0.8347\n",
            "Epoch [5/20], Step [6/142], Loss: 0.8626\n",
            "Epoch [5/20], Step [7/142], Loss: 0.8809\n",
            "Epoch [5/20], Step [8/142], Loss: 0.9303\n",
            "Epoch [5/20], Step [9/142], Loss: 0.7842\n",
            "Epoch [5/20], Step [10/142], Loss: 0.8225\n",
            "Epoch [5/20], Step [11/142], Loss: 0.8975\n",
            "Epoch [5/20], Step [12/142], Loss: 0.8498\n",
            "Epoch [5/20], Step [13/142], Loss: 0.8298\n",
            "Epoch [5/20], Step [14/142], Loss: 0.8365\n",
            "Epoch [5/20], Step [15/142], Loss: 0.8402\n",
            "Epoch [5/20], Step [16/142], Loss: 0.9470\n",
            "Epoch [5/20], Step [17/142], Loss: 0.8333\n",
            "Epoch [5/20], Step [18/142], Loss: 0.8836\n",
            "Epoch [5/20], Step [19/142], Loss: 0.8867\n",
            "Epoch [5/20], Step [20/142], Loss: 0.7809\n",
            "Epoch [5/20], Step [21/142], Loss: 0.8730\n",
            "Epoch [5/20], Step [22/142], Loss: 0.9057\n",
            "Epoch [5/20], Step [23/142], Loss: 0.9105\n",
            "Epoch [5/20], Step [24/142], Loss: 0.8560\n",
            "Epoch [5/20], Step [25/142], Loss: 0.8969\n",
            "Epoch [5/20], Step [26/142], Loss: 0.8823\n",
            "Epoch [5/20], Step [27/142], Loss: 0.7806\n",
            "Epoch [5/20], Step [28/142], Loss: 0.8088\n",
            "Epoch [5/20], Step [29/142], Loss: 0.9005\n",
            "Epoch [5/20], Step [30/142], Loss: 0.7874\n",
            "Epoch [5/20], Step [31/142], Loss: 0.8465\n",
            "Epoch [5/20], Step [32/142], Loss: 0.8751\n",
            "Epoch [5/20], Step [33/142], Loss: 0.8279\n",
            "Epoch [5/20], Step [34/142], Loss: 0.8018\n",
            "Epoch [5/20], Step [35/142], Loss: 0.7345\n",
            "Epoch [5/20], Step [36/142], Loss: 0.8124\n",
            "Epoch [5/20], Step [37/142], Loss: 0.8257\n",
            "Epoch [5/20], Step [38/142], Loss: 0.8403\n",
            "Epoch [5/20], Step [39/142], Loss: 0.8608\n",
            "Epoch [5/20], Step [40/142], Loss: 0.7753\n",
            "Epoch [5/20], Step [41/142], Loss: 0.8196\n",
            "Epoch [5/20], Step [42/142], Loss: 0.9173\n",
            "Epoch [5/20], Step [43/142], Loss: 0.8804\n",
            "Epoch [5/20], Step [44/142], Loss: 0.8446\n",
            "Epoch [5/20], Step [45/142], Loss: 0.8868\n",
            "Epoch [5/20], Step [46/142], Loss: 0.8640\n",
            "Epoch [5/20], Step [47/142], Loss: 0.8441\n",
            "Epoch [5/20], Step [48/142], Loss: 0.9276\n",
            "Epoch [5/20], Step [49/142], Loss: 0.8650\n",
            "Epoch [5/20], Step [50/142], Loss: 0.7467\n",
            "Epoch [5/20], Step [51/142], Loss: 0.7745\n",
            "Epoch [5/20], Step [52/142], Loss: 0.8230\n",
            "Epoch [5/20], Step [53/142], Loss: 0.8267\n",
            "Epoch [5/20], Step [54/142], Loss: 0.8048\n",
            "Epoch [5/20], Step [55/142], Loss: 0.7955\n",
            "Epoch [5/20], Step [56/142], Loss: 0.8151\n",
            "Epoch [5/20], Step [57/142], Loss: 0.8678\n",
            "Epoch [5/20], Step [58/142], Loss: 0.8322\n",
            "Epoch [5/20], Step [59/142], Loss: 0.9120\n",
            "Epoch [5/20], Step [60/142], Loss: 0.8611\n",
            "Epoch [5/20], Step [61/142], Loss: 0.8990\n",
            "Epoch [5/20], Step [62/142], Loss: 0.8408\n",
            "Epoch [5/20], Step [63/142], Loss: 0.8995\n",
            "Epoch [5/20], Step [64/142], Loss: 0.7900\n",
            "Epoch [5/20], Step [65/142], Loss: 0.8504\n",
            "Epoch [5/20], Step [66/142], Loss: 0.8381\n",
            "Epoch [5/20], Step [67/142], Loss: 0.8358\n",
            "Epoch [5/20], Step [68/142], Loss: 0.8659\n",
            "Epoch [5/20], Step [69/142], Loss: 0.8548\n",
            "Epoch [5/20], Step [70/142], Loss: 0.7662\n",
            "Epoch [5/20], Step [71/142], Loss: 0.8501\n",
            "Epoch [5/20], Step [72/142], Loss: 0.8372\n",
            "Epoch [5/20], Step [73/142], Loss: 0.8359\n",
            "Epoch [5/20], Step [74/142], Loss: 0.7239\n",
            "Epoch [5/20], Step [75/142], Loss: 0.8758\n",
            "Epoch [5/20], Step [76/142], Loss: 0.8437\n",
            "Epoch [5/20], Step [77/142], Loss: 0.8564\n",
            "Epoch [5/20], Step [78/142], Loss: 0.8206\n",
            "Epoch [5/20], Step [79/142], Loss: 0.8454\n",
            "Epoch [5/20], Step [80/142], Loss: 0.9113\n",
            "Epoch [5/20], Step [81/142], Loss: 0.8824\n",
            "Epoch [5/20], Step [82/142], Loss: 0.8068\n",
            "Epoch [5/20], Step [83/142], Loss: 0.7920\n",
            "Epoch [5/20], Step [84/142], Loss: 0.8849\n",
            "Epoch [5/20], Step [85/142], Loss: 0.7974\n",
            "Epoch [5/20], Step [86/142], Loss: 0.8917\n",
            "Epoch [5/20], Step [87/142], Loss: 0.9816\n",
            "Epoch [5/20], Step [88/142], Loss: 0.8148\n",
            "Epoch [5/20], Step [89/142], Loss: 0.8580\n",
            "Epoch [5/20], Step [90/142], Loss: 0.8433\n",
            "Epoch [5/20], Step [91/142], Loss: 0.8618\n",
            "Epoch [5/20], Step [92/142], Loss: 0.7833\n",
            "Epoch [5/20], Step [93/142], Loss: 0.8963\n",
            "Epoch [5/20], Step [94/142], Loss: 0.8437\n",
            "Epoch [5/20], Step [95/142], Loss: 0.7028\n",
            "Epoch [5/20], Step [96/142], Loss: 0.8657\n",
            "Epoch [5/20], Step [97/142], Loss: 0.7984\n",
            "Epoch [5/20], Step [98/142], Loss: 0.9007\n",
            "Epoch [5/20], Step [99/142], Loss: 0.8653\n",
            "Epoch [5/20], Step [100/142], Loss: 0.8380\n",
            "Epoch [5/20], Step [101/142], Loss: 0.9085\n",
            "Epoch [5/20], Step [102/142], Loss: 0.9673\n",
            "Epoch [5/20], Step [103/142], Loss: 0.7806\n",
            "Epoch [5/20], Step [104/142], Loss: 0.9006\n",
            "Epoch [5/20], Step [105/142], Loss: 0.8827\n",
            "Epoch [5/20], Step [106/142], Loss: 0.8901\n",
            "Epoch [5/20], Step [107/142], Loss: 0.8302\n",
            "Epoch [5/20], Step [108/142], Loss: 0.9191\n",
            "Epoch [5/20], Step [109/142], Loss: 0.8341\n",
            "Epoch [5/20], Step [110/142], Loss: 0.8296\n",
            "Epoch [5/20], Step [111/142], Loss: 0.8807\n",
            "Epoch [5/20], Step [112/142], Loss: 0.8767\n",
            "Epoch [5/20], Step [113/142], Loss: 0.8306\n",
            "Epoch [5/20], Step [114/142], Loss: 0.8416\n",
            "Epoch [5/20], Step [115/142], Loss: 0.8945\n",
            "Epoch [5/20], Step [116/142], Loss: 0.8947\n",
            "Epoch [5/20], Step [117/142], Loss: 0.9039\n",
            "Epoch [5/20], Step [118/142], Loss: 0.8135\n",
            "Epoch [5/20], Step [119/142], Loss: 0.8886\n",
            "Epoch [5/20], Step [120/142], Loss: 0.8509\n",
            "Epoch [5/20], Step [121/142], Loss: 0.8375\n",
            "Epoch [5/20], Step [122/142], Loss: 0.9264\n",
            "Epoch [5/20], Step [123/142], Loss: 0.8478\n",
            "Epoch [5/20], Step [124/142], Loss: 0.8033\n",
            "Epoch [5/20], Step [125/142], Loss: 0.8648\n",
            "Epoch [5/20], Step [126/142], Loss: 0.7595\n",
            "Epoch [5/20], Step [127/142], Loss: 0.7744\n",
            "Epoch [5/20], Step [128/142], Loss: 0.8780\n",
            "Epoch [5/20], Step [129/142], Loss: 0.8701\n",
            "Epoch [5/20], Step [130/142], Loss: 0.8806\n",
            "Epoch [5/20], Step [131/142], Loss: 0.8424\n",
            "Epoch [5/20], Step [132/142], Loss: 0.8387\n",
            "Epoch [5/20], Step [133/142], Loss: 0.7477\n",
            "Epoch [5/20], Step [134/142], Loss: 0.9314\n",
            "Epoch [5/20], Step [135/142], Loss: 0.8231\n",
            "Epoch [5/20], Step [136/142], Loss: 0.8215\n",
            "Epoch [5/20], Step [137/142], Loss: 0.8139\n",
            "Epoch [5/20], Step [138/142], Loss: 0.7446\n",
            "Epoch [5/20], Step [139/142], Loss: 0.8281\n",
            "Epoch [5/20], Step [140/142], Loss: 0.8669\n",
            "Epoch [5/20], Step [141/142], Loss: 0.8234\n",
            "Epoch [5/20], Step [142/142], Loss: 1.1119\n",
            "Epoch [6/20], Step [1/142], Loss: 0.8418\n",
            "Epoch [6/20], Step [2/142], Loss: 0.7725\n",
            "Epoch [6/20], Step [3/142], Loss: 0.8282\n",
            "Epoch [6/20], Step [4/142], Loss: 0.9023\n",
            "Epoch [6/20], Step [5/142], Loss: 0.8703\n",
            "Epoch [6/20], Step [6/142], Loss: 0.8104\n",
            "Epoch [6/20], Step [7/142], Loss: 0.8043\n",
            "Epoch [6/20], Step [8/142], Loss: 0.9236\n",
            "Epoch [6/20], Step [9/142], Loss: 0.7901\n",
            "Epoch [6/20], Step [10/142], Loss: 0.7570\n",
            "Epoch [6/20], Step [11/142], Loss: 0.8645\n",
            "Epoch [6/20], Step [12/142], Loss: 0.9338\n",
            "Epoch [6/20], Step [13/142], Loss: 0.7938\n",
            "Epoch [6/20], Step [14/142], Loss: 0.8580\n",
            "Epoch [6/20], Step [15/142], Loss: 0.9077\n",
            "Epoch [6/20], Step [16/142], Loss: 0.8833\n",
            "Epoch [6/20], Step [17/142], Loss: 0.7855\n",
            "Epoch [6/20], Step [18/142], Loss: 0.8007\n",
            "Epoch [6/20], Step [19/142], Loss: 0.8244\n",
            "Epoch [6/20], Step [20/142], Loss: 0.9817\n",
            "Epoch [6/20], Step [21/142], Loss: 0.9318\n",
            "Epoch [6/20], Step [22/142], Loss: 0.9266\n",
            "Epoch [6/20], Step [23/142], Loss: 0.8359\n",
            "Epoch [6/20], Step [24/142], Loss: 0.8426\n",
            "Epoch [6/20], Step [25/142], Loss: 0.8449\n",
            "Epoch [6/20], Step [26/142], Loss: 0.8186\n",
            "Epoch [6/20], Step [27/142], Loss: 0.8370\n",
            "Epoch [6/20], Step [28/142], Loss: 0.8764\n",
            "Epoch [6/20], Step [29/142], Loss: 0.7927\n",
            "Epoch [6/20], Step [30/142], Loss: 0.8245\n",
            "Epoch [6/20], Step [31/142], Loss: 0.7240\n",
            "Epoch [6/20], Step [32/142], Loss: 0.8075\n",
            "Epoch [6/20], Step [33/142], Loss: 1.0452\n",
            "Epoch [6/20], Step [34/142], Loss: 0.8500\n",
            "Epoch [6/20], Step [35/142], Loss: 0.7602\n",
            "Epoch [6/20], Step [36/142], Loss: 0.8637\n",
            "Epoch [6/20], Step [37/142], Loss: 0.8025\n",
            "Epoch [6/20], Step [38/142], Loss: 0.8880\n",
            "Epoch [6/20], Step [39/142], Loss: 0.8874\n",
            "Epoch [6/20], Step [40/142], Loss: 0.8599\n",
            "Epoch [6/20], Step [41/142], Loss: 0.8125\n",
            "Epoch [6/20], Step [42/142], Loss: 0.7929\n",
            "Epoch [6/20], Step [43/142], Loss: 0.8835\n",
            "Epoch [6/20], Step [44/142], Loss: 0.8118\n",
            "Epoch [6/20], Step [45/142], Loss: 0.8536\n",
            "Epoch [6/20], Step [46/142], Loss: 0.7937\n",
            "Epoch [6/20], Step [47/142], Loss: 0.8798\n",
            "Epoch [6/20], Step [48/142], Loss: 0.7833\n",
            "Epoch [6/20], Step [49/142], Loss: 0.8599\n",
            "Epoch [6/20], Step [50/142], Loss: 0.8739\n",
            "Epoch [6/20], Step [51/142], Loss: 0.8706\n",
            "Epoch [6/20], Step [52/142], Loss: 0.7665\n",
            "Epoch [6/20], Step [53/142], Loss: 0.7452\n",
            "Epoch [6/20], Step [54/142], Loss: 0.8644\n",
            "Epoch [6/20], Step [55/142], Loss: 0.9936\n",
            "Epoch [6/20], Step [56/142], Loss: 0.8791\n",
            "Epoch [6/20], Step [57/142], Loss: 0.8002\n",
            "Epoch [6/20], Step [58/142], Loss: 0.8594\n",
            "Epoch [6/20], Step [59/142], Loss: 0.8673\n",
            "Epoch [6/20], Step [60/142], Loss: 0.8860\n",
            "Epoch [6/20], Step [61/142], Loss: 0.8450\n",
            "Epoch [6/20], Step [62/142], Loss: 0.8473\n",
            "Epoch [6/20], Step [63/142], Loss: 0.8893\n",
            "Epoch [6/20], Step [64/142], Loss: 0.8272\n",
            "Epoch [6/20], Step [65/142], Loss: 0.8695\n",
            "Epoch [6/20], Step [66/142], Loss: 0.8271\n",
            "Epoch [6/20], Step [67/142], Loss: 0.9089\n",
            "Epoch [6/20], Step [68/142], Loss: 0.8627\n",
            "Epoch [6/20], Step [69/142], Loss: 0.8833\n",
            "Epoch [6/20], Step [70/142], Loss: 0.8088\n",
            "Epoch [6/20], Step [71/142], Loss: 0.7571\n",
            "Epoch [6/20], Step [72/142], Loss: 0.8086\n",
            "Epoch [6/20], Step [73/142], Loss: 0.9826\n",
            "Epoch [6/20], Step [74/142], Loss: 0.8928\n",
            "Epoch [6/20], Step [75/142], Loss: 0.8722\n",
            "Epoch [6/20], Step [76/142], Loss: 0.8344\n",
            "Epoch [6/20], Step [77/142], Loss: 0.8602\n",
            "Epoch [6/20], Step [78/142], Loss: 0.9197\n",
            "Epoch [6/20], Step [79/142], Loss: 0.7227\n",
            "Epoch [6/20], Step [80/142], Loss: 0.7348\n",
            "Epoch [6/20], Step [81/142], Loss: 0.8786\n",
            "Epoch [6/20], Step [82/142], Loss: 0.9095\n",
            "Epoch [6/20], Step [83/142], Loss: 0.8270\n",
            "Epoch [6/20], Step [84/142], Loss: 0.8382\n",
            "Epoch [6/20], Step [85/142], Loss: 0.8848\n",
            "Epoch [6/20], Step [86/142], Loss: 0.9066\n",
            "Epoch [6/20], Step [87/142], Loss: 0.9064\n",
            "Epoch [6/20], Step [88/142], Loss: 0.8538\n",
            "Epoch [6/20], Step [89/142], Loss: 0.9064\n",
            "Epoch [6/20], Step [90/142], Loss: 0.8766\n",
            "Epoch [6/20], Step [91/142], Loss: 0.7391\n",
            "Epoch [6/20], Step [92/142], Loss: 0.8519\n",
            "Epoch [6/20], Step [93/142], Loss: 0.7751\n",
            "Epoch [6/20], Step [94/142], Loss: 0.8393\n",
            "Epoch [6/20], Step [95/142], Loss: 0.7861\n",
            "Epoch [6/20], Step [96/142], Loss: 0.8009\n",
            "Epoch [6/20], Step [97/142], Loss: 0.8399\n",
            "Epoch [6/20], Step [98/142], Loss: 0.8351\n",
            "Epoch [6/20], Step [99/142], Loss: 0.8300\n",
            "Epoch [6/20], Step [100/142], Loss: 0.7491\n",
            "Epoch [6/20], Step [101/142], Loss: 0.8684\n",
            "Epoch [6/20], Step [102/142], Loss: 0.8429\n",
            "Epoch [6/20], Step [103/142], Loss: 0.8945\n",
            "Epoch [6/20], Step [104/142], Loss: 0.9870\n",
            "Epoch [6/20], Step [105/142], Loss: 0.7927\n",
            "Epoch [6/20], Step [106/142], Loss: 0.9110\n",
            "Epoch [6/20], Step [107/142], Loss: 0.9090\n",
            "Epoch [6/20], Step [108/142], Loss: 0.8333\n",
            "Epoch [6/20], Step [109/142], Loss: 0.8952\n",
            "Epoch [6/20], Step [110/142], Loss: 0.8999\n",
            "Epoch [6/20], Step [111/142], Loss: 0.7861\n",
            "Epoch [6/20], Step [112/142], Loss: 0.8559\n",
            "Epoch [6/20], Step [113/142], Loss: 0.7300\n",
            "Epoch [6/20], Step [114/142], Loss: 0.8300\n",
            "Epoch [6/20], Step [115/142], Loss: 0.8233\n",
            "Epoch [6/20], Step [116/142], Loss: 0.7816\n",
            "Epoch [6/20], Step [117/142], Loss: 0.8212\n",
            "Epoch [6/20], Step [118/142], Loss: 0.7914\n",
            "Epoch [6/20], Step [119/142], Loss: 0.9230\n",
            "Epoch [6/20], Step [120/142], Loss: 0.8317\n",
            "Epoch [6/20], Step [121/142], Loss: 0.7831\n",
            "Epoch [6/20], Step [122/142], Loss: 0.8203\n",
            "Epoch [6/20], Step [123/142], Loss: 0.8953\n",
            "Epoch [6/20], Step [124/142], Loss: 0.9517\n",
            "Epoch [6/20], Step [125/142], Loss: 0.8585\n",
            "Epoch [6/20], Step [126/142], Loss: 0.8377\n",
            "Epoch [6/20], Step [127/142], Loss: 0.8296\n",
            "Epoch [6/20], Step [128/142], Loss: 0.8476\n",
            "Epoch [6/20], Step [129/142], Loss: 0.7994\n",
            "Epoch [6/20], Step [130/142], Loss: 0.8770\n",
            "Epoch [6/20], Step [131/142], Loss: 0.7825\n",
            "Epoch [6/20], Step [132/142], Loss: 0.7706\n",
            "Epoch [6/20], Step [133/142], Loss: 0.9115\n",
            "Epoch [6/20], Step [134/142], Loss: 0.8391\n",
            "Epoch [6/20], Step [135/142], Loss: 0.8261\n",
            "Epoch [6/20], Step [136/142], Loss: 0.9925\n",
            "Epoch [6/20], Step [137/142], Loss: 0.8160\n",
            "Epoch [6/20], Step [138/142], Loss: 0.8747\n",
            "Epoch [6/20], Step [139/142], Loss: 0.8102\n",
            "Epoch [6/20], Step [140/142], Loss: 0.8007\n",
            "Epoch [6/20], Step [141/142], Loss: 0.8697\n",
            "Epoch [6/20], Step [142/142], Loss: 0.9007\n",
            "Epoch [7/20], Step [1/142], Loss: 0.7309\n",
            "Epoch [7/20], Step [2/142], Loss: 0.8634\n",
            "Epoch [7/20], Step [3/142], Loss: 0.7369\n",
            "Epoch [7/20], Step [4/142], Loss: 0.8758\n",
            "Epoch [7/20], Step [5/142], Loss: 0.8631\n",
            "Epoch [7/20], Step [6/142], Loss: 0.9066\n",
            "Epoch [7/20], Step [7/142], Loss: 0.7249\n",
            "Epoch [7/20], Step [8/142], Loss: 0.8014\n",
            "Epoch [7/20], Step [9/142], Loss: 0.8921\n",
            "Epoch [7/20], Step [10/142], Loss: 0.7837\n",
            "Epoch [7/20], Step [11/142], Loss: 0.8063\n",
            "Epoch [7/20], Step [12/142], Loss: 0.8843\n",
            "Epoch [7/20], Step [13/142], Loss: 0.8787\n",
            "Epoch [7/20], Step [14/142], Loss: 0.8404\n",
            "Epoch [7/20], Step [15/142], Loss: 0.8003\n",
            "Epoch [7/20], Step [16/142], Loss: 0.8337\n",
            "Epoch [7/20], Step [17/142], Loss: 0.8466\n",
            "Epoch [7/20], Step [18/142], Loss: 0.8268\n",
            "Epoch [7/20], Step [19/142], Loss: 0.8143\n",
            "Epoch [7/20], Step [20/142], Loss: 0.8489\n",
            "Epoch [7/20], Step [21/142], Loss: 0.8754\n",
            "Epoch [7/20], Step [22/142], Loss: 0.8959\n",
            "Epoch [7/20], Step [23/142], Loss: 0.8292\n",
            "Epoch [7/20], Step [24/142], Loss: 0.6752\n",
            "Epoch [7/20], Step [25/142], Loss: 0.8056\n",
            "Epoch [7/20], Step [26/142], Loss: 0.8443\n",
            "Epoch [7/20], Step [27/142], Loss: 0.8863\n",
            "Epoch [7/20], Step [28/142], Loss: 0.8004\n",
            "Epoch [7/20], Step [29/142], Loss: 0.8120\n",
            "Epoch [7/20], Step [30/142], Loss: 0.9217\n",
            "Epoch [7/20], Step [31/142], Loss: 0.8691\n",
            "Epoch [7/20], Step [32/142], Loss: 0.8973\n",
            "Epoch [7/20], Step [33/142], Loss: 0.8513\n",
            "Epoch [7/20], Step [34/142], Loss: 0.7985\n",
            "Epoch [7/20], Step [35/142], Loss: 0.7916\n",
            "Epoch [7/20], Step [36/142], Loss: 0.9087\n",
            "Epoch [7/20], Step [37/142], Loss: 0.9159\n",
            "Epoch [7/20], Step [38/142], Loss: 0.9393\n",
            "Epoch [7/20], Step [39/142], Loss: 0.8393\n",
            "Epoch [7/20], Step [40/142], Loss: 0.7764\n",
            "Epoch [7/20], Step [41/142], Loss: 0.8947\n",
            "Epoch [7/20], Step [42/142], Loss: 0.8542\n",
            "Epoch [7/20], Step [43/142], Loss: 0.7252\n",
            "Epoch [7/20], Step [44/142], Loss: 0.9046\n",
            "Epoch [7/20], Step [45/142], Loss: 0.8951\n",
            "Epoch [7/20], Step [46/142], Loss: 0.8284\n",
            "Epoch [7/20], Step [47/142], Loss: 0.7448\n",
            "Epoch [7/20], Step [48/142], Loss: 0.7774\n",
            "Epoch [7/20], Step [49/142], Loss: 0.8824\n",
            "Epoch [7/20], Step [50/142], Loss: 0.8215\n",
            "Epoch [7/20], Step [51/142], Loss: 0.7735\n",
            "Epoch [7/20], Step [52/142], Loss: 0.8995\n",
            "Epoch [7/20], Step [53/142], Loss: 0.7026\n",
            "Epoch [7/20], Step [54/142], Loss: 0.7786\n",
            "Epoch [7/20], Step [55/142], Loss: 0.7964\n",
            "Epoch [7/20], Step [56/142], Loss: 0.8256\n",
            "Epoch [7/20], Step [57/142], Loss: 0.8765\n",
            "Epoch [7/20], Step [58/142], Loss: 0.9154\n",
            "Epoch [7/20], Step [59/142], Loss: 0.8582\n",
            "Epoch [7/20], Step [60/142], Loss: 0.8460\n",
            "Epoch [7/20], Step [61/142], Loss: 0.8500\n",
            "Epoch [7/20], Step [62/142], Loss: 0.8469\n",
            "Epoch [7/20], Step [63/142], Loss: 0.9019\n",
            "Epoch [7/20], Step [64/142], Loss: 0.8833\n",
            "Epoch [7/20], Step [65/142], Loss: 0.7466\n",
            "Epoch [7/20], Step [66/142], Loss: 0.8783\n",
            "Epoch [7/20], Step [67/142], Loss: 0.8644\n",
            "Epoch [7/20], Step [68/142], Loss: 0.7654\n",
            "Epoch [7/20], Step [69/142], Loss: 0.7916\n",
            "Epoch [7/20], Step [70/142], Loss: 0.8047\n",
            "Epoch [7/20], Step [71/142], Loss: 0.8904\n",
            "Epoch [7/20], Step [72/142], Loss: 0.8116\n",
            "Epoch [7/20], Step [73/142], Loss: 0.8901\n",
            "Epoch [7/20], Step [74/142], Loss: 0.8511\n",
            "Epoch [7/20], Step [75/142], Loss: 0.8790\n",
            "Epoch [7/20], Step [76/142], Loss: 0.8568\n",
            "Epoch [7/20], Step [77/142], Loss: 0.9644\n",
            "Epoch [7/20], Step [78/142], Loss: 0.9727\n",
            "Epoch [7/20], Step [79/142], Loss: 0.8176\n",
            "Epoch [7/20], Step [80/142], Loss: 0.9137\n",
            "Epoch [7/20], Step [81/142], Loss: 0.8738\n",
            "Epoch [7/20], Step [82/142], Loss: 0.9029\n",
            "Epoch [7/20], Step [83/142], Loss: 0.8655\n",
            "Epoch [7/20], Step [84/142], Loss: 0.8508\n",
            "Epoch [7/20], Step [85/142], Loss: 0.8876\n",
            "Epoch [7/20], Step [86/142], Loss: 0.7743\n",
            "Epoch [7/20], Step [87/142], Loss: 0.8667\n",
            "Epoch [7/20], Step [88/142], Loss: 0.7688\n",
            "Epoch [7/20], Step [89/142], Loss: 0.9410\n",
            "Epoch [7/20], Step [90/142], Loss: 0.8145\n",
            "Epoch [7/20], Step [91/142], Loss: 0.8460\n",
            "Epoch [7/20], Step [92/142], Loss: 0.7964\n",
            "Epoch [7/20], Step [93/142], Loss: 0.8807\n",
            "Epoch [7/20], Step [94/142], Loss: 0.7769\n",
            "Epoch [7/20], Step [95/142], Loss: 0.9074\n",
            "Epoch [7/20], Step [96/142], Loss: 0.8596\n",
            "Epoch [7/20], Step [97/142], Loss: 0.9029\n",
            "Epoch [7/20], Step [98/142], Loss: 0.9292\n",
            "Epoch [7/20], Step [99/142], Loss: 0.8125\n",
            "Epoch [7/20], Step [100/142], Loss: 0.8768\n",
            "Epoch [7/20], Step [101/142], Loss: 0.8393\n",
            "Epoch [7/20], Step [102/142], Loss: 0.9003\n",
            "Epoch [7/20], Step [103/142], Loss: 0.8338\n",
            "Epoch [7/20], Step [104/142], Loss: 0.8568\n",
            "Epoch [7/20], Step [105/142], Loss: 0.8273\n",
            "Epoch [7/20], Step [106/142], Loss: 0.7623\n",
            "Epoch [7/20], Step [107/142], Loss: 0.9162\n",
            "Epoch [7/20], Step [108/142], Loss: 0.9059\n",
            "Epoch [7/20], Step [109/142], Loss: 0.9738\n",
            "Epoch [7/20], Step [110/142], Loss: 0.7754\n",
            "Epoch [7/20], Step [111/142], Loss: 0.8810\n",
            "Epoch [7/20], Step [112/142], Loss: 0.8709\n",
            "Epoch [7/20], Step [113/142], Loss: 0.8633\n",
            "Epoch [7/20], Step [114/142], Loss: 0.8759\n",
            "Epoch [7/20], Step [115/142], Loss: 0.9863\n",
            "Epoch [7/20], Step [116/142], Loss: 0.8107\n",
            "Epoch [7/20], Step [117/142], Loss: 0.7792\n",
            "Epoch [7/20], Step [118/142], Loss: 0.8229\n",
            "Epoch [7/20], Step [119/142], Loss: 0.7784\n",
            "Epoch [7/20], Step [120/142], Loss: 0.8491\n",
            "Epoch [7/20], Step [121/142], Loss: 0.8078\n",
            "Epoch [7/20], Step [122/142], Loss: 0.8894\n",
            "Epoch [7/20], Step [123/142], Loss: 0.8165\n",
            "Epoch [7/20], Step [124/142], Loss: 0.7433\n",
            "Epoch [7/20], Step [125/142], Loss: 0.8560\n",
            "Epoch [7/20], Step [126/142], Loss: 0.8555\n",
            "Epoch [7/20], Step [127/142], Loss: 0.8294\n",
            "Epoch [7/20], Step [128/142], Loss: 0.7697\n",
            "Epoch [7/20], Step [129/142], Loss: 0.8466\n",
            "Epoch [7/20], Step [130/142], Loss: 0.7971\n",
            "Epoch [7/20], Step [131/142], Loss: 0.9034\n",
            "Epoch [7/20], Step [132/142], Loss: 0.8965\n",
            "Epoch [7/20], Step [133/142], Loss: 0.9393\n",
            "Epoch [7/20], Step [134/142], Loss: 0.8198\n",
            "Epoch [7/20], Step [135/142], Loss: 0.8058\n",
            "Epoch [7/20], Step [136/142], Loss: 0.9006\n",
            "Epoch [7/20], Step [137/142], Loss: 0.8679\n",
            "Epoch [7/20], Step [138/142], Loss: 0.7950\n",
            "Epoch [7/20], Step [139/142], Loss: 0.7600\n",
            "Epoch [7/20], Step [140/142], Loss: 0.8175\n",
            "Epoch [7/20], Step [141/142], Loss: 0.9448\n",
            "Epoch [7/20], Step [142/142], Loss: 0.9427\n",
            "Epoch [8/20], Step [1/142], Loss: 0.8546\n",
            "Epoch [8/20], Step [2/142], Loss: 0.8485\n",
            "Epoch [8/20], Step [3/142], Loss: 0.8083\n",
            "Epoch [8/20], Step [4/142], Loss: 0.8757\n",
            "Epoch [8/20], Step [5/142], Loss: 0.9150\n",
            "Epoch [8/20], Step [6/142], Loss: 0.8227\n",
            "Epoch [8/20], Step [7/142], Loss: 0.8288\n",
            "Epoch [8/20], Step [8/142], Loss: 0.8120\n",
            "Epoch [8/20], Step [9/142], Loss: 0.8209\n",
            "Epoch [8/20], Step [10/142], Loss: 0.7974\n",
            "Epoch [8/20], Step [11/142], Loss: 0.8610\n",
            "Epoch [8/20], Step [12/142], Loss: 0.8455\n",
            "Epoch [8/20], Step [13/142], Loss: 0.8812\n",
            "Epoch [8/20], Step [14/142], Loss: 0.7467\n",
            "Epoch [8/20], Step [15/142], Loss: 0.8293\n",
            "Epoch [8/20], Step [16/142], Loss: 0.8925\n",
            "Epoch [8/20], Step [17/142], Loss: 0.8436\n",
            "Epoch [8/20], Step [18/142], Loss: 0.8730\n",
            "Epoch [8/20], Step [19/142], Loss: 0.8035\n",
            "Epoch [8/20], Step [20/142], Loss: 0.8875\n",
            "Epoch [8/20], Step [21/142], Loss: 0.8216\n",
            "Epoch [8/20], Step [22/142], Loss: 0.8512\n",
            "Epoch [8/20], Step [23/142], Loss: 0.8337\n",
            "Epoch [8/20], Step [24/142], Loss: 0.8448\n",
            "Epoch [8/20], Step [25/142], Loss: 0.8531\n",
            "Epoch [8/20], Step [26/142], Loss: 0.8857\n",
            "Epoch [8/20], Step [27/142], Loss: 0.7669\n",
            "Epoch [8/20], Step [28/142], Loss: 0.9808\n",
            "Epoch [8/20], Step [29/142], Loss: 0.8910\n",
            "Epoch [8/20], Step [30/142], Loss: 0.9009\n",
            "Epoch [8/20], Step [31/142], Loss: 0.7823\n",
            "Epoch [8/20], Step [32/142], Loss: 0.8738\n",
            "Epoch [8/20], Step [33/142], Loss: 0.8582\n",
            "Epoch [8/20], Step [34/142], Loss: 0.9167\n",
            "Epoch [8/20], Step [35/142], Loss: 0.9494\n",
            "Epoch [8/20], Step [36/142], Loss: 0.8438\n",
            "Epoch [8/20], Step [37/142], Loss: 0.7971\n",
            "Epoch [8/20], Step [38/142], Loss: 0.8053\n",
            "Epoch [8/20], Step [39/142], Loss: 0.8087\n",
            "Epoch [8/20], Step [40/142], Loss: 0.7690\n",
            "Epoch [8/20], Step [41/142], Loss: 0.9452\n",
            "Epoch [8/20], Step [42/142], Loss: 0.7872\n",
            "Epoch [8/20], Step [43/142], Loss: 0.7804\n",
            "Epoch [8/20], Step [44/142], Loss: 0.8960\n",
            "Epoch [8/20], Step [45/142], Loss: 0.8330\n",
            "Epoch [8/20], Step [46/142], Loss: 0.7934\n",
            "Epoch [8/20], Step [47/142], Loss: 0.7979\n",
            "Epoch [8/20], Step [48/142], Loss: 0.7740\n",
            "Epoch [8/20], Step [49/142], Loss: 0.7859\n",
            "Epoch [8/20], Step [50/142], Loss: 0.9009\n",
            "Epoch [8/20], Step [51/142], Loss: 0.7988\n",
            "Epoch [8/20], Step [52/142], Loss: 0.9684\n",
            "Epoch [8/20], Step [53/142], Loss: 0.8673\n",
            "Epoch [8/20], Step [54/142], Loss: 0.8289\n",
            "Epoch [8/20], Step [55/142], Loss: 0.8004\n",
            "Epoch [8/20], Step [56/142], Loss: 0.7921\n",
            "Epoch [8/20], Step [57/142], Loss: 0.8585\n",
            "Epoch [8/20], Step [58/142], Loss: 0.8197\n",
            "Epoch [8/20], Step [59/142], Loss: 0.8473\n",
            "Epoch [8/20], Step [60/142], Loss: 0.8045\n",
            "Epoch [8/20], Step [61/142], Loss: 0.7606\n",
            "Epoch [8/20], Step [62/142], Loss: 0.9094\n",
            "Epoch [8/20], Step [63/142], Loss: 0.8076\n",
            "Epoch [8/20], Step [64/142], Loss: 0.8916\n",
            "Epoch [8/20], Step [65/142], Loss: 0.8058\n",
            "Epoch [8/20], Step [66/142], Loss: 0.7529\n",
            "Epoch [8/20], Step [67/142], Loss: 0.8609\n",
            "Epoch [8/20], Step [68/142], Loss: 0.7200\n",
            "Epoch [8/20], Step [69/142], Loss: 0.9089\n",
            "Epoch [8/20], Step [70/142], Loss: 0.8301\n",
            "Epoch [8/20], Step [71/142], Loss: 0.9612\n",
            "Epoch [8/20], Step [72/142], Loss: 0.8723\n",
            "Epoch [8/20], Step [73/142], Loss: 0.8685\n",
            "Epoch [8/20], Step [74/142], Loss: 0.8712\n",
            "Epoch [8/20], Step [75/142], Loss: 0.8452\n",
            "Epoch [8/20], Step [76/142], Loss: 0.8228\n",
            "Epoch [8/20], Step [77/142], Loss: 0.8343\n",
            "Epoch [8/20], Step [78/142], Loss: 0.9487\n",
            "Epoch [8/20], Step [79/142], Loss: 0.9075\n",
            "Epoch [8/20], Step [80/142], Loss: 0.7547\n",
            "Epoch [8/20], Step [81/142], Loss: 0.8896\n",
            "Epoch [8/20], Step [82/142], Loss: 0.8755\n",
            "Epoch [8/20], Step [83/142], Loss: 0.8088\n",
            "Epoch [8/20], Step [84/142], Loss: 0.7857\n",
            "Epoch [8/20], Step [85/142], Loss: 0.7652\n",
            "Epoch [8/20], Step [86/142], Loss: 0.8707\n",
            "Epoch [8/20], Step [87/142], Loss: 0.7195\n",
            "Epoch [8/20], Step [88/142], Loss: 0.8942\n",
            "Epoch [8/20], Step [89/142], Loss: 0.8404\n",
            "Epoch [8/20], Step [90/142], Loss: 0.9217\n",
            "Epoch [8/20], Step [91/142], Loss: 0.8572\n",
            "Epoch [8/20], Step [92/142], Loss: 0.8146\n",
            "Epoch [8/20], Step [93/142], Loss: 0.8136\n",
            "Epoch [8/20], Step [94/142], Loss: 0.9724\n",
            "Epoch [8/20], Step [95/142], Loss: 0.7926\n",
            "Epoch [8/20], Step [96/142], Loss: 0.8342\n",
            "Epoch [8/20], Step [97/142], Loss: 0.7710\n",
            "Epoch [8/20], Step [98/142], Loss: 0.8988\n",
            "Epoch [8/20], Step [99/142], Loss: 0.8063\n",
            "Epoch [8/20], Step [100/142], Loss: 0.9295\n",
            "Epoch [8/20], Step [101/142], Loss: 0.8269\n",
            "Epoch [8/20], Step [102/142], Loss: 0.7295\n",
            "Epoch [8/20], Step [103/142], Loss: 0.9009\n",
            "Epoch [8/20], Step [104/142], Loss: 0.8500\n",
            "Epoch [8/20], Step [105/142], Loss: 0.9150\n",
            "Epoch [8/20], Step [106/142], Loss: 0.8598\n",
            "Epoch [8/20], Step [107/142], Loss: 0.8178\n",
            "Epoch [8/20], Step [108/142], Loss: 0.8494\n",
            "Epoch [8/20], Step [109/142], Loss: 0.9082\n",
            "Epoch [8/20], Step [110/142], Loss: 0.8368\n",
            "Epoch [8/20], Step [111/142], Loss: 0.8265\n",
            "Epoch [8/20], Step [112/142], Loss: 0.9114\n",
            "Epoch [8/20], Step [113/142], Loss: 0.8077\n",
            "Epoch [8/20], Step [114/142], Loss: 0.8605\n",
            "Epoch [8/20], Step [115/142], Loss: 0.8048\n",
            "Epoch [8/20], Step [116/142], Loss: 0.8690\n",
            "Epoch [8/20], Step [117/142], Loss: 0.8284\n",
            "Epoch [8/20], Step [118/142], Loss: 0.7896\n",
            "Epoch [8/20], Step [119/142], Loss: 0.8959\n",
            "Epoch [8/20], Step [120/142], Loss: 0.8023\n",
            "Epoch [8/20], Step [121/142], Loss: 0.7893\n",
            "Epoch [8/20], Step [122/142], Loss: 0.8356\n",
            "Epoch [8/20], Step [123/142], Loss: 0.8178\n",
            "Epoch [8/20], Step [124/142], Loss: 0.8675\n",
            "Epoch [8/20], Step [125/142], Loss: 0.9262\n",
            "Epoch [8/20], Step [126/142], Loss: 0.8016\n",
            "Epoch [8/20], Step [127/142], Loss: 0.8806\n",
            "Epoch [8/20], Step [128/142], Loss: 0.9171\n",
            "Epoch [8/20], Step [129/142], Loss: 0.9609\n",
            "Epoch [8/20], Step [130/142], Loss: 0.8659\n",
            "Epoch [8/20], Step [131/142], Loss: 0.8801\n",
            "Epoch [8/20], Step [132/142], Loss: 0.8537\n",
            "Epoch [8/20], Step [133/142], Loss: 0.9115\n",
            "Epoch [8/20], Step [134/142], Loss: 0.8908\n",
            "Epoch [8/20], Step [135/142], Loss: 0.7910\n",
            "Epoch [8/20], Step [136/142], Loss: 0.8593\n",
            "Epoch [8/20], Step [137/142], Loss: 0.8585\n",
            "Epoch [8/20], Step [138/142], Loss: 0.7643\n",
            "Epoch [8/20], Step [139/142], Loss: 0.8394\n",
            "Epoch [8/20], Step [140/142], Loss: 0.8374\n",
            "Epoch [8/20], Step [141/142], Loss: 0.8373\n",
            "Epoch [8/20], Step [142/142], Loss: 0.8471\n",
            "Epoch [9/20], Step [1/142], Loss: 0.8696\n",
            "Epoch [9/20], Step [2/142], Loss: 0.7862\n",
            "Epoch [9/20], Step [3/142], Loss: 0.8175\n",
            "Epoch [9/20], Step [4/142], Loss: 0.8194\n",
            "Epoch [9/20], Step [5/142], Loss: 0.8712\n",
            "Epoch [9/20], Step [6/142], Loss: 0.8679\n",
            "Epoch [9/20], Step [7/142], Loss: 0.9114\n",
            "Epoch [9/20], Step [8/142], Loss: 0.8086\n",
            "Epoch [9/20], Step [9/142], Loss: 0.7939\n",
            "Epoch [9/20], Step [10/142], Loss: 0.8211\n",
            "Epoch [9/20], Step [11/142], Loss: 0.8879\n",
            "Epoch [9/20], Step [12/142], Loss: 0.8381\n",
            "Epoch [9/20], Step [13/142], Loss: 0.8499\n",
            "Epoch [9/20], Step [14/142], Loss: 0.8104\n",
            "Epoch [9/20], Step [15/142], Loss: 0.8531\n",
            "Epoch [9/20], Step [16/142], Loss: 0.8046\n",
            "Epoch [9/20], Step [17/142], Loss: 0.8347\n",
            "Epoch [9/20], Step [18/142], Loss: 0.8439\n",
            "Epoch [9/20], Step [19/142], Loss: 0.9218\n",
            "Epoch [9/20], Step [20/142], Loss: 0.7836\n",
            "Epoch [9/20], Step [21/142], Loss: 0.7355\n",
            "Epoch [9/20], Step [22/142], Loss: 0.8367\n",
            "Epoch [9/20], Step [23/142], Loss: 0.9275\n",
            "Epoch [9/20], Step [24/142], Loss: 0.8710\n",
            "Epoch [9/20], Step [25/142], Loss: 0.8805\n",
            "Epoch [9/20], Step [26/142], Loss: 0.8069\n",
            "Epoch [9/20], Step [27/142], Loss: 0.7727\n",
            "Epoch [9/20], Step [28/142], Loss: 0.7210\n",
            "Epoch [9/20], Step [29/142], Loss: 0.7708\n",
            "Epoch [9/20], Step [30/142], Loss: 0.8252\n",
            "Epoch [9/20], Step [31/142], Loss: 0.8609\n",
            "Epoch [9/20], Step [32/142], Loss: 0.9544\n",
            "Epoch [9/20], Step [33/142], Loss: 0.7701\n",
            "Epoch [9/20], Step [34/142], Loss: 0.8269\n",
            "Epoch [9/20], Step [35/142], Loss: 0.7145\n",
            "Epoch [9/20], Step [36/142], Loss: 0.9624\n",
            "Epoch [9/20], Step [37/142], Loss: 0.8215\n",
            "Epoch [9/20], Step [38/142], Loss: 0.8634\n",
            "Epoch [9/20], Step [39/142], Loss: 0.8661\n",
            "Epoch [9/20], Step [40/142], Loss: 0.7961\n",
            "Epoch [9/20], Step [41/142], Loss: 0.8319\n",
            "Epoch [9/20], Step [42/142], Loss: 0.9375\n",
            "Epoch [9/20], Step [43/142], Loss: 0.8432\n",
            "Epoch [9/20], Step [44/142], Loss: 0.8044\n",
            "Epoch [9/20], Step [45/142], Loss: 0.8528\n",
            "Epoch [9/20], Step [46/142], Loss: 0.8230\n",
            "Epoch [9/20], Step [47/142], Loss: 0.9212\n",
            "Epoch [9/20], Step [48/142], Loss: 0.8173\n",
            "Epoch [9/20], Step [49/142], Loss: 0.8612\n",
            "Epoch [9/20], Step [50/142], Loss: 0.8313\n",
            "Epoch [9/20], Step [51/142], Loss: 0.8570\n",
            "Epoch [9/20], Step [52/142], Loss: 0.8097\n",
            "Epoch [9/20], Step [53/142], Loss: 0.8523\n",
            "Epoch [9/20], Step [54/142], Loss: 0.8608\n",
            "Epoch [9/20], Step [55/142], Loss: 0.8685\n",
            "Epoch [9/20], Step [56/142], Loss: 0.7728\n",
            "Epoch [9/20], Step [57/142], Loss: 0.9027\n",
            "Epoch [9/20], Step [58/142], Loss: 0.7697\n",
            "Epoch [9/20], Step [59/142], Loss: 0.8585\n",
            "Epoch [9/20], Step [60/142], Loss: 0.8460\n",
            "Epoch [9/20], Step [61/142], Loss: 0.8645\n",
            "Epoch [9/20], Step [62/142], Loss: 0.8232\n",
            "Epoch [9/20], Step [63/142], Loss: 0.7124\n",
            "Epoch [9/20], Step [64/142], Loss: 0.8360\n",
            "Epoch [9/20], Step [65/142], Loss: 0.9467\n",
            "Epoch [9/20], Step [66/142], Loss: 0.8163\n",
            "Epoch [9/20], Step [67/142], Loss: 0.8491\n",
            "Epoch [9/20], Step [68/142], Loss: 0.8765\n",
            "Epoch [9/20], Step [69/142], Loss: 0.8095\n",
            "Epoch [9/20], Step [70/142], Loss: 0.7821\n",
            "Epoch [9/20], Step [71/142], Loss: 0.8182\n",
            "Epoch [9/20], Step [72/142], Loss: 0.7712\n",
            "Epoch [9/20], Step [73/142], Loss: 0.8095\n",
            "Epoch [9/20], Step [74/142], Loss: 0.8755\n",
            "Epoch [9/20], Step [75/142], Loss: 0.8994\n",
            "Epoch [9/20], Step [76/142], Loss: 0.7751\n",
            "Epoch [9/20], Step [77/142], Loss: 0.9653\n",
            "Epoch [9/20], Step [78/142], Loss: 0.8211\n",
            "Epoch [9/20], Step [79/142], Loss: 0.8588\n",
            "Epoch [9/20], Step [80/142], Loss: 0.8753\n",
            "Epoch [9/20], Step [81/142], Loss: 0.8299\n",
            "Epoch [9/20], Step [82/142], Loss: 0.7997\n",
            "Epoch [9/20], Step [83/142], Loss: 0.8838\n",
            "Epoch [9/20], Step [84/142], Loss: 0.7861\n",
            "Epoch [9/20], Step [85/142], Loss: 0.8883\n",
            "Epoch [9/20], Step [86/142], Loss: 0.8588\n",
            "Epoch [9/20], Step [87/142], Loss: 0.8892\n",
            "Epoch [9/20], Step [88/142], Loss: 0.8832\n",
            "Epoch [9/20], Step [89/142], Loss: 0.8737\n",
            "Epoch [9/20], Step [90/142], Loss: 0.8251\n",
            "Epoch [9/20], Step [91/142], Loss: 0.8478\n",
            "Epoch [9/20], Step [92/142], Loss: 0.8550\n",
            "Epoch [9/20], Step [93/142], Loss: 0.8243\n",
            "Epoch [9/20], Step [94/142], Loss: 0.8590\n",
            "Epoch [9/20], Step [95/142], Loss: 0.8221\n",
            "Epoch [9/20], Step [96/142], Loss: 0.8778\n",
            "Epoch [9/20], Step [97/142], Loss: 0.8116\n",
            "Epoch [9/20], Step [98/142], Loss: 0.8469\n",
            "Epoch [9/20], Step [99/142], Loss: 0.8226\n",
            "Epoch [9/20], Step [100/142], Loss: 0.8584\n",
            "Epoch [9/20], Step [101/142], Loss: 0.7979\n",
            "Epoch [9/20], Step [102/142], Loss: 0.9265\n",
            "Epoch [9/20], Step [103/142], Loss: 0.8738\n",
            "Epoch [9/20], Step [104/142], Loss: 0.8739\n",
            "Epoch [9/20], Step [105/142], Loss: 0.8238\n",
            "Epoch [9/20], Step [106/142], Loss: 0.8103\n",
            "Epoch [9/20], Step [107/142], Loss: 0.8040\n",
            "Epoch [9/20], Step [108/142], Loss: 0.8757\n",
            "Epoch [9/20], Step [109/142], Loss: 0.9274\n",
            "Epoch [9/20], Step [110/142], Loss: 0.7965\n",
            "Epoch [9/20], Step [111/142], Loss: 0.8240\n",
            "Epoch [9/20], Step [112/142], Loss: 0.8388\n",
            "Epoch [9/20], Step [113/142], Loss: 0.8588\n",
            "Epoch [9/20], Step [114/142], Loss: 0.9165\n",
            "Epoch [9/20], Step [115/142], Loss: 0.8794\n",
            "Epoch [9/20], Step [116/142], Loss: 0.8127\n",
            "Epoch [9/20], Step [117/142], Loss: 0.8725\n",
            "Epoch [9/20], Step [118/142], Loss: 0.8356\n",
            "Epoch [9/20], Step [119/142], Loss: 0.9354\n",
            "Epoch [9/20], Step [120/142], Loss: 0.8428\n",
            "Epoch [9/20], Step [121/142], Loss: 0.9763\n",
            "Epoch [9/20], Step [122/142], Loss: 0.9276\n",
            "Epoch [9/20], Step [123/142], Loss: 0.8218\n",
            "Epoch [9/20], Step [124/142], Loss: 0.8775\n",
            "Epoch [9/20], Step [125/142], Loss: 0.7833\n",
            "Epoch [9/20], Step [126/142], Loss: 0.9279\n",
            "Epoch [9/20], Step [127/142], Loss: 0.8476\n",
            "Epoch [9/20], Step [128/142], Loss: 0.8495\n",
            "Epoch [9/20], Step [129/142], Loss: 0.8516\n",
            "Epoch [9/20], Step [130/142], Loss: 0.9163\n",
            "Epoch [9/20], Step [131/142], Loss: 0.8971\n",
            "Epoch [9/20], Step [132/142], Loss: 0.8032\n",
            "Epoch [9/20], Step [133/142], Loss: 0.7341\n",
            "Epoch [9/20], Step [134/142], Loss: 0.9196\n",
            "Epoch [9/20], Step [135/142], Loss: 0.8333\n",
            "Epoch [9/20], Step [136/142], Loss: 0.7689\n",
            "Epoch [9/20], Step [137/142], Loss: 0.8493\n",
            "Epoch [9/20], Step [138/142], Loss: 0.9151\n",
            "Epoch [9/20], Step [139/142], Loss: 0.8684\n",
            "Epoch [9/20], Step [140/142], Loss: 0.7936\n",
            "Epoch [9/20], Step [141/142], Loss: 0.8597\n",
            "Epoch [9/20], Step [142/142], Loss: 0.7120\n",
            "Epoch [10/20], Step [1/142], Loss: 0.7470\n",
            "Epoch [10/20], Step [2/142], Loss: 0.9235\n",
            "Epoch [10/20], Step [3/142], Loss: 0.8230\n",
            "Epoch [10/20], Step [4/142], Loss: 0.8771\n",
            "Epoch [10/20], Step [5/142], Loss: 0.8118\n",
            "Epoch [10/20], Step [6/142], Loss: 0.8836\n",
            "Epoch [10/20], Step [7/142], Loss: 0.7803\n",
            "Epoch [10/20], Step [8/142], Loss: 0.9604\n",
            "Epoch [10/20], Step [9/142], Loss: 0.8437\n",
            "Epoch [10/20], Step [10/142], Loss: 0.8687\n",
            "Epoch [10/20], Step [11/142], Loss: 0.8854\n",
            "Epoch [10/20], Step [12/142], Loss: 0.8582\n",
            "Epoch [10/20], Step [13/142], Loss: 0.7274\n",
            "Epoch [10/20], Step [14/142], Loss: 0.8224\n",
            "Epoch [10/20], Step [15/142], Loss: 0.8330\n",
            "Epoch [10/20], Step [16/142], Loss: 0.7963\n",
            "Epoch [10/20], Step [17/142], Loss: 0.8604\n",
            "Epoch [10/20], Step [18/142], Loss: 0.8078\n",
            "Epoch [10/20], Step [19/142], Loss: 0.8288\n",
            "Epoch [10/20], Step [20/142], Loss: 0.7655\n",
            "Epoch [10/20], Step [21/142], Loss: 0.8885\n",
            "Epoch [10/20], Step [22/142], Loss: 0.8485\n",
            "Epoch [10/20], Step [23/142], Loss: 0.8912\n",
            "Epoch [10/20], Step [24/142], Loss: 0.7959\n",
            "Epoch [10/20], Step [25/142], Loss: 0.9322\n",
            "Epoch [10/20], Step [26/142], Loss: 0.7794\n",
            "Epoch [10/20], Step [27/142], Loss: 0.8199\n",
            "Epoch [10/20], Step [28/142], Loss: 0.7263\n",
            "Epoch [10/20], Step [29/142], Loss: 0.8451\n",
            "Epoch [10/20], Step [30/142], Loss: 0.8313\n",
            "Epoch [10/20], Step [31/142], Loss: 0.8568\n",
            "Epoch [10/20], Step [32/142], Loss: 0.7967\n",
            "Epoch [10/20], Step [33/142], Loss: 0.8438\n",
            "Epoch [10/20], Step [34/142], Loss: 0.8083\n",
            "Epoch [10/20], Step [35/142], Loss: 0.8716\n",
            "Epoch [10/20], Step [36/142], Loss: 0.7644\n",
            "Epoch [10/20], Step [37/142], Loss: 0.8112\n",
            "Epoch [10/20], Step [38/142], Loss: 0.9058\n",
            "Epoch [10/20], Step [39/142], Loss: 0.7977\n",
            "Epoch [10/20], Step [40/142], Loss: 0.8297\n",
            "Epoch [10/20], Step [41/142], Loss: 0.8830\n",
            "Epoch [10/20], Step [42/142], Loss: 0.9038\n",
            "Epoch [10/20], Step [43/142], Loss: 0.8783\n",
            "Epoch [10/20], Step [44/142], Loss: 0.7683\n",
            "Epoch [10/20], Step [45/142], Loss: 0.8842\n",
            "Epoch [10/20], Step [46/142], Loss: 0.9667\n",
            "Epoch [10/20], Step [47/142], Loss: 0.8436\n",
            "Epoch [10/20], Step [48/142], Loss: 0.8640\n",
            "Epoch [10/20], Step [49/142], Loss: 0.8215\n",
            "Epoch [10/20], Step [50/142], Loss: 0.9106\n",
            "Epoch [10/20], Step [51/142], Loss: 0.8451\n",
            "Epoch [10/20], Step [52/142], Loss: 0.9446\n",
            "Epoch [10/20], Step [53/142], Loss: 0.8744\n",
            "Epoch [10/20], Step [54/142], Loss: 0.8126\n",
            "Epoch [10/20], Step [55/142], Loss: 0.8679\n",
            "Epoch [10/20], Step [56/142], Loss: 0.8403\n",
            "Epoch [10/20], Step [57/142], Loss: 0.8529\n",
            "Epoch [10/20], Step [58/142], Loss: 0.8339\n",
            "Epoch [10/20], Step [59/142], Loss: 0.8725\n",
            "Epoch [10/20], Step [60/142], Loss: 0.8978\n",
            "Epoch [10/20], Step [61/142], Loss: 0.8072\n",
            "Epoch [10/20], Step [62/142], Loss: 0.8956\n",
            "Epoch [10/20], Step [63/142], Loss: 0.9021\n",
            "Epoch [10/20], Step [64/142], Loss: 0.9168\n",
            "Epoch [10/20], Step [65/142], Loss: 0.8389\n",
            "Epoch [10/20], Step [66/142], Loss: 0.9206\n",
            "Epoch [10/20], Step [67/142], Loss: 0.7732\n",
            "Epoch [10/20], Step [68/142], Loss: 0.8758\n",
            "Epoch [10/20], Step [69/142], Loss: 0.8331\n",
            "Epoch [10/20], Step [70/142], Loss: 0.8849\n",
            "Epoch [10/20], Step [71/142], Loss: 0.9359\n",
            "Epoch [10/20], Step [72/142], Loss: 0.8745\n",
            "Epoch [10/20], Step [73/142], Loss: 0.8419\n",
            "Epoch [10/20], Step [74/142], Loss: 0.8948\n",
            "Epoch [10/20], Step [75/142], Loss: 0.8381\n",
            "Epoch [10/20], Step [76/142], Loss: 0.7986\n",
            "Epoch [10/20], Step [77/142], Loss: 0.8252\n",
            "Epoch [10/20], Step [78/142], Loss: 0.8326\n",
            "Epoch [10/20], Step [79/142], Loss: 0.9017\n",
            "Epoch [10/20], Step [80/142], Loss: 0.8800\n",
            "Epoch [10/20], Step [81/142], Loss: 0.8421\n",
            "Epoch [10/20], Step [82/142], Loss: 0.7594\n",
            "Epoch [10/20], Step [83/142], Loss: 0.8708\n",
            "Epoch [10/20], Step [84/142], Loss: 0.7987\n",
            "Epoch [10/20], Step [85/142], Loss: 0.8901\n",
            "Epoch [10/20], Step [86/142], Loss: 0.8027\n",
            "Epoch [10/20], Step [87/142], Loss: 0.7781\n",
            "Epoch [10/20], Step [88/142], Loss: 0.7662\n",
            "Epoch [10/20], Step [89/142], Loss: 0.8488\n",
            "Epoch [10/20], Step [90/142], Loss: 0.8306\n",
            "Epoch [10/20], Step [91/142], Loss: 0.9180\n",
            "Epoch [10/20], Step [92/142], Loss: 0.8858\n",
            "Epoch [10/20], Step [93/142], Loss: 0.7368\n",
            "Epoch [10/20], Step [94/142], Loss: 0.9042\n",
            "Epoch [10/20], Step [95/142], Loss: 0.8623\n",
            "Epoch [10/20], Step [96/142], Loss: 0.7643\n",
            "Epoch [10/20], Step [97/142], Loss: 0.8577\n",
            "Epoch [10/20], Step [98/142], Loss: 0.8474\n",
            "Epoch [10/20], Step [99/142], Loss: 0.8474\n",
            "Epoch [10/20], Step [100/142], Loss: 0.7959\n",
            "Epoch [10/20], Step [101/142], Loss: 0.8191\n",
            "Epoch [10/20], Step [102/142], Loss: 0.9397\n",
            "Epoch [10/20], Step [103/142], Loss: 0.7587\n",
            "Epoch [10/20], Step [104/142], Loss: 0.8383\n",
            "Epoch [10/20], Step [105/142], Loss: 0.8978\n",
            "Epoch [10/20], Step [106/142], Loss: 0.9270\n",
            "Epoch [10/20], Step [107/142], Loss: 0.7161\n",
            "Epoch [10/20], Step [108/142], Loss: 0.8406\n",
            "Epoch [10/20], Step [109/142], Loss: 0.8983\n",
            "Epoch [10/20], Step [110/142], Loss: 0.8535\n",
            "Epoch [10/20], Step [111/142], Loss: 0.8405\n",
            "Epoch [10/20], Step [112/142], Loss: 0.8051\n",
            "Epoch [10/20], Step [113/142], Loss: 0.7038\n",
            "Epoch [10/20], Step [114/142], Loss: 0.8041\n",
            "Epoch [10/20], Step [115/142], Loss: 0.8369\n",
            "Epoch [10/20], Step [116/142], Loss: 0.8899\n",
            "Epoch [10/20], Step [117/142], Loss: 0.8613\n",
            "Epoch [10/20], Step [118/142], Loss: 0.7718\n",
            "Epoch [10/20], Step [119/142], Loss: 0.7773\n",
            "Epoch [10/20], Step [120/142], Loss: 0.7976\n",
            "Epoch [10/20], Step [121/142], Loss: 0.8834\n",
            "Epoch [10/20], Step [122/142], Loss: 0.8519\n",
            "Epoch [10/20], Step [123/142], Loss: 0.8199\n",
            "Epoch [10/20], Step [124/142], Loss: 0.8153\n",
            "Epoch [10/20], Step [125/142], Loss: 0.8915\n",
            "Epoch [10/20], Step [126/142], Loss: 0.9193\n",
            "Epoch [10/20], Step [127/142], Loss: 0.8501\n",
            "Epoch [10/20], Step [128/142], Loss: 0.7367\n",
            "Epoch [10/20], Step [129/142], Loss: 0.9376\n",
            "Epoch [10/20], Step [130/142], Loss: 0.8857\n",
            "Epoch [10/20], Step [131/142], Loss: 0.7992\n",
            "Epoch [10/20], Step [132/142], Loss: 0.9122\n",
            "Epoch [10/20], Step [133/142], Loss: 0.9242\n",
            "Epoch [10/20], Step [134/142], Loss: 0.7818\n",
            "Epoch [10/20], Step [135/142], Loss: 0.8042\n",
            "Epoch [10/20], Step [136/142], Loss: 0.8967\n",
            "Epoch [10/20], Step [137/142], Loss: 0.8939\n",
            "Epoch [10/20], Step [138/142], Loss: 0.8968\n",
            "Epoch [10/20], Step [139/142], Loss: 0.8080\n",
            "Epoch [10/20], Step [140/142], Loss: 0.8609\n",
            "Epoch [10/20], Step [141/142], Loss: 0.7706\n",
            "Epoch [10/20], Step [142/142], Loss: 0.8380\n",
            "Epoch [11/20], Step [1/142], Loss: 0.8144\n",
            "Epoch [11/20], Step [2/142], Loss: 0.7833\n",
            "Epoch [11/20], Step [3/142], Loss: 0.7847\n",
            "Epoch [11/20], Step [4/142], Loss: 0.8556\n",
            "Epoch [11/20], Step [5/142], Loss: 0.8093\n",
            "Epoch [11/20], Step [6/142], Loss: 0.8507\n",
            "Epoch [11/20], Step [7/142], Loss: 0.8088\n",
            "Epoch [11/20], Step [8/142], Loss: 0.8580\n",
            "Epoch [11/20], Step [9/142], Loss: 0.8862\n",
            "Epoch [11/20], Step [10/142], Loss: 0.8416\n",
            "Epoch [11/20], Step [11/142], Loss: 0.8144\n",
            "Epoch [11/20], Step [12/142], Loss: 0.7763\n",
            "Epoch [11/20], Step [13/142], Loss: 0.9082\n",
            "Epoch [11/20], Step [14/142], Loss: 0.8845\n",
            "Epoch [11/20], Step [15/142], Loss: 0.8947\n",
            "Epoch [11/20], Step [16/142], Loss: 0.7294\n",
            "Epoch [11/20], Step [17/142], Loss: 0.8260\n",
            "Epoch [11/20], Step [18/142], Loss: 0.9288\n",
            "Epoch [11/20], Step [19/142], Loss: 0.8962\n",
            "Epoch [11/20], Step [20/142], Loss: 0.8547\n",
            "Epoch [11/20], Step [21/142], Loss: 0.8550\n",
            "Epoch [11/20], Step [22/142], Loss: 0.8234\n",
            "Epoch [11/20], Step [23/142], Loss: 0.8740\n",
            "Epoch [11/20], Step [24/142], Loss: 0.8795\n",
            "Epoch [11/20], Step [25/142], Loss: 0.8321\n",
            "Epoch [11/20], Step [26/142], Loss: 0.8130\n",
            "Epoch [11/20], Step [27/142], Loss: 0.8102\n",
            "Epoch [11/20], Step [28/142], Loss: 0.8036\n",
            "Epoch [11/20], Step [29/142], Loss: 0.8159\n",
            "Epoch [11/20], Step [30/142], Loss: 0.7849\n",
            "Epoch [11/20], Step [31/142], Loss: 0.8612\n",
            "Epoch [11/20], Step [32/142], Loss: 0.8629\n",
            "Epoch [11/20], Step [33/142], Loss: 0.9326\n",
            "Epoch [11/20], Step [34/142], Loss: 0.9286\n",
            "Epoch [11/20], Step [35/142], Loss: 0.8273\n",
            "Epoch [11/20], Step [36/142], Loss: 0.8304\n",
            "Epoch [11/20], Step [37/142], Loss: 0.8728\n",
            "Epoch [11/20], Step [38/142], Loss: 0.8436\n",
            "Epoch [11/20], Step [39/142], Loss: 0.8950\n",
            "Epoch [11/20], Step [40/142], Loss: 0.8386\n",
            "Epoch [11/20], Step [41/142], Loss: 0.9484\n",
            "Epoch [11/20], Step [42/142], Loss: 0.8161\n",
            "Epoch [11/20], Step [43/142], Loss: 0.7278\n",
            "Epoch [11/20], Step [44/142], Loss: 0.8241\n",
            "Epoch [11/20], Step [45/142], Loss: 0.8230\n",
            "Epoch [11/20], Step [46/142], Loss: 0.8296\n",
            "Epoch [11/20], Step [47/142], Loss: 0.8714\n",
            "Epoch [11/20], Step [48/142], Loss: 0.9196\n",
            "Epoch [11/20], Step [49/142], Loss: 0.8521\n",
            "Epoch [11/20], Step [50/142], Loss: 0.9237\n",
            "Epoch [11/20], Step [51/142], Loss: 0.8123\n",
            "Epoch [11/20], Step [52/142], Loss: 0.7930\n",
            "Epoch [11/20], Step [53/142], Loss: 0.8624\n",
            "Epoch [11/20], Step [54/142], Loss: 0.7878\n",
            "Epoch [11/20], Step [55/142], Loss: 0.7709\n",
            "Epoch [11/20], Step [56/142], Loss: 0.8476\n",
            "Epoch [11/20], Step [57/142], Loss: 0.8699\n",
            "Epoch [11/20], Step [58/142], Loss: 0.8376\n",
            "Epoch [11/20], Step [59/142], Loss: 0.8619\n",
            "Epoch [11/20], Step [60/142], Loss: 0.7678\n",
            "Epoch [11/20], Step [61/142], Loss: 0.8767\n",
            "Epoch [11/20], Step [62/142], Loss: 0.8507\n",
            "Epoch [11/20], Step [63/142], Loss: 0.9049\n",
            "Epoch [11/20], Step [64/142], Loss: 0.8978\n",
            "Epoch [11/20], Step [65/142], Loss: 0.7871\n",
            "Epoch [11/20], Step [66/142], Loss: 0.8470\n",
            "Epoch [11/20], Step [67/142], Loss: 0.8413\n",
            "Epoch [11/20], Step [68/142], Loss: 0.8967\n",
            "Epoch [11/20], Step [69/142], Loss: 0.7988\n",
            "Epoch [11/20], Step [70/142], Loss: 0.7718\n",
            "Epoch [11/20], Step [71/142], Loss: 0.9101\n",
            "Epoch [11/20], Step [72/142], Loss: 0.7091\n",
            "Epoch [11/20], Step [73/142], Loss: 0.7541\n",
            "Epoch [11/20], Step [74/142], Loss: 0.9130\n",
            "Epoch [11/20], Step [75/142], Loss: 0.8013\n",
            "Epoch [11/20], Step [76/142], Loss: 0.7937\n",
            "Epoch [11/20], Step [77/142], Loss: 0.7444\n",
            "Epoch [11/20], Step [78/142], Loss: 0.9281\n",
            "Epoch [11/20], Step [79/142], Loss: 0.8643\n",
            "Epoch [11/20], Step [80/142], Loss: 0.8273\n",
            "Epoch [11/20], Step [81/142], Loss: 1.0095\n",
            "Epoch [11/20], Step [82/142], Loss: 0.8671\n",
            "Epoch [11/20], Step [83/142], Loss: 0.8911\n",
            "Epoch [11/20], Step [84/142], Loss: 0.7970\n",
            "Epoch [11/20], Step [85/142], Loss: 0.9109\n",
            "Epoch [11/20], Step [86/142], Loss: 0.7755\n",
            "Epoch [11/20], Step [87/142], Loss: 0.9809\n",
            "Epoch [11/20], Step [88/142], Loss: 0.8648\n",
            "Epoch [11/20], Step [89/142], Loss: 0.8200\n",
            "Epoch [11/20], Step [90/142], Loss: 0.9414\n",
            "Epoch [11/20], Step [91/142], Loss: 0.8336\n",
            "Epoch [11/20], Step [92/142], Loss: 0.8398\n",
            "Epoch [11/20], Step [93/142], Loss: 0.8802\n",
            "Epoch [11/20], Step [94/142], Loss: 0.8402\n",
            "Epoch [11/20], Step [95/142], Loss: 0.9112\n",
            "Epoch [11/20], Step [96/142], Loss: 0.7506\n",
            "Epoch [11/20], Step [97/142], Loss: 0.8555\n",
            "Epoch [11/20], Step [98/142], Loss: 0.7215\n",
            "Epoch [11/20], Step [99/142], Loss: 0.8404\n",
            "Epoch [11/20], Step [100/142], Loss: 0.7834\n",
            "Epoch [11/20], Step [101/142], Loss: 0.8659\n",
            "Epoch [11/20], Step [102/142], Loss: 0.8275\n",
            "Epoch [11/20], Step [103/142], Loss: 0.9117\n",
            "Epoch [11/20], Step [104/142], Loss: 0.8245\n",
            "Epoch [11/20], Step [105/142], Loss: 0.9351\n",
            "Epoch [11/20], Step [106/142], Loss: 0.9182\n",
            "Epoch [11/20], Step [107/142], Loss: 0.9338\n",
            "Epoch [11/20], Step [108/142], Loss: 0.8155\n",
            "Epoch [11/20], Step [109/142], Loss: 0.8756\n",
            "Epoch [11/20], Step [110/142], Loss: 0.7411\n",
            "Epoch [11/20], Step [111/142], Loss: 0.8713\n",
            "Epoch [11/20], Step [112/142], Loss: 0.8438\n",
            "Epoch [11/20], Step [113/142], Loss: 0.9512\n",
            "Epoch [11/20], Step [114/142], Loss: 0.8100\n",
            "Epoch [11/20], Step [115/142], Loss: 0.8420\n",
            "Epoch [11/20], Step [116/142], Loss: 0.8466\n",
            "Epoch [11/20], Step [117/142], Loss: 0.9262\n",
            "Epoch [11/20], Step [118/142], Loss: 0.7905\n",
            "Epoch [11/20], Step [119/142], Loss: 0.8490\n",
            "Epoch [11/20], Step [120/142], Loss: 0.8731\n",
            "Epoch [11/20], Step [121/142], Loss: 0.7894\n",
            "Epoch [11/20], Step [122/142], Loss: 0.8584\n",
            "Epoch [11/20], Step [123/142], Loss: 0.8626\n",
            "Epoch [11/20], Step [124/142], Loss: 0.7981\n",
            "Epoch [11/20], Step [125/142], Loss: 0.8454\n",
            "Epoch [11/20], Step [126/142], Loss: 0.7897\n",
            "Epoch [11/20], Step [127/142], Loss: 0.7671\n",
            "Epoch [11/20], Step [128/142], Loss: 0.8132\n",
            "Epoch [11/20], Step [129/142], Loss: 0.7725\n",
            "Epoch [11/20], Step [130/142], Loss: 0.8270\n",
            "Epoch [11/20], Step [131/142], Loss: 0.8426\n",
            "Epoch [11/20], Step [132/142], Loss: 0.8666\n",
            "Epoch [11/20], Step [133/142], Loss: 0.7554\n",
            "Epoch [11/20], Step [134/142], Loss: 0.8427\n",
            "Epoch [11/20], Step [135/142], Loss: 0.8817\n",
            "Epoch [11/20], Step [136/142], Loss: 0.7733\n",
            "Epoch [11/20], Step [137/142], Loss: 0.8464\n",
            "Epoch [11/20], Step [138/142], Loss: 0.8103\n",
            "Epoch [11/20], Step [139/142], Loss: 0.7980\n",
            "Epoch [11/20], Step [140/142], Loss: 0.8890\n",
            "Epoch [11/20], Step [141/142], Loss: 0.8481\n",
            "Epoch [11/20], Step [142/142], Loss: 0.9550\n",
            "Epoch [12/20], Step [1/142], Loss: 0.8334\n",
            "Epoch [12/20], Step [2/142], Loss: 0.8193\n",
            "Epoch [12/20], Step [3/142], Loss: 0.7937\n",
            "Epoch [12/20], Step [4/142], Loss: 0.7774\n",
            "Epoch [12/20], Step [5/142], Loss: 0.8448\n",
            "Epoch [12/20], Step [6/142], Loss: 0.7777\n",
            "Epoch [12/20], Step [7/142], Loss: 0.8502\n",
            "Epoch [12/20], Step [8/142], Loss: 0.8540\n",
            "Epoch [12/20], Step [9/142], Loss: 0.9001\n",
            "Epoch [12/20], Step [10/142], Loss: 0.9245\n",
            "Epoch [12/20], Step [11/142], Loss: 0.9613\n",
            "Epoch [12/20], Step [12/142], Loss: 0.8540\n",
            "Epoch [12/20], Step [13/142], Loss: 0.7915\n",
            "Epoch [12/20], Step [14/142], Loss: 0.8342\n",
            "Epoch [12/20], Step [15/142], Loss: 0.8203\n",
            "Epoch [12/20], Step [16/142], Loss: 0.8496\n",
            "Epoch [12/20], Step [17/142], Loss: 0.9121\n",
            "Epoch [12/20], Step [18/142], Loss: 0.8757\n",
            "Epoch [12/20], Step [19/142], Loss: 0.8484\n",
            "Epoch [12/20], Step [20/142], Loss: 0.9292\n",
            "Epoch [12/20], Step [21/142], Loss: 0.8041\n",
            "Epoch [12/20], Step [22/142], Loss: 0.8862\n",
            "Epoch [12/20], Step [23/142], Loss: 0.8684\n",
            "Epoch [12/20], Step [24/142], Loss: 0.8476\n",
            "Epoch [12/20], Step [25/142], Loss: 0.8874\n",
            "Epoch [12/20], Step [26/142], Loss: 0.8663\n",
            "Epoch [12/20], Step [27/142], Loss: 0.8251\n",
            "Epoch [12/20], Step [28/142], Loss: 0.8312\n",
            "Epoch [12/20], Step [29/142], Loss: 0.7676\n",
            "Epoch [12/20], Step [30/142], Loss: 0.8083\n",
            "Epoch [12/20], Step [31/142], Loss: 0.9181\n",
            "Epoch [12/20], Step [32/142], Loss: 0.8727\n",
            "Epoch [12/20], Step [33/142], Loss: 0.8079\n",
            "Epoch [12/20], Step [34/142], Loss: 0.9432\n",
            "Epoch [12/20], Step [35/142], Loss: 0.7744\n",
            "Epoch [12/20], Step [36/142], Loss: 0.8417\n",
            "Epoch [12/20], Step [37/142], Loss: 0.7960\n",
            "Epoch [12/20], Step [38/142], Loss: 0.8507\n",
            "Epoch [12/20], Step [39/142], Loss: 0.9752\n",
            "Epoch [12/20], Step [40/142], Loss: 0.8761\n",
            "Epoch [12/20], Step [41/142], Loss: 0.8331\n",
            "Epoch [12/20], Step [42/142], Loss: 0.8475\n",
            "Epoch [12/20], Step [43/142], Loss: 0.8391\n",
            "Epoch [12/20], Step [44/142], Loss: 0.7741\n",
            "Epoch [12/20], Step [45/142], Loss: 0.7685\n",
            "Epoch [12/20], Step [46/142], Loss: 0.9056\n",
            "Epoch [12/20], Step [47/142], Loss: 0.9400\n",
            "Epoch [12/20], Step [48/142], Loss: 0.8527\n",
            "Epoch [12/20], Step [49/142], Loss: 0.8209\n",
            "Epoch [12/20], Step [50/142], Loss: 0.8240\n",
            "Epoch [12/20], Step [51/142], Loss: 0.8631\n",
            "Epoch [12/20], Step [52/142], Loss: 0.8431\n",
            "Epoch [12/20], Step [53/142], Loss: 0.9550\n",
            "Epoch [12/20], Step [54/142], Loss: 0.8595\n",
            "Epoch [12/20], Step [55/142], Loss: 0.9377\n",
            "Epoch [12/20], Step [56/142], Loss: 0.9206\n",
            "Epoch [12/20], Step [57/142], Loss: 0.7754\n",
            "Epoch [12/20], Step [58/142], Loss: 0.9095\n",
            "Epoch [12/20], Step [59/142], Loss: 0.8276\n",
            "Epoch [12/20], Step [60/142], Loss: 0.8473\n",
            "Epoch [12/20], Step [61/142], Loss: 0.8295\n",
            "Epoch [12/20], Step [62/142], Loss: 0.7076\n",
            "Epoch [12/20], Step [63/142], Loss: 0.7018\n",
            "Epoch [12/20], Step [64/142], Loss: 0.8833\n",
            "Epoch [12/20], Step [65/142], Loss: 0.8995\n",
            "Epoch [12/20], Step [66/142], Loss: 0.7831\n",
            "Epoch [12/20], Step [67/142], Loss: 0.8898\n",
            "Epoch [12/20], Step [68/142], Loss: 0.7777\n",
            "Epoch [12/20], Step [69/142], Loss: 0.7504\n",
            "Epoch [12/20], Step [70/142], Loss: 0.7889\n",
            "Epoch [12/20], Step [71/142], Loss: 0.8680\n",
            "Epoch [12/20], Step [72/142], Loss: 0.7926\n",
            "Epoch [12/20], Step [73/142], Loss: 0.8753\n",
            "Epoch [12/20], Step [74/142], Loss: 0.9518\n",
            "Epoch [12/20], Step [75/142], Loss: 0.7732\n",
            "Epoch [12/20], Step [76/142], Loss: 0.9666\n",
            "Epoch [12/20], Step [77/142], Loss: 0.7745\n",
            "Epoch [12/20], Step [78/142], Loss: 0.8168\n",
            "Epoch [12/20], Step [79/142], Loss: 0.7812\n",
            "Epoch [12/20], Step [80/142], Loss: 0.7949\n",
            "Epoch [12/20], Step [81/142], Loss: 0.9059\n",
            "Epoch [12/20], Step [82/142], Loss: 0.8768\n",
            "Epoch [12/20], Step [83/142], Loss: 0.8536\n",
            "Epoch [12/20], Step [84/142], Loss: 0.7624\n",
            "Epoch [12/20], Step [85/142], Loss: 0.8869\n",
            "Epoch [12/20], Step [86/142], Loss: 0.8569\n",
            "Epoch [12/20], Step [87/142], Loss: 0.7933\n",
            "Epoch [12/20], Step [88/142], Loss: 0.8598\n",
            "Epoch [12/20], Step [89/142], Loss: 0.8750\n",
            "Epoch [12/20], Step [90/142], Loss: 0.8748\n",
            "Epoch [12/20], Step [91/142], Loss: 0.7639\n",
            "Epoch [12/20], Step [92/142], Loss: 0.7986\n",
            "Epoch [12/20], Step [93/142], Loss: 0.8514\n",
            "Epoch [12/20], Step [94/142], Loss: 0.8107\n",
            "Epoch [12/20], Step [95/142], Loss: 0.7766\n",
            "Epoch [12/20], Step [96/142], Loss: 0.8371\n",
            "Epoch [12/20], Step [97/142], Loss: 0.8948\n",
            "Epoch [12/20], Step [98/142], Loss: 0.8609\n",
            "Epoch [12/20], Step [99/142], Loss: 0.8147\n",
            "Epoch [12/20], Step [100/142], Loss: 0.8042\n",
            "Epoch [12/20], Step [101/142], Loss: 0.8918\n",
            "Epoch [12/20], Step [102/142], Loss: 0.8007\n",
            "Epoch [12/20], Step [103/142], Loss: 0.8525\n",
            "Epoch [12/20], Step [104/142], Loss: 0.8821\n",
            "Epoch [12/20], Step [105/142], Loss: 0.8179\n",
            "Epoch [12/20], Step [106/142], Loss: 0.7999\n",
            "Epoch [12/20], Step [107/142], Loss: 0.8024\n",
            "Epoch [12/20], Step [108/142], Loss: 0.9543\n",
            "Epoch [12/20], Step [109/142], Loss: 0.8020\n",
            "Epoch [12/20], Step [110/142], Loss: 0.8164\n",
            "Epoch [12/20], Step [111/142], Loss: 0.8005\n",
            "Epoch [12/20], Step [112/142], Loss: 0.9520\n",
            "Epoch [12/20], Step [113/142], Loss: 0.9363\n",
            "Epoch [12/20], Step [114/142], Loss: 0.8196\n",
            "Epoch [12/20], Step [115/142], Loss: 0.7878\n",
            "Epoch [12/20], Step [116/142], Loss: 0.7919\n",
            "Epoch [12/20], Step [117/142], Loss: 0.8853\n",
            "Epoch [12/20], Step [118/142], Loss: 0.8617\n",
            "Epoch [12/20], Step [119/142], Loss: 0.8446\n",
            "Epoch [12/20], Step [120/142], Loss: 0.8247\n",
            "Epoch [12/20], Step [121/142], Loss: 0.8402\n",
            "Epoch [12/20], Step [122/142], Loss: 0.8471\n",
            "Epoch [12/20], Step [123/142], Loss: 0.8414\n",
            "Epoch [12/20], Step [124/142], Loss: 0.8537\n",
            "Epoch [12/20], Step [125/142], Loss: 0.7884\n",
            "Epoch [12/20], Step [126/142], Loss: 0.8647\n",
            "Epoch [12/20], Step [127/142], Loss: 0.7756\n",
            "Epoch [12/20], Step [128/142], Loss: 0.8739\n",
            "Epoch [12/20], Step [129/142], Loss: 0.9608\n",
            "Epoch [12/20], Step [130/142], Loss: 0.7386\n",
            "Epoch [12/20], Step [131/142], Loss: 0.7762\n",
            "Epoch [12/20], Step [132/142], Loss: 0.8470\n",
            "Epoch [12/20], Step [133/142], Loss: 0.8090\n",
            "Epoch [12/20], Step [134/142], Loss: 0.8207\n",
            "Epoch [12/20], Step [135/142], Loss: 0.8939\n",
            "Epoch [12/20], Step [136/142], Loss: 0.7812\n",
            "Epoch [12/20], Step [137/142], Loss: 0.8877\n",
            "Epoch [12/20], Step [138/142], Loss: 0.8755\n",
            "Epoch [12/20], Step [139/142], Loss: 0.7648\n",
            "Epoch [12/20], Step [140/142], Loss: 0.8720\n",
            "Epoch [12/20], Step [141/142], Loss: 0.9080\n",
            "Epoch [12/20], Step [142/142], Loss: 0.7140\n",
            "Epoch [13/20], Step [1/142], Loss: 0.8724\n",
            "Epoch [13/20], Step [2/142], Loss: 0.9206\n",
            "Epoch [13/20], Step [3/142], Loss: 0.8372\n",
            "Epoch [13/20], Step [4/142], Loss: 0.9012\n",
            "Epoch [13/20], Step [5/142], Loss: 0.8634\n",
            "Epoch [13/20], Step [6/142], Loss: 0.8785\n",
            "Epoch [13/20], Step [7/142], Loss: 0.8225\n",
            "Epoch [13/20], Step [8/142], Loss: 0.8755\n",
            "Epoch [13/20], Step [9/142], Loss: 0.7955\n",
            "Epoch [13/20], Step [10/142], Loss: 0.8080\n",
            "Epoch [13/20], Step [11/142], Loss: 0.8492\n",
            "Epoch [13/20], Step [12/142], Loss: 0.8074\n",
            "Epoch [13/20], Step [13/142], Loss: 0.9075\n",
            "Epoch [13/20], Step [14/142], Loss: 0.7532\n",
            "Epoch [13/20], Step [15/142], Loss: 0.7626\n",
            "Epoch [13/20], Step [16/142], Loss: 0.7596\n",
            "Epoch [13/20], Step [17/142], Loss: 0.9270\n",
            "Epoch [13/20], Step [18/142], Loss: 0.8543\n",
            "Epoch [13/20], Step [19/142], Loss: 0.8687\n",
            "Epoch [13/20], Step [20/142], Loss: 0.8682\n",
            "Epoch [13/20], Step [21/142], Loss: 0.7411\n",
            "Epoch [13/20], Step [22/142], Loss: 0.9016\n",
            "Epoch [13/20], Step [23/142], Loss: 0.8472\n",
            "Epoch [13/20], Step [24/142], Loss: 0.8174\n",
            "Epoch [13/20], Step [25/142], Loss: 0.9048\n",
            "Epoch [13/20], Step [26/142], Loss: 0.7960\n",
            "Epoch [13/20], Step [27/142], Loss: 0.8501\n",
            "Epoch [13/20], Step [28/142], Loss: 0.8512\n",
            "Epoch [13/20], Step [29/142], Loss: 0.7952\n",
            "Epoch [13/20], Step [30/142], Loss: 0.8174\n",
            "Epoch [13/20], Step [31/142], Loss: 0.8010\n",
            "Epoch [13/20], Step [32/142], Loss: 0.8030\n",
            "Epoch [13/20], Step [33/142], Loss: 0.8006\n",
            "Epoch [13/20], Step [34/142], Loss: 0.8346\n",
            "Epoch [13/20], Step [35/142], Loss: 0.8196\n",
            "Epoch [13/20], Step [36/142], Loss: 0.8913\n",
            "Epoch [13/20], Step [37/142], Loss: 0.7460\n",
            "Epoch [13/20], Step [38/142], Loss: 0.8504\n",
            "Epoch [13/20], Step [39/142], Loss: 0.8578\n",
            "Epoch [13/20], Step [40/142], Loss: 0.8444\n",
            "Epoch [13/20], Step [41/142], Loss: 0.8878\n",
            "Epoch [13/20], Step [42/142], Loss: 0.8776\n",
            "Epoch [13/20], Step [43/142], Loss: 0.8506\n",
            "Epoch [13/20], Step [44/142], Loss: 0.9763\n",
            "Epoch [13/20], Step [45/142], Loss: 0.8510\n",
            "Epoch [13/20], Step [46/142], Loss: 0.8396\n",
            "Epoch [13/20], Step [47/142], Loss: 0.7827\n",
            "Epoch [13/20], Step [48/142], Loss: 0.7462\n",
            "Epoch [13/20], Step [49/142], Loss: 0.8576\n",
            "Epoch [13/20], Step [50/142], Loss: 0.8787\n",
            "Epoch [13/20], Step [51/142], Loss: 0.7600\n",
            "Epoch [13/20], Step [52/142], Loss: 0.8432\n",
            "Epoch [13/20], Step [53/142], Loss: 0.8889\n",
            "Epoch [13/20], Step [54/142], Loss: 0.8934\n",
            "Epoch [13/20], Step [55/142], Loss: 0.8544\n",
            "Epoch [13/20], Step [56/142], Loss: 0.8459\n",
            "Epoch [13/20], Step [57/142], Loss: 0.8980\n",
            "Epoch [13/20], Step [58/142], Loss: 0.8812\n",
            "Epoch [13/20], Step [59/142], Loss: 0.8565\n",
            "Epoch [13/20], Step [60/142], Loss: 0.8812\n",
            "Epoch [13/20], Step [61/142], Loss: 0.9656\n",
            "Epoch [13/20], Step [62/142], Loss: 0.8006\n",
            "Epoch [13/20], Step [63/142], Loss: 0.8882\n",
            "Epoch [13/20], Step [64/142], Loss: 0.8142\n",
            "Epoch [13/20], Step [65/142], Loss: 0.7522\n",
            "Epoch [13/20], Step [66/142], Loss: 0.7871\n",
            "Epoch [13/20], Step [67/142], Loss: 0.8597\n",
            "Epoch [13/20], Step [68/142], Loss: 0.9626\n",
            "Epoch [13/20], Step [69/142], Loss: 0.7275\n",
            "Epoch [13/20], Step [70/142], Loss: 0.8922\n",
            "Epoch [13/20], Step [71/142], Loss: 0.7710\n",
            "Epoch [13/20], Step [72/142], Loss: 0.8028\n",
            "Epoch [13/20], Step [73/142], Loss: 0.7301\n",
            "Epoch [13/20], Step [74/142], Loss: 0.8105\n",
            "Epoch [13/20], Step [75/142], Loss: 0.8162\n",
            "Epoch [13/20], Step [76/142], Loss: 0.8507\n",
            "Epoch [13/20], Step [77/142], Loss: 0.7335\n",
            "Epoch [13/20], Step [78/142], Loss: 0.8474\n",
            "Epoch [13/20], Step [79/142], Loss: 0.8668\n",
            "Epoch [13/20], Step [80/142], Loss: 0.8453\n",
            "Epoch [13/20], Step [81/142], Loss: 0.8397\n",
            "Epoch [13/20], Step [82/142], Loss: 0.8654\n",
            "Epoch [13/20], Step [83/142], Loss: 0.7867\n",
            "Epoch [13/20], Step [84/142], Loss: 0.8107\n",
            "Epoch [13/20], Step [85/142], Loss: 0.9250\n",
            "Epoch [13/20], Step [86/142], Loss: 0.8800\n",
            "Epoch [13/20], Step [87/142], Loss: 0.7892\n",
            "Epoch [13/20], Step [88/142], Loss: 0.7811\n",
            "Epoch [13/20], Step [89/142], Loss: 0.7933\n",
            "Epoch [13/20], Step [90/142], Loss: 0.9206\n",
            "Epoch [13/20], Step [91/142], Loss: 0.8751\n",
            "Epoch [13/20], Step [92/142], Loss: 0.9097\n",
            "Epoch [13/20], Step [93/142], Loss: 0.8197\n",
            "Epoch [13/20], Step [94/142], Loss: 0.8385\n",
            "Epoch [13/20], Step [95/142], Loss: 0.7961\n",
            "Epoch [13/20], Step [96/142], Loss: 0.8704\n",
            "Epoch [13/20], Step [97/142], Loss: 0.8169\n",
            "Epoch [13/20], Step [98/142], Loss: 0.8722\n",
            "Epoch [13/20], Step [99/142], Loss: 0.9028\n",
            "Epoch [13/20], Step [100/142], Loss: 0.8322\n",
            "Epoch [13/20], Step [101/142], Loss: 0.8050\n",
            "Epoch [13/20], Step [102/142], Loss: 0.7747\n",
            "Epoch [13/20], Step [103/142], Loss: 0.7837\n",
            "Epoch [13/20], Step [104/142], Loss: 0.8980\n",
            "Epoch [13/20], Step [105/142], Loss: 0.7767\n",
            "Epoch [13/20], Step [106/142], Loss: 0.7984\n",
            "Epoch [13/20], Step [107/142], Loss: 0.9022\n",
            "Epoch [13/20], Step [108/142], Loss: 0.8082\n",
            "Epoch [13/20], Step [109/142], Loss: 0.8817\n",
            "Epoch [13/20], Step [110/142], Loss: 0.8045\n",
            "Epoch [13/20], Step [111/142], Loss: 0.8907\n",
            "Epoch [13/20], Step [112/142], Loss: 0.8699\n",
            "Epoch [13/20], Step [113/142], Loss: 0.9091\n",
            "Epoch [13/20], Step [114/142], Loss: 0.8107\n",
            "Epoch [13/20], Step [115/142], Loss: 0.8806\n",
            "Epoch [13/20], Step [116/142], Loss: 0.8238\n",
            "Epoch [13/20], Step [117/142], Loss: 0.7567\n",
            "Epoch [13/20], Step [118/142], Loss: 0.8447\n",
            "Epoch [13/20], Step [119/142], Loss: 0.8157\n",
            "Epoch [13/20], Step [120/142], Loss: 0.7551\n",
            "Epoch [13/20], Step [121/142], Loss: 0.7146\n",
            "Epoch [13/20], Step [122/142], Loss: 0.8766\n",
            "Epoch [13/20], Step [123/142], Loss: 0.8862\n",
            "Epoch [13/20], Step [124/142], Loss: 0.8837\n",
            "Epoch [13/20], Step [125/142], Loss: 0.8948\n",
            "Epoch [13/20], Step [126/142], Loss: 0.8492\n",
            "Epoch [13/20], Step [127/142], Loss: 0.9484\n",
            "Epoch [13/20], Step [128/142], Loss: 0.8487\n",
            "Epoch [13/20], Step [129/142], Loss: 0.7734\n",
            "Epoch [13/20], Step [130/142], Loss: 0.8615\n",
            "Epoch [13/20], Step [131/142], Loss: 0.8505\n",
            "Epoch [13/20], Step [132/142], Loss: 0.8380\n",
            "Epoch [13/20], Step [133/142], Loss: 0.7876\n",
            "Epoch [13/20], Step [134/142], Loss: 0.8596\n",
            "Epoch [13/20], Step [135/142], Loss: 0.8007\n",
            "Epoch [13/20], Step [136/142], Loss: 0.8524\n",
            "Epoch [13/20], Step [137/142], Loss: 0.8485\n",
            "Epoch [13/20], Step [138/142], Loss: 0.9151\n",
            "Epoch [13/20], Step [139/142], Loss: 0.8401\n",
            "Epoch [13/20], Step [140/142], Loss: 0.8622\n",
            "Epoch [13/20], Step [141/142], Loss: 0.8584\n",
            "Epoch [13/20], Step [142/142], Loss: 1.0451\n",
            "Epoch [14/20], Step [1/142], Loss: 0.9184\n",
            "Epoch [14/20], Step [2/142], Loss: 0.7942\n",
            "Epoch [14/20], Step [3/142], Loss: 0.8615\n",
            "Epoch [14/20], Step [4/142], Loss: 0.8208\n",
            "Epoch [14/20], Step [5/142], Loss: 0.8458\n",
            "Epoch [14/20], Step [6/142], Loss: 0.8488\n",
            "Epoch [14/20], Step [7/142], Loss: 0.8453\n",
            "Epoch [14/20], Step [8/142], Loss: 0.8006\n",
            "Epoch [14/20], Step [9/142], Loss: 0.8290\n",
            "Epoch [14/20], Step [10/142], Loss: 0.8726\n",
            "Epoch [14/20], Step [11/142], Loss: 0.8443\n",
            "Epoch [14/20], Step [12/142], Loss: 0.8201\n",
            "Epoch [14/20], Step [13/142], Loss: 0.8110\n",
            "Epoch [14/20], Step [14/142], Loss: 0.8655\n",
            "Epoch [14/20], Step [15/142], Loss: 0.8135\n",
            "Epoch [14/20], Step [16/142], Loss: 0.8606\n",
            "Epoch [14/20], Step [17/142], Loss: 0.8255\n",
            "Epoch [14/20], Step [18/142], Loss: 0.7793\n",
            "Epoch [14/20], Step [19/142], Loss: 0.7771\n",
            "Epoch [14/20], Step [20/142], Loss: 0.8398\n",
            "Epoch [14/20], Step [21/142], Loss: 0.7681\n",
            "Epoch [14/20], Step [22/142], Loss: 0.8367\n",
            "Epoch [14/20], Step [23/142], Loss: 0.7986\n",
            "Epoch [14/20], Step [24/142], Loss: 0.8379\n",
            "Epoch [14/20], Step [25/142], Loss: 0.8429\n",
            "Epoch [14/20], Step [26/142], Loss: 0.7734\n",
            "Epoch [14/20], Step [27/142], Loss: 0.9137\n",
            "Epoch [14/20], Step [28/142], Loss: 0.8280\n",
            "Epoch [14/20], Step [29/142], Loss: 0.8336\n",
            "Epoch [14/20], Step [30/142], Loss: 0.8146\n",
            "Epoch [14/20], Step [31/142], Loss: 0.7584\n",
            "Epoch [14/20], Step [32/142], Loss: 0.8437\n",
            "Epoch [14/20], Step [33/142], Loss: 0.8319\n",
            "Epoch [14/20], Step [34/142], Loss: 0.8962\n",
            "Epoch [14/20], Step [35/142], Loss: 0.8716\n",
            "Epoch [14/20], Step [36/142], Loss: 0.9003\n",
            "Epoch [14/20], Step [37/142], Loss: 0.9082\n",
            "Epoch [14/20], Step [38/142], Loss: 0.7938\n",
            "Epoch [14/20], Step [39/142], Loss: 0.7871\n",
            "Epoch [14/20], Step [40/142], Loss: 0.8458\n",
            "Epoch [14/20], Step [41/142], Loss: 0.9170\n",
            "Epoch [14/20], Step [42/142], Loss: 0.8513\n",
            "Epoch [14/20], Step [43/142], Loss: 0.8324\n",
            "Epoch [14/20], Step [44/142], Loss: 0.8290\n",
            "Epoch [14/20], Step [45/142], Loss: 0.9005\n",
            "Epoch [14/20], Step [46/142], Loss: 0.8316\n",
            "Epoch [14/20], Step [47/142], Loss: 0.7772\n",
            "Epoch [14/20], Step [48/142], Loss: 0.7323\n",
            "Epoch [14/20], Step [49/142], Loss: 0.7848\n",
            "Epoch [14/20], Step [50/142], Loss: 0.8163\n",
            "Epoch [14/20], Step [51/142], Loss: 0.7830\n",
            "Epoch [14/20], Step [52/142], Loss: 0.7672\n",
            "Epoch [14/20], Step [53/142], Loss: 0.8460\n",
            "Epoch [14/20], Step [54/142], Loss: 0.8200\n",
            "Epoch [14/20], Step [55/142], Loss: 0.8538\n",
            "Epoch [14/20], Step [56/142], Loss: 0.8013\n",
            "Epoch [14/20], Step [57/142], Loss: 0.9080\n",
            "Epoch [14/20], Step [58/142], Loss: 0.8020\n",
            "Epoch [14/20], Step [59/142], Loss: 0.8285\n",
            "Epoch [14/20], Step [60/142], Loss: 0.8823\n",
            "Epoch [14/20], Step [61/142], Loss: 0.7971\n",
            "Epoch [14/20], Step [62/142], Loss: 0.9524\n",
            "Epoch [14/20], Step [63/142], Loss: 0.8056\n",
            "Epoch [14/20], Step [64/142], Loss: 0.8445\n",
            "Epoch [14/20], Step [65/142], Loss: 0.8108\n",
            "Epoch [14/20], Step [66/142], Loss: 0.8387\n",
            "Epoch [14/20], Step [67/142], Loss: 0.7927\n",
            "Epoch [14/20], Step [68/142], Loss: 0.8103\n",
            "Epoch [14/20], Step [69/142], Loss: 0.8770\n",
            "Epoch [14/20], Step [70/142], Loss: 0.8567\n",
            "Epoch [14/20], Step [71/142], Loss: 0.8413\n",
            "Epoch [14/20], Step [72/142], Loss: 0.7740\n",
            "Epoch [14/20], Step [73/142], Loss: 0.9886\n",
            "Epoch [14/20], Step [74/142], Loss: 0.8072\n",
            "Epoch [14/20], Step [75/142], Loss: 0.8672\n",
            "Epoch [14/20], Step [76/142], Loss: 0.7807\n",
            "Epoch [14/20], Step [77/142], Loss: 0.8685\n",
            "Epoch [14/20], Step [78/142], Loss: 0.9311\n",
            "Epoch [14/20], Step [79/142], Loss: 0.8974\n",
            "Epoch [14/20], Step [80/142], Loss: 0.8477\n",
            "Epoch [14/20], Step [81/142], Loss: 0.8628\n",
            "Epoch [14/20], Step [82/142], Loss: 0.7782\n",
            "Epoch [14/20], Step [83/142], Loss: 0.7578\n",
            "Epoch [14/20], Step [84/142], Loss: 0.8830\n",
            "Epoch [14/20], Step [85/142], Loss: 0.9347\n",
            "Epoch [14/20], Step [86/142], Loss: 0.8014\n",
            "Epoch [14/20], Step [87/142], Loss: 0.7563\n",
            "Epoch [14/20], Step [88/142], Loss: 0.8962\n",
            "Epoch [14/20], Step [89/142], Loss: 0.9530\n",
            "Epoch [14/20], Step [90/142], Loss: 0.8779\n",
            "Epoch [14/20], Step [91/142], Loss: 0.7956\n",
            "Epoch [14/20], Step [92/142], Loss: 0.9087\n",
            "Epoch [14/20], Step [93/142], Loss: 0.8273\n",
            "Epoch [14/20], Step [94/142], Loss: 0.8493\n",
            "Epoch [14/20], Step [95/142], Loss: 0.7923\n",
            "Epoch [14/20], Step [96/142], Loss: 0.8681\n",
            "Epoch [14/20], Step [97/142], Loss: 0.8780\n",
            "Epoch [14/20], Step [98/142], Loss: 0.8097\n",
            "Epoch [14/20], Step [99/142], Loss: 0.8335\n",
            "Epoch [14/20], Step [100/142], Loss: 0.8089\n",
            "Epoch [14/20], Step [101/142], Loss: 0.7382\n",
            "Epoch [14/20], Step [102/142], Loss: 0.8001\n",
            "Epoch [14/20], Step [103/142], Loss: 0.8410\n",
            "Epoch [14/20], Step [104/142], Loss: 0.9601\n",
            "Epoch [14/20], Step [105/142], Loss: 0.9266\n",
            "Epoch [14/20], Step [106/142], Loss: 0.8703\n",
            "Epoch [14/20], Step [107/142], Loss: 0.8586\n",
            "Epoch [14/20], Step [108/142], Loss: 0.8456\n",
            "Epoch [14/20], Step [109/142], Loss: 0.9121\n",
            "Epoch [14/20], Step [110/142], Loss: 0.8528\n",
            "Epoch [14/20], Step [111/142], Loss: 0.8046\n",
            "Epoch [14/20], Step [112/142], Loss: 0.8990\n",
            "Epoch [14/20], Step [113/142], Loss: 0.8267\n",
            "Epoch [14/20], Step [114/142], Loss: 0.8957\n",
            "Epoch [14/20], Step [115/142], Loss: 0.8653\n",
            "Epoch [14/20], Step [116/142], Loss: 0.7906\n",
            "Epoch [14/20], Step [117/142], Loss: 0.8619\n",
            "Epoch [14/20], Step [118/142], Loss: 0.8456\n",
            "Epoch [14/20], Step [119/142], Loss: 0.7934\n",
            "Epoch [14/20], Step [120/142], Loss: 0.8307\n",
            "Epoch [14/20], Step [121/142], Loss: 0.8806\n",
            "Epoch [14/20], Step [122/142], Loss: 0.8114\n",
            "Epoch [14/20], Step [123/142], Loss: 0.8273\n",
            "Epoch [14/20], Step [124/142], Loss: 0.8851\n",
            "Epoch [14/20], Step [125/142], Loss: 0.8250\n",
            "Epoch [14/20], Step [126/142], Loss: 0.8262\n",
            "Epoch [14/20], Step [127/142], Loss: 0.8566\n",
            "Epoch [14/20], Step [128/142], Loss: 0.8663\n",
            "Epoch [14/20], Step [129/142], Loss: 0.9290\n",
            "Epoch [14/20], Step [130/142], Loss: 0.7522\n",
            "Epoch [14/20], Step [131/142], Loss: 0.8100\n",
            "Epoch [14/20], Step [132/142], Loss: 0.7472\n",
            "Epoch [14/20], Step [133/142], Loss: 0.9393\n",
            "Epoch [14/20], Step [134/142], Loss: 0.8564\n",
            "Epoch [14/20], Step [135/142], Loss: 0.7503\n",
            "Epoch [14/20], Step [136/142], Loss: 0.8648\n",
            "Epoch [14/20], Step [137/142], Loss: 0.8611\n",
            "Epoch [14/20], Step [138/142], Loss: 0.8211\n",
            "Epoch [14/20], Step [139/142], Loss: 0.9297\n",
            "Epoch [14/20], Step [140/142], Loss: 0.9230\n",
            "Epoch [14/20], Step [141/142], Loss: 0.7928\n",
            "Epoch [14/20], Step [142/142], Loss: 0.9044\n",
            "Epoch [15/20], Step [1/142], Loss: 0.8677\n",
            "Epoch [15/20], Step [2/142], Loss: 0.8205\n",
            "Epoch [15/20], Step [3/142], Loss: 0.8780\n",
            "Epoch [15/20], Step [4/142], Loss: 0.9821\n",
            "Epoch [15/20], Step [5/142], Loss: 0.8478\n",
            "Epoch [15/20], Step [6/142], Loss: 0.7845\n",
            "Epoch [15/20], Step [7/142], Loss: 0.8181\n",
            "Epoch [15/20], Step [8/142], Loss: 0.8596\n",
            "Epoch [15/20], Step [9/142], Loss: 0.8432\n",
            "Epoch [15/20], Step [10/142], Loss: 0.8549\n",
            "Epoch [15/20], Step [11/142], Loss: 0.7986\n",
            "Epoch [15/20], Step [12/142], Loss: 0.8572\n",
            "Epoch [15/20], Step [13/142], Loss: 0.7862\n",
            "Epoch [15/20], Step [14/142], Loss: 0.8149\n",
            "Epoch [15/20], Step [15/142], Loss: 0.8728\n",
            "Epoch [15/20], Step [16/142], Loss: 0.7955\n",
            "Epoch [15/20], Step [17/142], Loss: 0.8591\n",
            "Epoch [15/20], Step [18/142], Loss: 0.8899\n",
            "Epoch [15/20], Step [19/142], Loss: 0.7874\n",
            "Epoch [15/20], Step [20/142], Loss: 0.8544\n",
            "Epoch [15/20], Step [21/142], Loss: 0.8623\n",
            "Epoch [15/20], Step [22/142], Loss: 0.8598\n",
            "Epoch [15/20], Step [23/142], Loss: 0.8876\n",
            "Epoch [15/20], Step [24/142], Loss: 0.9488\n",
            "Epoch [15/20], Step [25/142], Loss: 0.8660\n",
            "Epoch [15/20], Step [26/142], Loss: 0.8074\n",
            "Epoch [15/20], Step [27/142], Loss: 0.7702\n",
            "Epoch [15/20], Step [28/142], Loss: 0.8451\n",
            "Epoch [15/20], Step [29/142], Loss: 0.8370\n",
            "Epoch [15/20], Step [30/142], Loss: 0.8553\n",
            "Epoch [15/20], Step [31/142], Loss: 0.7547\n",
            "Epoch [15/20], Step [32/142], Loss: 0.8670\n",
            "Epoch [15/20], Step [33/142], Loss: 0.8060\n",
            "Epoch [15/20], Step [34/142], Loss: 0.7970\n",
            "Epoch [15/20], Step [35/142], Loss: 0.8857\n",
            "Epoch [15/20], Step [36/142], Loss: 0.9239\n",
            "Epoch [15/20], Step [37/142], Loss: 0.7679\n",
            "Epoch [15/20], Step [38/142], Loss: 0.8723\n",
            "Epoch [15/20], Step [39/142], Loss: 0.8588\n",
            "Epoch [15/20], Step [40/142], Loss: 0.9067\n",
            "Epoch [15/20], Step [41/142], Loss: 0.8110\n",
            "Epoch [15/20], Step [42/142], Loss: 0.8715\n",
            "Epoch [15/20], Step [43/142], Loss: 0.7699\n",
            "Epoch [15/20], Step [44/142], Loss: 0.7529\n",
            "Epoch [15/20], Step [45/142], Loss: 0.9410\n",
            "Epoch [15/20], Step [46/142], Loss: 0.9105\n",
            "Epoch [15/20], Step [47/142], Loss: 0.8144\n",
            "Epoch [15/20], Step [48/142], Loss: 0.8681\n",
            "Epoch [15/20], Step [49/142], Loss: 0.7858\n",
            "Epoch [15/20], Step [50/142], Loss: 0.9292\n",
            "Epoch [15/20], Step [51/142], Loss: 0.7993\n",
            "Epoch [15/20], Step [52/142], Loss: 0.7839\n",
            "Epoch [15/20], Step [53/142], Loss: 0.7715\n",
            "Epoch [15/20], Step [54/142], Loss: 0.8703\n",
            "Epoch [15/20], Step [55/142], Loss: 0.7213\n",
            "Epoch [15/20], Step [56/142], Loss: 0.8494\n",
            "Epoch [15/20], Step [57/142], Loss: 0.8887\n",
            "Epoch [15/20], Step [58/142], Loss: 0.8179\n",
            "Epoch [15/20], Step [59/142], Loss: 0.8639\n",
            "Epoch [15/20], Step [60/142], Loss: 0.8685\n",
            "Epoch [15/20], Step [61/142], Loss: 0.8570\n",
            "Epoch [15/20], Step [62/142], Loss: 0.8530\n",
            "Epoch [15/20], Step [63/142], Loss: 0.8442\n",
            "Epoch [15/20], Step [64/142], Loss: 0.7721\n",
            "Epoch [15/20], Step [65/142], Loss: 0.8786\n",
            "Epoch [15/20], Step [66/142], Loss: 0.8224\n",
            "Epoch [15/20], Step [67/142], Loss: 0.8843\n",
            "Epoch [15/20], Step [68/142], Loss: 0.8322\n",
            "Epoch [15/20], Step [69/142], Loss: 0.7533\n",
            "Epoch [15/20], Step [70/142], Loss: 0.8597\n",
            "Epoch [15/20], Step [71/142], Loss: 0.7724\n",
            "Epoch [15/20], Step [72/142], Loss: 0.8602\n",
            "Epoch [15/20], Step [73/142], Loss: 0.7937\n",
            "Epoch [15/20], Step [74/142], Loss: 0.8828\n",
            "Epoch [15/20], Step [75/142], Loss: 0.7590\n",
            "Epoch [15/20], Step [76/142], Loss: 0.8216\n",
            "Epoch [15/20], Step [77/142], Loss: 0.7499\n",
            "Epoch [15/20], Step [78/142], Loss: 0.9086\n",
            "Epoch [15/20], Step [79/142], Loss: 0.8012\n",
            "Epoch [15/20], Step [80/142], Loss: 0.8632\n",
            "Epoch [15/20], Step [81/142], Loss: 0.8211\n",
            "Epoch [15/20], Step [82/142], Loss: 0.8506\n",
            "Epoch [15/20], Step [83/142], Loss: 0.9120\n",
            "Epoch [15/20], Step [84/142], Loss: 0.8551\n",
            "Epoch [15/20], Step [85/142], Loss: 0.9004\n",
            "Epoch [15/20], Step [86/142], Loss: 0.8393\n",
            "Epoch [15/20], Step [87/142], Loss: 0.8884\n",
            "Epoch [15/20], Step [88/142], Loss: 0.8714\n",
            "Epoch [15/20], Step [89/142], Loss: 0.8307\n",
            "Epoch [15/20], Step [90/142], Loss: 0.9479\n",
            "Epoch [15/20], Step [91/142], Loss: 0.8561\n",
            "Epoch [15/20], Step [92/142], Loss: 0.8497\n",
            "Epoch [15/20], Step [93/142], Loss: 0.8448\n",
            "Epoch [15/20], Step [94/142], Loss: 0.7422\n",
            "Epoch [15/20], Step [95/142], Loss: 1.0080\n",
            "Epoch [15/20], Step [96/142], Loss: 0.8486\n",
            "Epoch [15/20], Step [97/142], Loss: 0.7953\n",
            "Epoch [15/20], Step [98/142], Loss: 0.8106\n",
            "Epoch [15/20], Step [99/142], Loss: 0.8776\n",
            "Epoch [15/20], Step [100/142], Loss: 0.8480\n",
            "Epoch [15/20], Step [101/142], Loss: 0.8955\n",
            "Epoch [15/20], Step [102/142], Loss: 0.7810\n",
            "Epoch [15/20], Step [103/142], Loss: 0.8480\n",
            "Epoch [15/20], Step [104/142], Loss: 0.6953\n",
            "Epoch [15/20], Step [105/142], Loss: 0.9681\n",
            "Epoch [15/20], Step [106/142], Loss: 0.8081\n",
            "Epoch [15/20], Step [107/142], Loss: 0.8062\n",
            "Epoch [15/20], Step [108/142], Loss: 0.8743\n",
            "Epoch [15/20], Step [109/142], Loss: 0.7560\n",
            "Epoch [15/20], Step [110/142], Loss: 0.8392\n",
            "Epoch [15/20], Step [111/142], Loss: 0.8408\n",
            "Epoch [15/20], Step [112/142], Loss: 0.9444\n",
            "Epoch [15/20], Step [113/142], Loss: 0.7926\n",
            "Epoch [15/20], Step [114/142], Loss: 0.8442\n",
            "Epoch [15/20], Step [115/142], Loss: 0.8659\n",
            "Epoch [15/20], Step [116/142], Loss: 0.7866\n",
            "Epoch [15/20], Step [117/142], Loss: 0.8573\n",
            "Epoch [15/20], Step [118/142], Loss: 0.8726\n",
            "Epoch [15/20], Step [119/142], Loss: 0.8412\n",
            "Epoch [15/20], Step [120/142], Loss: 0.8126\n",
            "Epoch [15/20], Step [121/142], Loss: 0.7944\n",
            "Epoch [15/20], Step [122/142], Loss: 0.8968\n",
            "Epoch [15/20], Step [123/142], Loss: 0.8100\n",
            "Epoch [15/20], Step [124/142], Loss: 0.7731\n",
            "Epoch [15/20], Step [125/142], Loss: 0.9485\n",
            "Epoch [15/20], Step [126/142], Loss: 0.8642\n",
            "Epoch [15/20], Step [127/142], Loss: 0.7882\n",
            "Epoch [15/20], Step [128/142], Loss: 0.7830\n",
            "Epoch [15/20], Step [129/142], Loss: 0.8339\n",
            "Epoch [15/20], Step [130/142], Loss: 0.9367\n",
            "Epoch [15/20], Step [131/142], Loss: 0.8838\n",
            "Epoch [15/20], Step [132/142], Loss: 0.8192\n",
            "Epoch [15/20], Step [133/142], Loss: 0.8549\n",
            "Epoch [15/20], Step [134/142], Loss: 0.8486\n",
            "Epoch [15/20], Step [135/142], Loss: 0.8567\n",
            "Epoch [15/20], Step [136/142], Loss: 0.8242\n",
            "Epoch [15/20], Step [137/142], Loss: 0.8544\n",
            "Epoch [15/20], Step [138/142], Loss: 0.7724\n",
            "Epoch [15/20], Step [139/142], Loss: 0.8080\n",
            "Epoch [15/20], Step [140/142], Loss: 0.7949\n",
            "Epoch [15/20], Step [141/142], Loss: 0.7402\n",
            "Epoch [15/20], Step [142/142], Loss: 0.8227\n",
            "Epoch [16/20], Step [1/142], Loss: 0.7911\n",
            "Epoch [16/20], Step [2/142], Loss: 0.7106\n",
            "Epoch [16/20], Step [3/142], Loss: 0.8183\n",
            "Epoch [16/20], Step [4/142], Loss: 0.8413\n",
            "Epoch [16/20], Step [5/142], Loss: 0.8903\n",
            "Epoch [16/20], Step [6/142], Loss: 0.8674\n",
            "Epoch [16/20], Step [7/142], Loss: 0.7874\n",
            "Epoch [16/20], Step [8/142], Loss: 0.8818\n",
            "Epoch [16/20], Step [9/142], Loss: 0.7967\n",
            "Epoch [16/20], Step [10/142], Loss: 0.8910\n",
            "Epoch [16/20], Step [11/142], Loss: 0.8742\n",
            "Epoch [16/20], Step [12/142], Loss: 0.8813\n",
            "Epoch [16/20], Step [13/142], Loss: 0.8895\n",
            "Epoch [16/20], Step [14/142], Loss: 0.8747\n",
            "Epoch [16/20], Step [15/142], Loss: 0.8817\n",
            "Epoch [16/20], Step [16/142], Loss: 0.7749\n",
            "Epoch [16/20], Step [17/142], Loss: 0.7744\n",
            "Epoch [16/20], Step [18/142], Loss: 0.8473\n",
            "Epoch [16/20], Step [19/142], Loss: 0.8013\n",
            "Epoch [16/20], Step [20/142], Loss: 0.9176\n",
            "Epoch [16/20], Step [21/142], Loss: 0.8017\n",
            "Epoch [16/20], Step [22/142], Loss: 0.8108\n",
            "Epoch [16/20], Step [23/142], Loss: 0.7935\n",
            "Epoch [16/20], Step [24/142], Loss: 0.9708\n",
            "Epoch [16/20], Step [25/142], Loss: 0.7769\n",
            "Epoch [16/20], Step [26/142], Loss: 0.8298\n",
            "Epoch [16/20], Step [27/142], Loss: 0.9169\n",
            "Epoch [16/20], Step [28/142], Loss: 0.9916\n",
            "Epoch [16/20], Step [29/142], Loss: 0.8475\n",
            "Epoch [16/20], Step [30/142], Loss: 0.8106\n",
            "Epoch [16/20], Step [31/142], Loss: 0.8134\n",
            "Epoch [16/20], Step [32/142], Loss: 0.7923\n",
            "Epoch [16/20], Step [33/142], Loss: 0.9400\n",
            "Epoch [16/20], Step [34/142], Loss: 0.8340\n",
            "Epoch [16/20], Step [35/142], Loss: 0.8388\n",
            "Epoch [16/20], Step [36/142], Loss: 0.7129\n",
            "Epoch [16/20], Step [37/142], Loss: 0.7066\n",
            "Epoch [16/20], Step [38/142], Loss: 0.8911\n",
            "Epoch [16/20], Step [39/142], Loss: 0.8365\n",
            "Epoch [16/20], Step [40/142], Loss: 0.8373\n",
            "Epoch [16/20], Step [41/142], Loss: 0.7589\n",
            "Epoch [16/20], Step [42/142], Loss: 0.8153\n",
            "Epoch [16/20], Step [43/142], Loss: 0.9493\n",
            "Epoch [16/20], Step [44/142], Loss: 0.8751\n",
            "Epoch [16/20], Step [45/142], Loss: 0.8212\n",
            "Epoch [16/20], Step [46/142], Loss: 0.9059\n",
            "Epoch [16/20], Step [47/142], Loss: 0.8182\n",
            "Epoch [16/20], Step [48/142], Loss: 0.8050\n",
            "Epoch [16/20], Step [49/142], Loss: 0.7922\n",
            "Epoch [16/20], Step [50/142], Loss: 0.7794\n",
            "Epoch [16/20], Step [51/142], Loss: 0.7844\n",
            "Epoch [16/20], Step [52/142], Loss: 0.8017\n",
            "Epoch [16/20], Step [53/142], Loss: 0.6996\n",
            "Epoch [16/20], Step [54/142], Loss: 0.8715\n",
            "Epoch [16/20], Step [55/142], Loss: 0.8323\n",
            "Epoch [16/20], Step [56/142], Loss: 0.8481\n",
            "Epoch [16/20], Step [57/142], Loss: 0.9273\n",
            "Epoch [16/20], Step [58/142], Loss: 0.8823\n",
            "Epoch [16/20], Step [59/142], Loss: 0.8650\n",
            "Epoch [16/20], Step [60/142], Loss: 0.8686\n",
            "Epoch [16/20], Step [61/142], Loss: 0.8416\n",
            "Epoch [16/20], Step [62/142], Loss: 0.8903\n",
            "Epoch [16/20], Step [63/142], Loss: 0.7753\n",
            "Epoch [16/20], Step [64/142], Loss: 0.8551\n",
            "Epoch [16/20], Step [65/142], Loss: 0.8730\n",
            "Epoch [16/20], Step [66/142], Loss: 0.8737\n",
            "Epoch [16/20], Step [67/142], Loss: 0.8970\n",
            "Epoch [16/20], Step [68/142], Loss: 0.7795\n",
            "Epoch [16/20], Step [69/142], Loss: 0.8518\n",
            "Epoch [16/20], Step [70/142], Loss: 0.8032\n",
            "Epoch [16/20], Step [71/142], Loss: 0.9439\n",
            "Epoch [16/20], Step [72/142], Loss: 0.7525\n",
            "Epoch [16/20], Step [73/142], Loss: 0.8123\n",
            "Epoch [16/20], Step [74/142], Loss: 0.8273\n",
            "Epoch [16/20], Step [75/142], Loss: 0.9252\n",
            "Epoch [16/20], Step [76/142], Loss: 0.9129\n",
            "Epoch [16/20], Step [77/142], Loss: 0.8442\n",
            "Epoch [16/20], Step [78/142], Loss: 0.7970\n",
            "Epoch [16/20], Step [79/142], Loss: 0.9190\n",
            "Epoch [16/20], Step [80/142], Loss: 0.8034\n",
            "Epoch [16/20], Step [81/142], Loss: 0.9169\n",
            "Epoch [16/20], Step [82/142], Loss: 0.7369\n",
            "Epoch [16/20], Step [83/142], Loss: 0.8146\n",
            "Epoch [16/20], Step [84/142], Loss: 0.8663\n",
            "Epoch [16/20], Step [85/142], Loss: 0.8961\n",
            "Epoch [16/20], Step [86/142], Loss: 0.9213\n",
            "Epoch [16/20], Step [87/142], Loss: 0.7990\n",
            "Epoch [16/20], Step [88/142], Loss: 0.8104\n",
            "Epoch [16/20], Step [89/142], Loss: 0.8217\n",
            "Epoch [16/20], Step [90/142], Loss: 0.7935\n",
            "Epoch [16/20], Step [91/142], Loss: 0.8336\n",
            "Epoch [16/20], Step [92/142], Loss: 0.7254\n",
            "Epoch [16/20], Step [93/142], Loss: 0.8204\n",
            "Epoch [16/20], Step [94/142], Loss: 0.7890\n",
            "Epoch [16/20], Step [95/142], Loss: 0.8307\n",
            "Epoch [16/20], Step [96/142], Loss: 0.8517\n",
            "Epoch [16/20], Step [97/142], Loss: 0.8315\n",
            "Epoch [16/20], Step [98/142], Loss: 0.8939\n",
            "Epoch [16/20], Step [99/142], Loss: 0.8820\n",
            "Epoch [16/20], Step [100/142], Loss: 0.8749\n",
            "Epoch [16/20], Step [101/142], Loss: 0.8102\n",
            "Epoch [16/20], Step [102/142], Loss: 0.8959\n",
            "Epoch [16/20], Step [103/142], Loss: 0.9129\n",
            "Epoch [16/20], Step [104/142], Loss: 0.8616\n",
            "Epoch [16/20], Step [105/142], Loss: 0.8551\n",
            "Epoch [16/20], Step [106/142], Loss: 0.8171\n",
            "Epoch [16/20], Step [107/142], Loss: 0.7432\n",
            "Epoch [16/20], Step [108/142], Loss: 0.8543\n",
            "Epoch [16/20], Step [109/142], Loss: 0.8515\n",
            "Epoch [16/20], Step [110/142], Loss: 0.7753\n",
            "Epoch [16/20], Step [111/142], Loss: 0.9582\n",
            "Epoch [16/20], Step [112/142], Loss: 0.7873\n",
            "Epoch [16/20], Step [113/142], Loss: 0.8077\n",
            "Epoch [16/20], Step [114/142], Loss: 0.8813\n",
            "Epoch [16/20], Step [115/142], Loss: 0.8915\n",
            "Epoch [16/20], Step [116/142], Loss: 0.8620\n",
            "Epoch [16/20], Step [117/142], Loss: 0.8391\n",
            "Epoch [16/20], Step [118/142], Loss: 0.8975\n",
            "Epoch [16/20], Step [119/142], Loss: 0.8175\n",
            "Epoch [16/20], Step [120/142], Loss: 0.8371\n",
            "Epoch [16/20], Step [121/142], Loss: 0.8237\n",
            "Epoch [16/20], Step [122/142], Loss: 0.8139\n",
            "Epoch [16/20], Step [123/142], Loss: 0.8791\n",
            "Epoch [16/20], Step [124/142], Loss: 0.7827\n",
            "Epoch [16/20], Step [125/142], Loss: 0.9555\n",
            "Epoch [16/20], Step [126/142], Loss: 0.9131\n",
            "Epoch [16/20], Step [127/142], Loss: 0.9240\n",
            "Epoch [16/20], Step [128/142], Loss: 0.8013\n",
            "Epoch [16/20], Step [129/142], Loss: 0.8649\n",
            "Epoch [16/20], Step [130/142], Loss: 0.8270\n",
            "Epoch [16/20], Step [131/142], Loss: 0.8567\n",
            "Epoch [16/20], Step [132/142], Loss: 0.6881\n",
            "Epoch [16/20], Step [133/142], Loss: 0.7796\n",
            "Epoch [16/20], Step [134/142], Loss: 0.8772\n",
            "Epoch [16/20], Step [135/142], Loss: 0.9145\n",
            "Epoch [16/20], Step [136/142], Loss: 0.8431\n",
            "Epoch [16/20], Step [137/142], Loss: 0.8145\n",
            "Epoch [16/20], Step [138/142], Loss: 0.7851\n",
            "Epoch [16/20], Step [139/142], Loss: 0.8107\n",
            "Epoch [16/20], Step [140/142], Loss: 0.8873\n",
            "Epoch [16/20], Step [141/142], Loss: 0.7743\n",
            "Epoch [16/20], Step [142/142], Loss: 0.6889\n",
            "Epoch [17/20], Step [1/142], Loss: 0.8649\n",
            "Epoch [17/20], Step [2/142], Loss: 0.8981\n",
            "Epoch [17/20], Step [3/142], Loss: 0.9455\n",
            "Epoch [17/20], Step [4/142], Loss: 0.7918\n",
            "Epoch [17/20], Step [5/142], Loss: 0.7988\n",
            "Epoch [17/20], Step [6/142], Loss: 0.7930\n",
            "Epoch [17/20], Step [7/142], Loss: 0.8041\n",
            "Epoch [17/20], Step [8/142], Loss: 0.9066\n",
            "Epoch [17/20], Step [9/142], Loss: 0.7922\n",
            "Epoch [17/20], Step [10/142], Loss: 0.8399\n",
            "Epoch [17/20], Step [11/142], Loss: 0.7790\n",
            "Epoch [17/20], Step [12/142], Loss: 0.8030\n",
            "Epoch [17/20], Step [13/142], Loss: 0.9506\n",
            "Epoch [17/20], Step [14/142], Loss: 0.9431\n",
            "Epoch [17/20], Step [15/142], Loss: 0.9388\n",
            "Epoch [17/20], Step [16/142], Loss: 0.9651\n",
            "Epoch [17/20], Step [17/142], Loss: 0.7788\n",
            "Epoch [17/20], Step [18/142], Loss: 0.8815\n",
            "Epoch [17/20], Step [19/142], Loss: 0.8527\n",
            "Epoch [17/20], Step [20/142], Loss: 0.9121\n",
            "Epoch [17/20], Step [21/142], Loss: 0.7907\n",
            "Epoch [17/20], Step [22/142], Loss: 0.8199\n",
            "Epoch [17/20], Step [23/142], Loss: 0.8677\n",
            "Epoch [17/20], Step [24/142], Loss: 0.7835\n",
            "Epoch [17/20], Step [25/142], Loss: 0.7990\n",
            "Epoch [17/20], Step [26/142], Loss: 0.8136\n",
            "Epoch [17/20], Step [27/142], Loss: 0.7325\n",
            "Epoch [17/20], Step [28/142], Loss: 0.8268\n",
            "Epoch [17/20], Step [29/142], Loss: 0.8088\n",
            "Epoch [17/20], Step [30/142], Loss: 0.9095\n",
            "Epoch [17/20], Step [31/142], Loss: 0.8035\n",
            "Epoch [17/20], Step [32/142], Loss: 0.8581\n",
            "Epoch [17/20], Step [33/142], Loss: 0.8926\n",
            "Epoch [17/20], Step [34/142], Loss: 0.8666\n",
            "Epoch [17/20], Step [35/142], Loss: 0.8002\n",
            "Epoch [17/20], Step [36/142], Loss: 0.8292\n",
            "Epoch [17/20], Step [37/142], Loss: 0.9368\n",
            "Epoch [17/20], Step [38/142], Loss: 0.8862\n",
            "Epoch [17/20], Step [39/142], Loss: 0.8748\n",
            "Epoch [17/20], Step [40/142], Loss: 0.7668\n",
            "Epoch [17/20], Step [41/142], Loss: 0.8151\n",
            "Epoch [17/20], Step [42/142], Loss: 0.9003\n",
            "Epoch [17/20], Step [43/142], Loss: 0.7865\n",
            "Epoch [17/20], Step [44/142], Loss: 0.8402\n",
            "Epoch [17/20], Step [45/142], Loss: 0.9062\n",
            "Epoch [17/20], Step [46/142], Loss: 0.8112\n",
            "Epoch [17/20], Step [47/142], Loss: 0.8410\n",
            "Epoch [17/20], Step [48/142], Loss: 0.7956\n",
            "Epoch [17/20], Step [49/142], Loss: 0.7412\n",
            "Epoch [17/20], Step [50/142], Loss: 0.7823\n",
            "Epoch [17/20], Step [51/142], Loss: 0.8442\n",
            "Epoch [17/20], Step [52/142], Loss: 0.8350\n",
            "Epoch [17/20], Step [53/142], Loss: 0.8564\n",
            "Epoch [17/20], Step [54/142], Loss: 0.7821\n",
            "Epoch [17/20], Step [55/142], Loss: 0.8104\n",
            "Epoch [17/20], Step [56/142], Loss: 0.9519\n",
            "Epoch [17/20], Step [57/142], Loss: 0.7367\n",
            "Epoch [17/20], Step [58/142], Loss: 0.7389\n",
            "Epoch [17/20], Step [59/142], Loss: 0.8565\n",
            "Epoch [17/20], Step [60/142], Loss: 0.8742\n",
            "Epoch [17/20], Step [61/142], Loss: 0.7709\n",
            "Epoch [17/20], Step [62/142], Loss: 0.7726\n",
            "Epoch [17/20], Step [63/142], Loss: 0.7752\n",
            "Epoch [17/20], Step [64/142], Loss: 0.8765\n",
            "Epoch [17/20], Step [65/142], Loss: 0.7410\n",
            "Epoch [17/20], Step [66/142], Loss: 0.8934\n",
            "Epoch [17/20], Step [67/142], Loss: 0.8052\n",
            "Epoch [17/20], Step [68/142], Loss: 0.8473\n",
            "Epoch [17/20], Step [69/142], Loss: 0.7229\n",
            "Epoch [17/20], Step [70/142], Loss: 0.9130\n",
            "Epoch [17/20], Step [71/142], Loss: 0.8477\n",
            "Epoch [17/20], Step [72/142], Loss: 0.8497\n",
            "Epoch [17/20], Step [73/142], Loss: 0.8510\n",
            "Epoch [17/20], Step [74/142], Loss: 0.8465\n",
            "Epoch [17/20], Step [75/142], Loss: 0.8117\n",
            "Epoch [17/20], Step [76/142], Loss: 0.8320\n",
            "Epoch [17/20], Step [77/142], Loss: 0.7502\n",
            "Epoch [17/20], Step [78/142], Loss: 0.8023\n",
            "Epoch [17/20], Step [79/142], Loss: 0.8282\n",
            "Epoch [17/20], Step [80/142], Loss: 0.9388\n",
            "Epoch [17/20], Step [81/142], Loss: 0.8060\n",
            "Epoch [17/20], Step [82/142], Loss: 0.8888\n",
            "Epoch [17/20], Step [83/142], Loss: 0.8829\n",
            "Epoch [17/20], Step [84/142], Loss: 0.8455\n",
            "Epoch [17/20], Step [85/142], Loss: 0.8312\n",
            "Epoch [17/20], Step [86/142], Loss: 0.8367\n",
            "Epoch [17/20], Step [87/142], Loss: 0.8397\n",
            "Epoch [17/20], Step [88/142], Loss: 0.7962\n",
            "Epoch [17/20], Step [89/142], Loss: 0.7882\n",
            "Epoch [17/20], Step [90/142], Loss: 0.8536\n",
            "Epoch [17/20], Step [91/142], Loss: 0.7985\n",
            "Epoch [17/20], Step [92/142], Loss: 0.8522\n",
            "Epoch [17/20], Step [93/142], Loss: 0.8941\n",
            "Epoch [17/20], Step [94/142], Loss: 0.8984\n",
            "Epoch [17/20], Step [95/142], Loss: 0.9008\n",
            "Epoch [17/20], Step [96/142], Loss: 0.8249\n",
            "Epoch [17/20], Step [97/142], Loss: 0.8127\n",
            "Epoch [17/20], Step [98/142], Loss: 0.8733\n",
            "Epoch [17/20], Step [99/142], Loss: 0.8885\n",
            "Epoch [17/20], Step [100/142], Loss: 0.7817\n",
            "Epoch [17/20], Step [101/142], Loss: 0.8712\n",
            "Epoch [17/20], Step [102/142], Loss: 0.9406\n",
            "Epoch [17/20], Step [103/142], Loss: 0.8556\n",
            "Epoch [17/20], Step [104/142], Loss: 0.7906\n",
            "Epoch [17/20], Step [105/142], Loss: 0.7519\n",
            "Epoch [17/20], Step [106/142], Loss: 0.8132\n",
            "Epoch [17/20], Step [107/142], Loss: 0.8442\n",
            "Epoch [17/20], Step [108/142], Loss: 0.9013\n",
            "Epoch [17/20], Step [109/142], Loss: 0.8209\n",
            "Epoch [17/20], Step [110/142], Loss: 0.8772\n",
            "Epoch [17/20], Step [111/142], Loss: 0.9475\n",
            "Epoch [17/20], Step [112/142], Loss: 0.7712\n",
            "Epoch [17/20], Step [113/142], Loss: 0.8441\n",
            "Epoch [17/20], Step [114/142], Loss: 0.8055\n",
            "Epoch [17/20], Step [115/142], Loss: 0.8208\n",
            "Epoch [17/20], Step [116/142], Loss: 0.8092\n",
            "Epoch [17/20], Step [117/142], Loss: 0.8310\n",
            "Epoch [17/20], Step [118/142], Loss: 0.8655\n",
            "Epoch [17/20], Step [119/142], Loss: 0.8392\n",
            "Epoch [17/20], Step [120/142], Loss: 0.8025\n",
            "Epoch [17/20], Step [121/142], Loss: 0.7608\n",
            "Epoch [17/20], Step [122/142], Loss: 0.8391\n",
            "Epoch [17/20], Step [123/142], Loss: 0.8717\n",
            "Epoch [17/20], Step [124/142], Loss: 0.8799\n",
            "Epoch [17/20], Step [125/142], Loss: 0.8669\n",
            "Epoch [17/20], Step [126/142], Loss: 0.8514\n",
            "Epoch [17/20], Step [127/142], Loss: 0.8212\n",
            "Epoch [17/20], Step [128/142], Loss: 0.9364\n",
            "Epoch [17/20], Step [129/142], Loss: 0.8471\n",
            "Epoch [17/20], Step [130/142], Loss: 0.8794\n",
            "Epoch [17/20], Step [131/142], Loss: 0.8587\n",
            "Epoch [17/20], Step [132/142], Loss: 0.8478\n",
            "Epoch [17/20], Step [133/142], Loss: 0.9247\n",
            "Epoch [17/20], Step [134/142], Loss: 0.7909\n",
            "Epoch [17/20], Step [135/142], Loss: 0.7843\n",
            "Epoch [17/20], Step [136/142], Loss: 0.9050\n",
            "Epoch [17/20], Step [137/142], Loss: 0.8016\n",
            "Epoch [17/20], Step [138/142], Loss: 0.7814\n",
            "Epoch [17/20], Step [139/142], Loss: 0.8622\n",
            "Epoch [17/20], Step [140/142], Loss: 0.8385\n",
            "Epoch [17/20], Step [141/142], Loss: 0.8935\n",
            "Epoch [17/20], Step [142/142], Loss: 0.8363\n",
            "Epoch [18/20], Step [1/142], Loss: 0.8309\n",
            "Epoch [18/20], Step [2/142], Loss: 0.8266\n",
            "Epoch [18/20], Step [3/142], Loss: 0.9320\n",
            "Epoch [18/20], Step [4/142], Loss: 0.7955\n",
            "Epoch [18/20], Step [5/142], Loss: 0.8872\n",
            "Epoch [18/20], Step [6/142], Loss: 0.9233\n",
            "Epoch [18/20], Step [7/142], Loss: 0.7899\n",
            "Epoch [18/20], Step [8/142], Loss: 0.8689\n",
            "Epoch [18/20], Step [9/142], Loss: 0.8392\n",
            "Epoch [18/20], Step [10/142], Loss: 0.8499\n",
            "Epoch [18/20], Step [11/142], Loss: 0.7922\n",
            "Epoch [18/20], Step [12/142], Loss: 0.7684\n",
            "Epoch [18/20], Step [13/142], Loss: 0.8300\n",
            "Epoch [18/20], Step [14/142], Loss: 0.8656\n",
            "Epoch [18/20], Step [15/142], Loss: 0.8681\n",
            "Epoch [18/20], Step [16/142], Loss: 0.7752\n",
            "Epoch [18/20], Step [17/142], Loss: 0.8631\n",
            "Epoch [18/20], Step [18/142], Loss: 0.9544\n",
            "Epoch [18/20], Step [19/142], Loss: 0.8120\n",
            "Epoch [18/20], Step [20/142], Loss: 0.7780\n",
            "Epoch [18/20], Step [21/142], Loss: 0.8734\n",
            "Epoch [18/20], Step [22/142], Loss: 0.8373\n",
            "Epoch [18/20], Step [23/142], Loss: 0.8073\n",
            "Epoch [18/20], Step [24/142], Loss: 0.8502\n",
            "Epoch [18/20], Step [25/142], Loss: 0.8753\n",
            "Epoch [18/20], Step [26/142], Loss: 0.8944\n",
            "Epoch [18/20], Step [27/142], Loss: 0.8298\n",
            "Epoch [18/20], Step [28/142], Loss: 0.8559\n",
            "Epoch [18/20], Step [29/142], Loss: 0.8308\n",
            "Epoch [18/20], Step [30/142], Loss: 0.8136\n",
            "Epoch [18/20], Step [31/142], Loss: 0.7805\n",
            "Epoch [18/20], Step [32/142], Loss: 0.8489\n",
            "Epoch [18/20], Step [33/142], Loss: 0.8529\n",
            "Epoch [18/20], Step [34/142], Loss: 0.8114\n",
            "Epoch [18/20], Step [35/142], Loss: 0.9233\n",
            "Epoch [18/20], Step [36/142], Loss: 0.8205\n",
            "Epoch [18/20], Step [37/142], Loss: 0.8667\n",
            "Epoch [18/20], Step [38/142], Loss: 0.8389\n",
            "Epoch [18/20], Step [39/142], Loss: 0.7967\n",
            "Epoch [18/20], Step [40/142], Loss: 0.7877\n",
            "Epoch [18/20], Step [41/142], Loss: 0.8375\n",
            "Epoch [18/20], Step [42/142], Loss: 0.8673\n",
            "Epoch [18/20], Step [43/142], Loss: 0.7546\n",
            "Epoch [18/20], Step [44/142], Loss: 0.8803\n",
            "Epoch [18/20], Step [45/142], Loss: 0.8922\n",
            "Epoch [18/20], Step [46/142], Loss: 0.8579\n",
            "Epoch [18/20], Step [47/142], Loss: 0.8825\n",
            "Epoch [18/20], Step [48/142], Loss: 0.8365\n",
            "Epoch [18/20], Step [49/142], Loss: 0.8196\n",
            "Epoch [18/20], Step [50/142], Loss: 0.8363\n",
            "Epoch [18/20], Step [51/142], Loss: 0.8332\n",
            "Epoch [18/20], Step [52/142], Loss: 0.8282\n",
            "Epoch [18/20], Step [53/142], Loss: 0.8016\n",
            "Epoch [18/20], Step [54/142], Loss: 0.7521\n",
            "Epoch [18/20], Step [55/142], Loss: 0.8968\n",
            "Epoch [18/20], Step [56/142], Loss: 0.7457\n",
            "Epoch [18/20], Step [57/142], Loss: 0.8055\n",
            "Epoch [18/20], Step [58/142], Loss: 0.8170\n",
            "Epoch [18/20], Step [59/142], Loss: 0.8211\n",
            "Epoch [18/20], Step [60/142], Loss: 0.8669\n",
            "Epoch [18/20], Step [61/142], Loss: 0.7593\n",
            "Epoch [18/20], Step [62/142], Loss: 0.7901\n",
            "Epoch [18/20], Step [63/142], Loss: 0.8410\n",
            "Epoch [18/20], Step [64/142], Loss: 0.8034\n",
            "Epoch [18/20], Step [65/142], Loss: 0.8734\n",
            "Epoch [18/20], Step [66/142], Loss: 0.9462\n",
            "Epoch [18/20], Step [67/142], Loss: 0.8220\n",
            "Epoch [18/20], Step [68/142], Loss: 1.0544\n",
            "Epoch [18/20], Step [69/142], Loss: 0.7992\n",
            "Epoch [18/20], Step [70/142], Loss: 0.8199\n",
            "Epoch [18/20], Step [71/142], Loss: 0.8196\n",
            "Epoch [18/20], Step [72/142], Loss: 0.8391\n",
            "Epoch [18/20], Step [73/142], Loss: 0.7626\n",
            "Epoch [18/20], Step [74/142], Loss: 0.9427\n",
            "Epoch [18/20], Step [75/142], Loss: 0.7683\n",
            "Epoch [18/20], Step [76/142], Loss: 0.8215\n",
            "Epoch [18/20], Step [77/142], Loss: 0.7404\n",
            "Epoch [18/20], Step [78/142], Loss: 0.9101\n",
            "Epoch [18/20], Step [79/142], Loss: 0.8174\n",
            "Epoch [18/20], Step [80/142], Loss: 0.9131\n",
            "Epoch [18/20], Step [81/142], Loss: 0.7457\n",
            "Epoch [18/20], Step [82/142], Loss: 0.7847\n",
            "Epoch [18/20], Step [83/142], Loss: 0.8893\n",
            "Epoch [18/20], Step [84/142], Loss: 0.8067\n",
            "Epoch [18/20], Step [85/142], Loss: 0.9000\n",
            "Epoch [18/20], Step [86/142], Loss: 0.8137\n",
            "Epoch [18/20], Step [87/142], Loss: 0.8467\n",
            "Epoch [18/20], Step [88/142], Loss: 0.8070\n",
            "Epoch [18/20], Step [89/142], Loss: 0.8577\n",
            "Epoch [18/20], Step [90/142], Loss: 0.8248\n",
            "Epoch [18/20], Step [91/142], Loss: 0.8247\n",
            "Epoch [18/20], Step [92/142], Loss: 0.8525\n",
            "Epoch [18/20], Step [93/142], Loss: 0.8437\n",
            "Epoch [18/20], Step [94/142], Loss: 0.8065\n",
            "Epoch [18/20], Step [95/142], Loss: 0.8598\n",
            "Epoch [18/20], Step [96/142], Loss: 0.8540\n",
            "Epoch [18/20], Step [97/142], Loss: 0.7640\n",
            "Epoch [18/20], Step [98/142], Loss: 0.8741\n",
            "Epoch [18/20], Step [99/142], Loss: 0.8018\n",
            "Epoch [18/20], Step [100/142], Loss: 0.8485\n",
            "Epoch [18/20], Step [101/142], Loss: 0.8561\n",
            "Epoch [18/20], Step [102/142], Loss: 0.8622\n",
            "Epoch [18/20], Step [103/142], Loss: 0.8684\n",
            "Epoch [18/20], Step [104/142], Loss: 0.7924\n",
            "Epoch [18/20], Step [105/142], Loss: 0.8129\n",
            "Epoch [18/20], Step [106/142], Loss: 0.9321\n",
            "Epoch [18/20], Step [107/142], Loss: 0.7664\n",
            "Epoch [18/20], Step [108/142], Loss: 0.8694\n",
            "Epoch [18/20], Step [109/142], Loss: 0.8707\n",
            "Epoch [18/20], Step [110/142], Loss: 0.9375\n",
            "Epoch [18/20], Step [111/142], Loss: 0.8011\n",
            "Epoch [18/20], Step [112/142], Loss: 0.8326\n",
            "Epoch [18/20], Step [113/142], Loss: 0.7943\n",
            "Epoch [18/20], Step [114/142], Loss: 0.8143\n",
            "Epoch [18/20], Step [115/142], Loss: 0.8499\n",
            "Epoch [18/20], Step [116/142], Loss: 0.8171\n",
            "Epoch [18/20], Step [117/142], Loss: 0.8015\n",
            "Epoch [18/20], Step [118/142], Loss: 0.7429\n",
            "Epoch [18/20], Step [119/142], Loss: 0.7744\n",
            "Epoch [18/20], Step [120/142], Loss: 0.8330\n",
            "Epoch [18/20], Step [121/142], Loss: 0.7766\n",
            "Epoch [18/20], Step [122/142], Loss: 0.7781\n",
            "Epoch [18/20], Step [123/142], Loss: 0.8739\n",
            "Epoch [18/20], Step [124/142], Loss: 0.8714\n",
            "Epoch [18/20], Step [125/142], Loss: 0.8828\n",
            "Epoch [18/20], Step [126/142], Loss: 0.8953\n",
            "Epoch [18/20], Step [127/142], Loss: 0.9002\n",
            "Epoch [18/20], Step [128/142], Loss: 0.8742\n",
            "Epoch [18/20], Step [129/142], Loss: 0.8299\n",
            "Epoch [18/20], Step [130/142], Loss: 0.8128\n",
            "Epoch [18/20], Step [131/142], Loss: 0.8255\n",
            "Epoch [18/20], Step [132/142], Loss: 0.8570\n",
            "Epoch [18/20], Step [133/142], Loss: 0.8371\n",
            "Epoch [18/20], Step [134/142], Loss: 0.8283\n",
            "Epoch [18/20], Step [135/142], Loss: 0.7917\n",
            "Epoch [18/20], Step [136/142], Loss: 0.8228\n",
            "Epoch [18/20], Step [137/142], Loss: 0.8514\n",
            "Epoch [18/20], Step [138/142], Loss: 0.8756\n",
            "Epoch [18/20], Step [139/142], Loss: 0.9143\n",
            "Epoch [18/20], Step [140/142], Loss: 0.9039\n",
            "Epoch [18/20], Step [141/142], Loss: 0.9095\n",
            "Epoch [18/20], Step [142/142], Loss: 0.8279\n",
            "Epoch [19/20], Step [1/142], Loss: 0.9056\n",
            "Epoch [19/20], Step [2/142], Loss: 0.7537\n",
            "Epoch [19/20], Step [3/142], Loss: 0.7898\n",
            "Epoch [19/20], Step [4/142], Loss: 0.7691\n",
            "Epoch [19/20], Step [5/142], Loss: 0.8880\n",
            "Epoch [19/20], Step [6/142], Loss: 0.8087\n",
            "Epoch [19/20], Step [7/142], Loss: 0.8770\n",
            "Epoch [19/20], Step [8/142], Loss: 0.8064\n",
            "Epoch [19/20], Step [9/142], Loss: 0.7983\n",
            "Epoch [19/20], Step [10/142], Loss: 0.8208\n",
            "Epoch [19/20], Step [11/142], Loss: 0.8265\n",
            "Epoch [19/20], Step [12/142], Loss: 0.8702\n",
            "Epoch [19/20], Step [13/142], Loss: 0.8459\n",
            "Epoch [19/20], Step [14/142], Loss: 0.8464\n",
            "Epoch [19/20], Step [15/142], Loss: 0.8262\n",
            "Epoch [19/20], Step [16/142], Loss: 0.9285\n",
            "Epoch [19/20], Step [17/142], Loss: 0.8126\n",
            "Epoch [19/20], Step [18/142], Loss: 0.8221\n",
            "Epoch [19/20], Step [19/142], Loss: 0.8352\n",
            "Epoch [19/20], Step [20/142], Loss: 0.8180\n",
            "Epoch [19/20], Step [21/142], Loss: 0.7765\n",
            "Epoch [19/20], Step [22/142], Loss: 0.9639\n",
            "Epoch [19/20], Step [23/142], Loss: 0.8276\n",
            "Epoch [19/20], Step [24/142], Loss: 0.7613\n",
            "Epoch [19/20], Step [25/142], Loss: 0.8030\n",
            "Epoch [19/20], Step [26/142], Loss: 0.8493\n",
            "Epoch [19/20], Step [27/142], Loss: 0.8354\n",
            "Epoch [19/20], Step [28/142], Loss: 0.8862\n",
            "Epoch [19/20], Step [29/142], Loss: 0.8363\n",
            "Epoch [19/20], Step [30/142], Loss: 0.8260\n",
            "Epoch [19/20], Step [31/142], Loss: 0.8743\n",
            "Epoch [19/20], Step [32/142], Loss: 0.8631\n",
            "Epoch [19/20], Step [33/142], Loss: 0.8643\n",
            "Epoch [19/20], Step [34/142], Loss: 0.7731\n",
            "Epoch [19/20], Step [35/142], Loss: 0.8503\n",
            "Epoch [19/20], Step [36/142], Loss: 0.8584\n",
            "Epoch [19/20], Step [37/142], Loss: 0.8390\n",
            "Epoch [19/20], Step [38/142], Loss: 0.7563\n",
            "Epoch [19/20], Step [39/142], Loss: 0.7983\n",
            "Epoch [19/20], Step [40/142], Loss: 0.7708\n",
            "Epoch [19/20], Step [41/142], Loss: 0.9449\n",
            "Epoch [19/20], Step [42/142], Loss: 0.7909\n",
            "Epoch [19/20], Step [43/142], Loss: 0.8775\n",
            "Epoch [19/20], Step [44/142], Loss: 0.8553\n",
            "Epoch [19/20], Step [45/142], Loss: 0.8650\n",
            "Epoch [19/20], Step [46/142], Loss: 0.8168\n",
            "Epoch [19/20], Step [47/142], Loss: 0.8723\n",
            "Epoch [19/20], Step [48/142], Loss: 0.9445\n",
            "Epoch [19/20], Step [49/142], Loss: 0.7448\n",
            "Epoch [19/20], Step [50/142], Loss: 0.8546\n",
            "Epoch [19/20], Step [51/142], Loss: 0.8474\n",
            "Epoch [19/20], Step [52/142], Loss: 0.8735\n",
            "Epoch [19/20], Step [53/142], Loss: 0.8179\n",
            "Epoch [19/20], Step [54/142], Loss: 0.8188\n",
            "Epoch [19/20], Step [55/142], Loss: 0.8391\n",
            "Epoch [19/20], Step [56/142], Loss: 0.8503\n",
            "Epoch [19/20], Step [57/142], Loss: 0.7958\n",
            "Epoch [19/20], Step [58/142], Loss: 0.7042\n",
            "Epoch [19/20], Step [59/142], Loss: 0.7982\n",
            "Epoch [19/20], Step [60/142], Loss: 0.8806\n",
            "Epoch [19/20], Step [61/142], Loss: 0.8628\n",
            "Epoch [19/20], Step [62/142], Loss: 0.8205\n",
            "Epoch [19/20], Step [63/142], Loss: 0.8796\n",
            "Epoch [19/20], Step [64/142], Loss: 0.8850\n",
            "Epoch [19/20], Step [65/142], Loss: 0.7786\n",
            "Epoch [19/20], Step [66/142], Loss: 0.8126\n",
            "Epoch [19/20], Step [67/142], Loss: 0.8196\n",
            "Epoch [19/20], Step [68/142], Loss: 0.8981\n",
            "Epoch [19/20], Step [69/142], Loss: 0.9008\n",
            "Epoch [19/20], Step [70/142], Loss: 0.9120\n",
            "Epoch [19/20], Step [71/142], Loss: 0.7842\n",
            "Epoch [19/20], Step [72/142], Loss: 0.8714\n",
            "Epoch [19/20], Step [73/142], Loss: 0.8458\n",
            "Epoch [19/20], Step [74/142], Loss: 0.8749\n",
            "Epoch [19/20], Step [75/142], Loss: 0.8414\n",
            "Epoch [19/20], Step [76/142], Loss: 0.8692\n",
            "Epoch [19/20], Step [77/142], Loss: 0.7613\n",
            "Epoch [19/20], Step [78/142], Loss: 0.7982\n",
            "Epoch [19/20], Step [79/142], Loss: 0.9474\n",
            "Epoch [19/20], Step [80/142], Loss: 0.7827\n",
            "Epoch [19/20], Step [81/142], Loss: 0.9106\n",
            "Epoch [19/20], Step [82/142], Loss: 0.7634\n",
            "Epoch [19/20], Step [83/142], Loss: 0.7970\n",
            "Epoch [19/20], Step [84/142], Loss: 0.8078\n",
            "Epoch [19/20], Step [85/142], Loss: 0.8494\n",
            "Epoch [19/20], Step [86/142], Loss: 1.0020\n",
            "Epoch [19/20], Step [87/142], Loss: 0.7863\n",
            "Epoch [19/20], Step [88/142], Loss: 0.9196\n",
            "Epoch [19/20], Step [89/142], Loss: 0.7881\n",
            "Epoch [19/20], Step [90/142], Loss: 0.8080\n",
            "Epoch [19/20], Step [91/142], Loss: 0.8989\n",
            "Epoch [19/20], Step [92/142], Loss: 0.8846\n",
            "Epoch [19/20], Step [93/142], Loss: 0.8923\n",
            "Epoch [19/20], Step [94/142], Loss: 0.8705\n",
            "Epoch [19/20], Step [95/142], Loss: 0.9517\n",
            "Epoch [19/20], Step [96/142], Loss: 0.9054\n",
            "Epoch [19/20], Step [97/142], Loss: 0.8717\n",
            "Epoch [19/20], Step [98/142], Loss: 0.8555\n",
            "Epoch [19/20], Step [99/142], Loss: 0.7958\n",
            "Epoch [19/20], Step [100/142], Loss: 0.8509\n",
            "Epoch [19/20], Step [101/142], Loss: 0.8440\n",
            "Epoch [19/20], Step [102/142], Loss: 0.7790\n",
            "Epoch [19/20], Step [103/142], Loss: 0.8482\n",
            "Epoch [19/20], Step [104/142], Loss: 0.8511\n",
            "Epoch [19/20], Step [105/142], Loss: 0.8023\n",
            "Epoch [19/20], Step [106/142], Loss: 0.8330\n",
            "Epoch [19/20], Step [107/142], Loss: 0.8969\n",
            "Epoch [19/20], Step [108/142], Loss: 0.8560\n",
            "Epoch [19/20], Step [109/142], Loss: 0.8637\n",
            "Epoch [19/20], Step [110/142], Loss: 0.8001\n",
            "Epoch [19/20], Step [111/142], Loss: 0.8442\n",
            "Epoch [19/20], Step [112/142], Loss: 0.8186\n",
            "Epoch [19/20], Step [113/142], Loss: 0.8557\n",
            "Epoch [19/20], Step [114/142], Loss: 0.7026\n",
            "Epoch [19/20], Step [115/142], Loss: 0.8908\n",
            "Epoch [19/20], Step [116/142], Loss: 0.8674\n",
            "Epoch [19/20], Step [117/142], Loss: 0.8291\n",
            "Epoch [19/20], Step [118/142], Loss: 0.9270\n",
            "Epoch [19/20], Step [119/142], Loss: 0.7580\n",
            "Epoch [19/20], Step [120/142], Loss: 0.7964\n",
            "Epoch [19/20], Step [121/142], Loss: 0.7991\n",
            "Epoch [19/20], Step [122/142], Loss: 0.7601\n",
            "Epoch [19/20], Step [123/142], Loss: 0.7925\n",
            "Epoch [19/20], Step [124/142], Loss: 0.7782\n",
            "Epoch [19/20], Step [125/142], Loss: 0.8023\n",
            "Epoch [19/20], Step [126/142], Loss: 0.8203\n",
            "Epoch [19/20], Step [127/142], Loss: 0.8175\n",
            "Epoch [19/20], Step [128/142], Loss: 0.9602\n",
            "Epoch [19/20], Step [129/142], Loss: 0.8667\n",
            "Epoch [19/20], Step [130/142], Loss: 0.8093\n",
            "Epoch [19/20], Step [131/142], Loss: 0.8278\n",
            "Epoch [19/20], Step [132/142], Loss: 0.7519\n",
            "Epoch [19/20], Step [133/142], Loss: 0.8016\n",
            "Epoch [19/20], Step [134/142], Loss: 0.7640\n",
            "Epoch [19/20], Step [135/142], Loss: 0.7056\n",
            "Epoch [19/20], Step [136/142], Loss: 0.9643\n",
            "Epoch [19/20], Step [137/142], Loss: 0.8899\n",
            "Epoch [19/20], Step [138/142], Loss: 0.8868\n",
            "Epoch [19/20], Step [139/142], Loss: 0.9341\n",
            "Epoch [19/20], Step [140/142], Loss: 0.7848\n",
            "Epoch [19/20], Step [141/142], Loss: 0.8072\n",
            "Epoch [19/20], Step [142/142], Loss: 0.8437\n",
            "Epoch [20/20], Step [1/142], Loss: 0.7021\n",
            "Epoch [20/20], Step [2/142], Loss: 0.8239\n",
            "Epoch [20/20], Step [3/142], Loss: 0.7723\n",
            "Epoch [20/20], Step [4/142], Loss: 0.7715\n",
            "Epoch [20/20], Step [5/142], Loss: 0.7744\n",
            "Epoch [20/20], Step [6/142], Loss: 0.8479\n",
            "Epoch [20/20], Step [7/142], Loss: 0.8335\n",
            "Epoch [20/20], Step [8/142], Loss: 0.7704\n",
            "Epoch [20/20], Step [9/142], Loss: 0.9118\n",
            "Epoch [20/20], Step [10/142], Loss: 0.7595\n",
            "Epoch [20/20], Step [11/142], Loss: 0.8424\n",
            "Epoch [20/20], Step [12/142], Loss: 0.8511\n",
            "Epoch [20/20], Step [13/142], Loss: 0.8799\n",
            "Epoch [20/20], Step [14/142], Loss: 0.7690\n",
            "Epoch [20/20], Step [15/142], Loss: 0.7795\n",
            "Epoch [20/20], Step [16/142], Loss: 0.9442\n",
            "Epoch [20/20], Step [17/142], Loss: 0.7330\n",
            "Epoch [20/20], Step [18/142], Loss: 0.8203\n",
            "Epoch [20/20], Step [19/142], Loss: 0.8599\n",
            "Epoch [20/20], Step [20/142], Loss: 0.8366\n",
            "Epoch [20/20], Step [21/142], Loss: 0.8512\n",
            "Epoch [20/20], Step [22/142], Loss: 0.8598\n",
            "Epoch [20/20], Step [23/142], Loss: 0.7909\n",
            "Epoch [20/20], Step [24/142], Loss: 0.7870\n",
            "Epoch [20/20], Step [25/142], Loss: 0.8136\n",
            "Epoch [20/20], Step [26/142], Loss: 0.7688\n",
            "Epoch [20/20], Step [27/142], Loss: 0.8501\n",
            "Epoch [20/20], Step [28/142], Loss: 0.8366\n",
            "Epoch [20/20], Step [29/142], Loss: 0.8148\n",
            "Epoch [20/20], Step [30/142], Loss: 0.7907\n",
            "Epoch [20/20], Step [31/142], Loss: 0.7768\n",
            "Epoch [20/20], Step [32/142], Loss: 0.7443\n",
            "Epoch [20/20], Step [33/142], Loss: 0.8486\n",
            "Epoch [20/20], Step [34/142], Loss: 0.8530\n",
            "Epoch [20/20], Step [35/142], Loss: 0.8176\n",
            "Epoch [20/20], Step [36/142], Loss: 0.9084\n",
            "Epoch [20/20], Step [37/142], Loss: 0.9381\n",
            "Epoch [20/20], Step [38/142], Loss: 0.8502\n",
            "Epoch [20/20], Step [39/142], Loss: 0.7655\n",
            "Epoch [20/20], Step [40/142], Loss: 0.8678\n",
            "Epoch [20/20], Step [41/142], Loss: 0.8732\n",
            "Epoch [20/20], Step [42/142], Loss: 0.7865\n",
            "Epoch [20/20], Step [43/142], Loss: 0.8073\n",
            "Epoch [20/20], Step [44/142], Loss: 0.8076\n",
            "Epoch [20/20], Step [45/142], Loss: 0.8233\n",
            "Epoch [20/20], Step [46/142], Loss: 0.7434\n",
            "Epoch [20/20], Step [47/142], Loss: 0.7970\n",
            "Epoch [20/20], Step [48/142], Loss: 0.9119\n",
            "Epoch [20/20], Step [49/142], Loss: 0.8914\n",
            "Epoch [20/20], Step [50/142], Loss: 0.8459\n",
            "Epoch [20/20], Step [51/142], Loss: 0.7469\n",
            "Epoch [20/20], Step [52/142], Loss: 0.8311\n",
            "Epoch [20/20], Step [53/142], Loss: 0.8808\n",
            "Epoch [20/20], Step [54/142], Loss: 0.8803\n",
            "Epoch [20/20], Step [55/142], Loss: 0.8796\n",
            "Epoch [20/20], Step [56/142], Loss: 0.8763\n",
            "Epoch [20/20], Step [57/142], Loss: 0.8243\n",
            "Epoch [20/20], Step [58/142], Loss: 0.8261\n",
            "Epoch [20/20], Step [59/142], Loss: 0.8450\n",
            "Epoch [20/20], Step [60/142], Loss: 0.8920\n",
            "Epoch [20/20], Step [61/142], Loss: 0.8971\n",
            "Epoch [20/20], Step [62/142], Loss: 0.8843\n",
            "Epoch [20/20], Step [63/142], Loss: 0.8506\n",
            "Epoch [20/20], Step [64/142], Loss: 0.8584\n",
            "Epoch [20/20], Step [65/142], Loss: 0.9275\n",
            "Epoch [20/20], Step [66/142], Loss: 0.9051\n",
            "Epoch [20/20], Step [67/142], Loss: 0.8763\n",
            "Epoch [20/20], Step [68/142], Loss: 0.8170\n",
            "Epoch [20/20], Step [69/142], Loss: 0.8647\n",
            "Epoch [20/20], Step [70/142], Loss: 0.8600\n",
            "Epoch [20/20], Step [71/142], Loss: 0.8066\n",
            "Epoch [20/20], Step [72/142], Loss: 0.7640\n",
            "Epoch [20/20], Step [73/142], Loss: 0.7978\n",
            "Epoch [20/20], Step [74/142], Loss: 0.8994\n",
            "Epoch [20/20], Step [75/142], Loss: 0.9140\n",
            "Epoch [20/20], Step [76/142], Loss: 0.8162\n",
            "Epoch [20/20], Step [77/142], Loss: 0.8094\n",
            "Epoch [20/20], Step [78/142], Loss: 0.8826\n",
            "Epoch [20/20], Step [79/142], Loss: 0.7957\n",
            "Epoch [20/20], Step [80/142], Loss: 0.9554\n",
            "Epoch [20/20], Step [81/142], Loss: 0.7605\n",
            "Epoch [20/20], Step [82/142], Loss: 0.8504\n",
            "Epoch [20/20], Step [83/142], Loss: 0.8630\n",
            "Epoch [20/20], Step [84/142], Loss: 0.8082\n",
            "Epoch [20/20], Step [85/142], Loss: 0.8932\n",
            "Epoch [20/20], Step [86/142], Loss: 0.7821\n",
            "Epoch [20/20], Step [87/142], Loss: 0.8019\n",
            "Epoch [20/20], Step [88/142], Loss: 0.9066\n",
            "Epoch [20/20], Step [89/142], Loss: 0.8293\n",
            "Epoch [20/20], Step [90/142], Loss: 1.0031\n",
            "Epoch [20/20], Step [91/142], Loss: 0.8624\n",
            "Epoch [20/20], Step [92/142], Loss: 0.8291\n",
            "Epoch [20/20], Step [93/142], Loss: 0.8967\n",
            "Epoch [20/20], Step [94/142], Loss: 0.8500\n",
            "Epoch [20/20], Step [95/142], Loss: 0.7932\n",
            "Epoch [20/20], Step [96/142], Loss: 0.8819\n",
            "Epoch [20/20], Step [97/142], Loss: 0.8296\n",
            "Epoch [20/20], Step [98/142], Loss: 0.9498\n",
            "Epoch [20/20], Step [99/142], Loss: 0.8480\n",
            "Epoch [20/20], Step [100/142], Loss: 0.7950\n",
            "Epoch [20/20], Step [101/142], Loss: 0.8291\n",
            "Epoch [20/20], Step [102/142], Loss: 0.8621\n",
            "Epoch [20/20], Step [103/142], Loss: 0.8443\n",
            "Epoch [20/20], Step [104/142], Loss: 0.8647\n",
            "Epoch [20/20], Step [105/142], Loss: 0.7810\n",
            "Epoch [20/20], Step [106/142], Loss: 0.7921\n",
            "Epoch [20/20], Step [107/142], Loss: 0.8909\n",
            "Epoch [20/20], Step [108/142], Loss: 0.8523\n",
            "Epoch [20/20], Step [109/142], Loss: 0.8421\n",
            "Epoch [20/20], Step [110/142], Loss: 0.8527\n",
            "Epoch [20/20], Step [111/142], Loss: 0.7807\n",
            "Epoch [20/20], Step [112/142], Loss: 0.8105\n",
            "Epoch [20/20], Step [113/142], Loss: 0.7979\n",
            "Epoch [20/20], Step [114/142], Loss: 0.7583\n",
            "Epoch [20/20], Step [115/142], Loss: 0.8467\n",
            "Epoch [20/20], Step [116/142], Loss: 0.7998\n",
            "Epoch [20/20], Step [117/142], Loss: 0.7981\n",
            "Epoch [20/20], Step [118/142], Loss: 0.9195\n",
            "Epoch [20/20], Step [119/142], Loss: 0.8590\n",
            "Epoch [20/20], Step [120/142], Loss: 0.7620\n",
            "Epoch [20/20], Step [121/142], Loss: 0.7919\n",
            "Epoch [20/20], Step [122/142], Loss: 0.8634\n",
            "Epoch [20/20], Step [123/142], Loss: 0.7845\n",
            "Epoch [20/20], Step [124/142], Loss: 0.7614\n",
            "Epoch [20/20], Step [125/142], Loss: 0.8920\n",
            "Epoch [20/20], Step [126/142], Loss: 0.8358\n",
            "Epoch [20/20], Step [127/142], Loss: 0.7657\n",
            "Epoch [20/20], Step [128/142], Loss: 0.9054\n",
            "Epoch [20/20], Step [129/142], Loss: 0.8587\n",
            "Epoch [20/20], Step [130/142], Loss: 0.8575\n",
            "Epoch [20/20], Step [131/142], Loss: 0.9069\n",
            "Epoch [20/20], Step [132/142], Loss: 0.8740\n",
            "Epoch [20/20], Step [133/142], Loss: 0.8434\n",
            "Epoch [20/20], Step [134/142], Loss: 0.8514\n",
            "Epoch [20/20], Step [135/142], Loss: 0.8636\n",
            "Epoch [20/20], Step [136/142], Loss: 0.8383\n",
            "Epoch [20/20], Step [137/142], Loss: 0.8617\n",
            "Epoch [20/20], Step [138/142], Loss: 0.8151\n",
            "Epoch [20/20], Step [139/142], Loss: 0.7702\n",
            "Epoch [20/20], Step [140/142], Loss: 0.7650\n",
            "Epoch [20/20], Step [141/142], Loss: 0.8211\n",
            "Epoch [20/20], Step [142/142], Loss: 0.8652\n",
            "Validation Accuracy: 0.6151\n",
            "Accuracy of RNN: 0.62\n",
            "Running LSTM with adam optimizer...\n",
            "497\n",
            "461\n",
            "Training LSTM model with adam optimizer using full sequences...\n",
            "Epoch 1/20, Training Loss: 0.7336\n",
            "Epoch 2/20, Training Loss: 0.7612\n",
            "Epoch 3/20, Training Loss: 0.7645\n",
            "Epoch 4/20, Training Loss: 0.7404\n",
            "Epoch 5/20, Training Loss: 0.7199\n",
            "Epoch 6/20, Training Loss: 0.7353\n",
            "Epoch 7/20, Training Loss: 0.7303\n",
            "Epoch 8/20, Training Loss: 0.7082\n",
            "Epoch 9/20, Training Loss: 0.7022\n",
            "Epoch 10/20, Training Loss: 0.7070\n",
            "Epoch 11/20, Training Loss: 0.7001\n",
            "Epoch 12/20, Training Loss: 0.6849\n",
            "Epoch 13/20, Training Loss: 0.6791\n",
            "Epoch 14/20, Training Loss: 0.6778\n",
            "Epoch 15/20, Training Loss: 0.6660\n",
            "Epoch 16/20, Training Loss: 0.6604\n",
            "Epoch 17/20, Training Loss: 0.6586\n",
            "Epoch 18/20, Training Loss: 0.6445\n",
            "Epoch 19/20, Training Loss: 0.6353\n",
            "Epoch 20/20, Training Loss: 0.6324\n",
            "Accuracy of LSTM: 0.63\n",
            "Running LSTM with sgd optimizer...\n",
            "497\n",
            "461\n",
            "Training LSTM model with sgd optimizer using full sequences...\n",
            "Epoch 1/20, Training Loss: 0.6213\n",
            "Epoch 2/20, Training Loss: 0.6213\n",
            "Epoch 3/20, Training Loss: 0.6213\n",
            "Epoch 4/20, Training Loss: 0.6213\n",
            "Epoch 5/20, Training Loss: 0.6212\n",
            "Epoch 6/20, Training Loss: 0.6212\n",
            "Epoch 7/20, Training Loss: 0.6212\n",
            "Epoch 8/20, Training Loss: 0.6211\n",
            "Epoch 9/20, Training Loss: 0.6211\n",
            "Epoch 10/20, Training Loss: 0.6211\n",
            "Epoch 11/20, Training Loss: 0.6211\n",
            "Epoch 12/20, Training Loss: 0.6210\n",
            "Epoch 13/20, Training Loss: 0.6210\n",
            "Epoch 14/20, Training Loss: 0.6210\n",
            "Epoch 15/20, Training Loss: 0.6210\n",
            "Epoch 16/20, Training Loss: 0.6209\n",
            "Epoch 17/20, Training Loss: 0.6209\n",
            "Epoch 18/20, Training Loss: 0.6209\n",
            "Epoch 19/20, Training Loss: 0.6209\n",
            "Epoch 20/20, Training Loss: 0.6208\n",
            "Accuracy of LSTM: 0.63\n",
            "Running LSTM with rmsprop optimizer...\n",
            "497\n",
            "461\n",
            "Training LSTM model with rmsprop optimizer using full sequences...\n",
            "Epoch 1/20, Training Loss: 0.6208\n",
            "Epoch 2/20, Training Loss: 6.2561\n",
            "Epoch 3/20, Training Loss: 1.0733\n",
            "Epoch 4/20, Training Loss: 2.1598\n",
            "Epoch 5/20, Training Loss: 1.0114\n",
            "Epoch 6/20, Training Loss: 0.8639\n",
            "Epoch 7/20, Training Loss: 0.8355\n",
            "Epoch 8/20, Training Loss: 0.8233\n",
            "Epoch 9/20, Training Loss: 0.8133\n",
            "Epoch 10/20, Training Loss: 0.8040\n",
            "Epoch 11/20, Training Loss: 0.7950\n",
            "Epoch 12/20, Training Loss: 0.7860\n",
            "Epoch 13/20, Training Loss: 0.7770\n",
            "Epoch 14/20, Training Loss: 0.7678\n",
            "Epoch 15/20, Training Loss: 0.7584\n",
            "Epoch 16/20, Training Loss: 0.7486\n",
            "Epoch 17/20, Training Loss: 0.7385\n",
            "Epoch 18/20, Training Loss: 0.7297\n",
            "Epoch 19/20, Training Loss: 0.7287\n",
            "Epoch 20/20, Training Loss: 0.7456\n",
            "Accuracy of LSTM: 0.60\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_multi_ex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "Fpf4Ctc0j-0O",
        "outputId": "1c62a330-fd18-4a69-e480-33b369d5754b"
      },
      "id": "Fpf4Ctc0j-0O",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Model Optimizer  Accuracy\n",
              "0   MLP      adam  0.626420\n",
              "1   MLP       sgd  0.630682\n",
              "2   MLP   rmsprop  0.616477\n",
              "3   RNN      adam  0.620739\n",
              "4   RNN       sgd  0.622159\n",
              "5   RNN   rmsprop  0.615057\n",
              "6  LSTM      adam  0.626989\n",
              "7  LSTM       sgd  0.627557\n",
              "8  LSTM   rmsprop  0.604830"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9de26faf-989b-4fee-9a5e-b83db69e4111\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Optimizer</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MLP</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.626420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>MLP</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.630682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>MLP</td>\n",
              "      <td>rmsprop</td>\n",
              "      <td>0.616477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RNN</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.620739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RNN</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.622159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>RNN</td>\n",
              "      <td>rmsprop</td>\n",
              "      <td>0.615057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LSTM</td>\n",
              "      <td>adam</td>\n",
              "      <td>0.626989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>LSTM</td>\n",
              "      <td>sgd</td>\n",
              "      <td>0.627557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>LSTM</td>\n",
              "      <td>rmsprop</td>\n",
              "      <td>0.604830</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9de26faf-989b-4fee-9a5e-b83db69e4111')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9de26faf-989b-4fee-9a5e-b83db69e4111 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9de26faf-989b-4fee-9a5e-b83db69e4111');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f7ef3335-5f75-4462-a8a3-a9f4f7e7ce42\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f7ef3335-5f75-4462-a8a3-a9f4f7e7ce42')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f7ef3335-5f75-4462-a8a3-a9f4f7e7ce42 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_034aaa11-055e-4740-b7c3-9e5ac1c5629d\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_multi_ex')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_034aaa11-055e-4740-b7c3-9e5ac1c5629d button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_multi_ex');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_multi_ex",
              "summary": "{\n  \"name\": \"results_multi_ex\",\n  \"rows\": 9,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"MLP\",\n          \"RNN\",\n          \"LSTM\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Optimizer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"adam\",\n          \"sgd\",\n          \"rmsprop\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.008064129504112076,\n        \"min\": 0.6048295454545455,\n        \"max\": 0.6306818181818182,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.6275568181818182,\n          0.6306818181818182,\n          0.6150568181818182\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "source": [
        "# @title Accuracy\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "results_multi_ex['Accuracy'].plot(kind='hist', bins=20, title='Accuracy')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "7aik8J4FekFO",
        "outputId": "e767f253-7ab2-45c5-f14a-fda92ce1584f"
      },
      "id": "7aik8J4FekFO"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}